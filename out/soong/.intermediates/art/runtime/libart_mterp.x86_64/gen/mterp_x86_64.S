/* DO NOT EDIT: This file was generated by gen-mterp.py. */
/*
 * Copyright (C) 2016 The Android Open Source Project
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/*
  Art assembly interpreter notes:

  First validate assembly code by implementing ExecuteXXXImpl() style body (doesn't
  handle invoke, allows higher-level code to create frame & shadow frame.

  Once that's working, support direct entry code & eliminate shadow frame (and
  excess locals allocation.

  Some (hopefully) temporary ugliness.  We'll treat rFP as pointing to the
  base of the vreg array within the shadow frame.  Access the other fields,
  dex_pc_, method_ and number_of_vregs_ via negative offsets.  For now, we'll continue
  the shadow frame mechanism of double-storing object references - via rFP &
  number_of_vregs_.

 */

/*
x86_64 ABI general notes:

Caller save set:
   rax, rdx, rcx, rsi, rdi, r8-r11, st(0)-st(7)
Callee save set:
   rbx, rbp, r12-r15
Return regs:
   32-bit in eax
   64-bit in rax
   fp on xmm0

First 8 fp parameters came in xmm0-xmm7.
First 6 non-fp parameters came in rdi, rsi, rdx, rcx, r8, r9.
Other parameters passed on stack, pushed right-to-left.  On entry to target, first
param is at 8(%esp).  Traditional entry code is:

Stack must be 16-byte aligned to support SSE in native code.

If we're not doing variable stack allocation (alloca), the frame pointer can be
eliminated and all arg references adjusted to be esp relative.
*/

/*
Mterp and x86_64 notes:

Some key interpreter variables will be assigned to registers.

  nick     reg   purpose
  rPROFILE rbp   countdown register for jit profiling
  rPC      r12   interpreted program counter, used for fetching instructions
  rFP      r13   interpreted frame pointer, used for accessing locals and args
  rINSTw   bx    first 16-bit code of current instruction
  rINSTbl  bl    opcode portion of instruction word
  rINSTbh  bh    high byte of inst word, usually contains src/tgt reg names
  rIBASE   r14   base of instruction handler table
  rREFS    r15   base of object references in shadow frame.

Notes:
   o High order 16 bits of ebx must be zero on entry to handler
   o rPC, rFP, rINSTw/rINSTbl valid on handler entry and exit
   o eax and ecx are scratch, rINSTw/ebx sometimes scratch

Macros are provided for common operations.  Each macro MUST emit only
one instruction to make instruction-counting easier.  They MUST NOT alter
unspecified registers or condition codes.
*/

/*
 * This is a #include, not a %include, because we want the C pre-processor
 * to expand the macros into assembler assignment statements.
 */
#include "asm_support.h"
#include "interpreter/cfi_asm_support.h"

#define LITERAL(value) $(value)

/*
 * Handle mac compiler specific
 */
#if defined(__APPLE__)
    #define MACRO_LITERAL(value) $(value)
    #define FUNCTION_TYPE(name)
    #define OBJECT_TYPE(name)
    #define SIZE(start,end)
    // Mac OS' symbols have an _ prefix.
    #define SYMBOL(name) _ ## name
    #define ASM_HIDDEN .private_extern
#else
    #define MACRO_LITERAL(value) $value
    #define FUNCTION_TYPE(name) .type name, @function
    #define OBJECT_TYPE(name) .type name, @object
    #define SIZE(start,end) .size start, .-end
    #define SYMBOL(name) name
    #define ASM_HIDDEN .hidden
#endif

.macro PUSH _reg
    pushq \_reg
    .cfi_adjust_cfa_offset 8
    .cfi_rel_offset \_reg, 0
.endm

.macro POP _reg
    popq \_reg
    .cfi_adjust_cfa_offset -8
    .cfi_restore \_reg
.endm

/*
 * Instead of holding a pointer to the shadow frame, we keep rFP at the base of the vregs.  So,
 * to access other shadow frame fields, we need to use a backwards offset.  Define those here.
 */
#define OFF_FP(a) (a - SHADOWFRAME_VREGS_OFFSET)
#define OFF_FP_NUMBER_OF_VREGS OFF_FP(SHADOWFRAME_NUMBER_OF_VREGS_OFFSET)
#define OFF_FP_DEX_PC OFF_FP(SHADOWFRAME_DEX_PC_OFFSET)
#define OFF_FP_LINK OFF_FP(SHADOWFRAME_LINK_OFFSET)
#define OFF_FP_METHOD OFF_FP(SHADOWFRAME_METHOD_OFFSET)
#define OFF_FP_RESULT_REGISTER OFF_FP(SHADOWFRAME_RESULT_REGISTER_OFFSET)
#define OFF_FP_DEX_PC_PTR OFF_FP(SHADOWFRAME_DEX_PC_PTR_OFFSET)
#define OFF_FP_DEX_INSTRUCTIONS OFF_FP(SHADOWFRAME_DEX_INSTRUCTIONS_OFFSET)
#define OFF_FP_COUNTDOWN_OFFSET OFF_FP(SHADOWFRAME_HOTNESS_COUNTDOWN_OFFSET)
#define OFF_FP_SHADOWFRAME (-SHADOWFRAME_VREGS_OFFSET)

/* Frame size must be 16-byte aligned.
 * Remember about 8 bytes for return address + 6 * 8 for spills.
 */
#define FRAME_SIZE     8

/* Frame diagram while executing ExecuteMterpImpl, high to low addresses */
#define IN_ARG3        %rcx
#define IN_ARG2        %rdx
#define IN_ARG1        %rsi
#define IN_ARG0        %rdi
/* Spill offsets relative to %esp */
#define SELF_SPILL     (FRAME_SIZE -  8)
/* Out Args  */
#define OUT_ARG3       %rcx
#define OUT_ARG2       %rdx
#define OUT_ARG1       %rsi
#define OUT_ARG0       %rdi
#define OUT_32_ARG3    %ecx
#define OUT_32_ARG2    %edx
#define OUT_32_ARG1    %esi
#define OUT_32_ARG0    %edi
#define OUT_FP_ARG1    %xmm1
#define OUT_FP_ARG0    %xmm0

/* During bringup, we'll use the shadow frame model instead of rFP */
/* single-purpose registers, given names for clarity */
#define rSELF    SELF_SPILL(%rsp)
#define rPC      %r12
#define CFI_DEX  12 // DWARF register number of the register holding dex-pc (rPC).
#define CFI_TMP  5  // DWARF register number of the first argument register (rdi).
#define rFP      %r13
#define rINST    %ebx
#define rINSTq   %rbx
#define rINSTw   %bx
#define rINSTbh  %bh
#define rINSTbl  %bl
#define rIBASE   %r14
#define rREFS    %r15
#define rPROFILE %ebp

#define MTERP_LOGGING 0

/*
 * "export" the PC to dex_pc field in the shadow frame, f/b/o future exception objects.  Must
 * be done *before* something throws.
 *
 * It's okay to do this more than once.
 *
 * NOTE: the fast interpreter keeps track of dex pc as a direct pointer to the mapped
 * dex byte codes.  However, the rest of the runtime expects dex pc to be an instruction
 * offset into the code_items_[] array.  For effiency, we will "export" the
 * current dex pc as a direct pointer using the EXPORT_PC macro, and rely on GetDexPC
 * to convert to a dex pc when needed.
 */
.macro EXPORT_PC
    movq    rPC, OFF_FP_DEX_PC_PTR(rFP)
.endm

/*
 * Refresh handler table.
 * IBase handles uses the caller save register so we must restore it after each call.
 * Also it is used as a result of some 64-bit operations (like imul) and we should
 * restore it in such cases also.
 *
 */
.macro REFRESH_IBASE_REG self_reg
    movq    THREAD_CURRENT_IBASE_OFFSET(\self_reg), rIBASE
.endm
.macro REFRESH_IBASE
    movq    rSELF, rIBASE
    REFRESH_IBASE_REG rIBASE
.endm

/*
 * Refresh rINST.
 * At enter to handler rINST does not contain the opcode number.
 * However some utilities require the full value, so this macro
 * restores the opcode number.
 */
.macro REFRESH_INST _opnum
    movb    rINSTbl, rINSTbh
    movb    $\_opnum, rINSTbl
.endm

/*
 * Fetch the next instruction from rPC into rINSTw.  Does not advance rPC.
 */
.macro FETCH_INST
    movzwq  (rPC), rINSTq
.endm

/*
 * Remove opcode from rINST, compute the address of handler and jump to it.
 */
.macro GOTO_NEXT
    movzx   rINSTbl,%eax
    movzbl  rINSTbh,rINST
    shll    MACRO_LITERAL(MTERP_HANDLER_SIZE_LOG2), %eax
    addq    rIBASE, %rax
    jmp     *%rax
.endm

/*
 * Advance rPC by instruction count.
 */
.macro ADVANCE_PC _count
    leaq    2*\_count(rPC), rPC
.endm

/*
 * Advance rPC by instruction count, fetch instruction and jump to handler.
 */
.macro ADVANCE_PC_FETCH_AND_GOTO_NEXT _count
    ADVANCE_PC \_count
    FETCH_INST
    GOTO_NEXT
.endm

/*
 * Get/set the 32-bit value from a Dalvik register.
 */
#define VREG_ADDRESS(_vreg) (rFP,_vreg,4)
#define VREG_HIGH_ADDRESS(_vreg) 4(rFP,_vreg,4)
#define VREG_REF_ADDRESS(_vreg) (rREFS,_vreg,4)
#define VREG_REF_HIGH_ADDRESS(_vreg) 4(rREFS,_vreg,4)

.macro GET_VREG _reg _vreg
    movl    VREG_ADDRESS(\_vreg), \_reg
.endm

/* Read wide value. */
.macro GET_WIDE_VREG _reg _vreg
    movq    VREG_ADDRESS(\_vreg), \_reg
.endm

.macro SET_VREG _reg _vreg
    movl    \_reg, VREG_ADDRESS(\_vreg)
    movl    MACRO_LITERAL(0), VREG_REF_ADDRESS(\_vreg)
.endm

/* Write wide value. reg is clobbered. */
.macro SET_WIDE_VREG _reg _vreg
    movq    \_reg, VREG_ADDRESS(\_vreg)
    xorq    \_reg, \_reg
    movq    \_reg, VREG_REF_ADDRESS(\_vreg)
.endm

.macro SET_VREG_OBJECT _reg _vreg
    movl    \_reg, VREG_ADDRESS(\_vreg)
    movl    \_reg, VREG_REF_ADDRESS(\_vreg)
.endm

.macro GET_VREG_HIGH _reg _vreg
    movl    VREG_HIGH_ADDRESS(\_vreg), \_reg
.endm

.macro SET_VREG_HIGH _reg _vreg
    movl    \_reg, VREG_HIGH_ADDRESS(\_vreg)
    movl    MACRO_LITERAL(0), VREG_REF_HIGH_ADDRESS(\_vreg)
.endm

.macro CLEAR_REF _vreg
    movl    MACRO_LITERAL(0), VREG_REF_ADDRESS(\_vreg)
.endm

.macro CLEAR_WIDE_REF _vreg
    movl    MACRO_LITERAL(0), VREG_REF_ADDRESS(\_vreg)
    movl    MACRO_LITERAL(0), VREG_REF_HIGH_ADDRESS(\_vreg)
.endm

.macro GET_VREG_XMMs _xmmreg _vreg
    movss VREG_ADDRESS(\_vreg), \_xmmreg
.endm
.macro GET_VREG_XMMd _xmmreg _vreg
    movsd VREG_ADDRESS(\_vreg), \_xmmreg
.endm
.macro SET_VREG_XMMs _xmmreg _vreg
    movss \_xmmreg, VREG_ADDRESS(\_vreg)
.endm
.macro SET_VREG_XMMd _xmmreg _vreg
    movsd \_xmmreg, VREG_ADDRESS(\_vreg)
.endm

/*
 * function support macros.
 */
.macro ENTRY name
    .text
    ASM_HIDDEN SYMBOL(\name)
    .global SYMBOL(\name)
    FUNCTION_TYPE(\name)
SYMBOL(\name):
.endm

.macro END name
    SIZE(\name,\name)
.endm

/*
 * Copyright (C) 2016 The Android Open Source Project
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
/*
 * Interpreter entry point.
 *
 * On entry:
 *  0  Thread* self
 *  1  insns_
 *  2  ShadowFrame
 *  3  JValue* result_register
 *
 */

ENTRY ExecuteMterpImpl
    .cfi_startproc
    .cfi_def_cfa rsp, 8

    /* Spill callee save regs */
    PUSH %rbx
    PUSH %rbp
    PUSH %r12
    PUSH %r13
    PUSH %r14
    PUSH %r15

    /* Allocate frame */
    subq    $FRAME_SIZE, %rsp
    .cfi_adjust_cfa_offset FRAME_SIZE

    /* Remember the return register */
    movq    IN_ARG3, SHADOWFRAME_RESULT_REGISTER_OFFSET(IN_ARG2)

    /* Remember the code_item */
    movq    IN_ARG1, SHADOWFRAME_DEX_INSTRUCTIONS_OFFSET(IN_ARG2)

    /* set up "named" registers */
    movl    SHADOWFRAME_NUMBER_OF_VREGS_OFFSET(IN_ARG2), %eax
    leaq    SHADOWFRAME_VREGS_OFFSET(IN_ARG2), rFP
    leaq    (rFP, %rax, 4), rREFS
    movl    SHADOWFRAME_DEX_PC_OFFSET(IN_ARG2), %eax
    leaq    (IN_ARG1, %rax, 2), rPC
    CFI_DEFINE_DEX_PC_WITH_OFFSET(CFI_TMP, CFI_DEX, 0)
    EXPORT_PC

    /* Starting ibase */
    movq    IN_ARG0, rSELF
    REFRESH_IBASE_REG IN_ARG0

    /* Set up for backwards branches & osr profiling */
    movq    IN_ARG0, OUT_ARG2  /* Set up OUT_ARG2 before clobbering IN_ARG0 */
    movq    OFF_FP_METHOD(rFP), OUT_ARG0
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG1
    call    SYMBOL(MterpSetUpHotnessCountdown)
    movswl  %ax, rPROFILE

    /* start executing the instruction at rPC */
    FETCH_INST
    GOTO_NEXT
    /* NOTE: no fallthrough */
    // cfi info continues, and covers the whole mterp implementation.
    END ExecuteMterpImpl

    OBJECT_TYPE(artMterpAsmInstructionStart)
    ASM_HIDDEN SYMBOL(artMterpAsmInstructionStart)
    .global SYMBOL(artMterpAsmInstructionStart)
SYMBOL(artMterpAsmInstructionStart) = .L_op_nop
    .text

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_nop: /* 0x00 */
    ENTRY mterp_op_nop
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_nop

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_move: /* 0x01 */
    ENTRY mterp_op_move
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /* for move, move-object, long-to-int */
    /* op vA, vB */
    movl    rINST, %eax                     # eax <- BA
    andb    $0xf, %al                      # eax <- A
    shrl    $4, rINST                      # rINST <- B
    GET_VREG %edx, rINSTq
    .if 0
    SET_VREG_OBJECT %edx, %rax              # fp[A] <- fp[B]
    .else
    SET_VREG %edx, %rax                     # fp[A] <- fp[B]
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_move

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_move_from16: /* 0x02 */
    ENTRY mterp_op_move_from16
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /* for: move/from16, move-object/from16 */
    /* op vAA, vBBBB */
    movzwq  2(rPC), %rax                    # eax <- BBBB
    GET_VREG %edx, %rax                     # edx <- fp[BBBB]
    .if 0
    SET_VREG_OBJECT %edx, rINSTq            # fp[A] <- fp[B]
    .else
    SET_VREG %edx, rINSTq                   # fp[A] <- fp[B]
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_move_from16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_move_16: /* 0x03 */
    ENTRY mterp_op_move_16
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /* for: move/16, move-object/16 */
    /* op vAAAA, vBBBB */
    movzwq  4(rPC), %rcx                    # ecx <- BBBB
    movzwq  2(rPC), %rax                    # eax <- AAAA
    GET_VREG %edx, %rcx
    .if 0
    SET_VREG_OBJECT %edx, %rax              # fp[A] <- fp[B]
    .else
    SET_VREG %edx, %rax                     # fp[A] <- fp[B]
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 3

    END mterp_op_move_16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_move_wide: /* 0x04 */
    ENTRY mterp_op_move_wide
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /* move-wide vA, vB */
    /* NOTE: regs can overlap, e.g. "move v6,v7" or "move v7,v6" */
    movl    rINST, %ecx                     # ecx <- BA
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # ecx <- A
    GET_WIDE_VREG %rdx, rINSTq              # rdx <- v[B]
    SET_WIDE_VREG %rdx, %rcx                # v[A] <- rdx
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_move_wide

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_move_wide_from16: /* 0x05 */
    ENTRY mterp_op_move_wide_from16
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /* move-wide/from16 vAA, vBBBB */
    /* NOTE: regs can overlap, e.g. "move v6,v7" or "move v7,v6" */
    movzwl  2(rPC), %ecx                    # ecx <- BBBB
    GET_WIDE_VREG %rdx, %rcx                # rdx <- v[B]
    SET_WIDE_VREG %rdx, rINSTq              # v[A] <- rdx
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_move_wide_from16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_move_wide_16: /* 0x06 */
    ENTRY mterp_op_move_wide_16
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /* move-wide/16 vAAAA, vBBBB */
    /* NOTE: regs can overlap, e.g. "move v6,v7" or "move v7,v6" */
    movzwq  4(rPC), %rcx                    # ecx<- BBBB
    movzwq  2(rPC), %rax                    # eax<- AAAA
    GET_WIDE_VREG %rdx, %rcx                # rdx <- v[B]
    SET_WIDE_VREG %rdx, %rax                # v[A] <- rdx
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 3

    END mterp_op_move_wide_16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_move_object: /* 0x07 */
    ENTRY mterp_op_move_object
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /* for move, move-object, long-to-int */
    /* op vA, vB */
    movl    rINST, %eax                     # eax <- BA
    andb    $0xf, %al                      # eax <- A
    shrl    $4, rINST                      # rINST <- B
    GET_VREG %edx, rINSTq
    .if 1
    SET_VREG_OBJECT %edx, %rax              # fp[A] <- fp[B]
    .else
    SET_VREG %edx, %rax                     # fp[A] <- fp[B]
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_move_object

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_move_object_from16: /* 0x08 */
    ENTRY mterp_op_move_object_from16
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /* for: move/from16, move-object/from16 */
    /* op vAA, vBBBB */
    movzwq  2(rPC), %rax                    # eax <- BBBB
    GET_VREG %edx, %rax                     # edx <- fp[BBBB]
    .if 1
    SET_VREG_OBJECT %edx, rINSTq            # fp[A] <- fp[B]
    .else
    SET_VREG %edx, rINSTq                   # fp[A] <- fp[B]
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_move_object_from16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_move_object_16: /* 0x09 */
    ENTRY mterp_op_move_object_16
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /* for: move/16, move-object/16 */
    /* op vAAAA, vBBBB */
    movzwq  4(rPC), %rcx                    # ecx <- BBBB
    movzwq  2(rPC), %rax                    # eax <- AAAA
    GET_VREG %edx, %rcx
    .if 1
    SET_VREG_OBJECT %edx, %rax              # fp[A] <- fp[B]
    .else
    SET_VREG %edx, %rax                     # fp[A] <- fp[B]
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 3

    END mterp_op_move_object_16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_move_result: /* 0x0a */
    ENTRY mterp_op_move_result
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /* for: move-result, move-result-object */
    /* op vAA */
    movq    OFF_FP_RESULT_REGISTER(rFP), %rax    # get pointer to result JType.
    movl    (%rax), %eax                    # r0 <- result.i.
    .if 0
    SET_VREG_OBJECT %eax, rINSTq            # fp[A] <- fp[B]
    .else
    SET_VREG %eax, rINSTq                   # fp[A] <- fp[B]
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_move_result

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_move_result_wide: /* 0x0b */
    ENTRY mterp_op_move_result_wide
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /* move-result-wide vAA */
    movq    OFF_FP_RESULT_REGISTER(rFP), %rax    # get pointer to result JType.
    movq    (%rax), %rdx                         # Get wide
    SET_WIDE_VREG %rdx, rINSTq                   # v[AA] <- rdx
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_move_result_wide

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_move_result_object: /* 0x0c */
    ENTRY mterp_op_move_result_object
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /* for: move-result, move-result-object */
    /* op vAA */
    movq    OFF_FP_RESULT_REGISTER(rFP), %rax    # get pointer to result JType.
    movl    (%rax), %eax                    # r0 <- result.i.
    .if 1
    SET_VREG_OBJECT %eax, rINSTq            # fp[A] <- fp[B]
    .else
    SET_VREG %eax, rINSTq                   # fp[A] <- fp[B]
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_move_result_object

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_move_exception: /* 0x0d */
    ENTRY mterp_op_move_exception
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /* move-exception vAA */
    movq    rSELF, %rcx
    movl    THREAD_EXCEPTION_OFFSET(%rcx), %eax
    SET_VREG_OBJECT %eax, rINSTq            # fp[AA] <- exception object
    movl    $0, THREAD_EXCEPTION_OFFSET(%rcx)
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_move_exception

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_return_void: /* 0x0e */
    ENTRY mterp_op_return_void
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    .extern MterpThreadFenceForConstructor
    call    SYMBOL(MterpThreadFenceForConstructor)
    movq    rSELF, OUT_ARG0
    testl   $(THREAD_SUSPEND_OR_CHECKPOINT_REQUEST), THREAD_FLAGS_OFFSET(OUT_ARG0)
    jz      1f
    call    SYMBOL(MterpSuspendCheck)
1:
    xorq    %rax, %rax
    jmp     MterpReturn

    END mterp_op_return_void

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_return: /* 0x0f */
    ENTRY mterp_op_return
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Return a 32-bit value.
 *
 * for: return, return-object
 */
    /* op vAA */
    .extern MterpThreadFenceForConstructor
    call    SYMBOL(MterpThreadFenceForConstructor)
    movq    rSELF, OUT_ARG0
    testl   $(THREAD_SUSPEND_OR_CHECKPOINT_REQUEST), THREAD_FLAGS_OFFSET(OUT_ARG0)
    jz      1f
    call    SYMBOL(MterpSuspendCheck)
1:
    GET_VREG %eax, rINSTq                   # eax <- vAA
    jmp     MterpReturn

    END mterp_op_return

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_return_wide: /* 0x10 */
    ENTRY mterp_op_return_wide
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Return a 64-bit value.
 */
    /* return-wide vAA */
    .extern MterpThreadFenceForConstructor
    call    SYMBOL(MterpThreadFenceForConstructor)
    movq    rSELF, OUT_ARG0
    testl   $(THREAD_SUSPEND_OR_CHECKPOINT_REQUEST), THREAD_FLAGS_OFFSET(OUT_ARG0)
    jz      1f
    call    SYMBOL(MterpSuspendCheck)
1:
    GET_WIDE_VREG %rax, rINSTq              # eax <- v[AA]
    jmp     MterpReturn

    END mterp_op_return_wide

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_return_object: /* 0x11 */
    ENTRY mterp_op_return_object
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Return a 32-bit value.
 *
 * for: return, return-object
 */
    /* op vAA */
    .extern MterpThreadFenceForConstructor
    call    SYMBOL(MterpThreadFenceForConstructor)
    movq    rSELF, OUT_ARG0
    testl   $(THREAD_SUSPEND_OR_CHECKPOINT_REQUEST), THREAD_FLAGS_OFFSET(OUT_ARG0)
    jz      1f
    call    SYMBOL(MterpSuspendCheck)
1:
    GET_VREG %eax, rINSTq                   # eax <- vAA
    jmp     MterpReturn

    END mterp_op_return_object

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_const_4: /* 0x12 */
    ENTRY mterp_op_const_4
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /* const/4 vA, #+B */
    movsbl  rINSTbl, %eax                   # eax <-ssssssBx
    movl    $0xf, rINST
    andl    %eax, rINST                     # rINST <- A
    sarl    $4, %eax
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_const_4

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_const_16: /* 0x13 */
    ENTRY mterp_op_const_16
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /* const/16 vAA, #+BBBB */
    movswl  2(rPC), %ecx                    # ecx <- ssssBBBB
    SET_VREG %ecx, rINSTq                   # vAA <- ssssBBBB
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_const_16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_const: /* 0x14 */
    ENTRY mterp_op_const
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /* const vAA, #+BBBBbbbb */
    movl    2(rPC), %eax                    # grab all 32 bits at once
    SET_VREG %eax, rINSTq                   # vAA<- eax
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 3

    END mterp_op_const

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_const_high16: /* 0x15 */
    ENTRY mterp_op_const_high16
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /* const/high16 vAA, #+BBBB0000 */
    movzwl  2(rPC), %eax                    # eax <- 0000BBBB
    sall    $16, %eax                      # eax <- BBBB0000
    SET_VREG %eax, rINSTq                   # vAA <- eax
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_const_high16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_const_wide_16: /* 0x16 */
    ENTRY mterp_op_const_wide_16
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /* const-wide/16 vAA, #+BBBB */
    movswq  2(rPC), %rax                    # rax <- ssssBBBB
    SET_WIDE_VREG %rax, rINSTq              # store
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_const_wide_16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_const_wide_32: /* 0x17 */
    ENTRY mterp_op_const_wide_32
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /* const-wide/32 vAA, #+BBBBbbbb */
    movslq   2(rPC), %rax                   # eax <- ssssssssBBBBbbbb
    SET_WIDE_VREG %rax, rINSTq              # store
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 3

    END mterp_op_const_wide_32

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_const_wide: /* 0x18 */
    ENTRY mterp_op_const_wide
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /* const-wide vAA, #+HHHHhhhhBBBBbbbb */
    movq    2(rPC), %rax                    # rax <- HHHHhhhhBBBBbbbb
    SET_WIDE_VREG %rax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 5

    END mterp_op_const_wide

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_const_wide_high16: /* 0x19 */
    ENTRY mterp_op_const_wide_high16
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /* const-wide/high16 vAA, #+BBBB000000000000 */
    movzwq  2(rPC), %rax                    # eax <- 0000BBBB
    salq    $48, %rax                      # eax <- BBBB0000
    SET_WIDE_VREG %rax, rINSTq              # v[AA+0] <- eax
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_const_wide_high16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_const_string: /* 0x1a */
    ENTRY mterp_op_const_string
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /* const/class vAA, type@BBBB */
    /* const/method-handle vAA, method_handle@BBBB */
    /* const/method-type vAA, proto@BBBB */
    /* const/string vAA, string@@BBBB */
    .extern MterpConstString
    EXPORT_PC
    movzwq  2(rPC), OUT_ARG0                # eax <- OUT_ARG0
    movq    rINSTq, OUT_ARG1
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG2
    movq    rSELF, OUT_ARG3
    call    SYMBOL(MterpConstString)                 # (index, tgt_reg, shadow_frame, self)
    testb   %al, %al
    jnz     MterpPossibleException
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_const_string

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_const_string_jumbo: /* 0x1b */
    ENTRY mterp_op_const_string_jumbo
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /* const/string vAA, String@BBBBBBBB */
    EXPORT_PC
    movl    2(rPC), OUT_32_ARG0             # OUT_32_ARG0 <- BBBB
    movq    rINSTq, OUT_ARG1
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG2
    movq    rSELF, OUT_ARG3
    call    SYMBOL(MterpConstString)        # (index, tgt_reg, shadow_frame, self)
    testb   %al, %al
    jnz     MterpPossibleException
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 3

    END mterp_op_const_string_jumbo

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_const_class: /* 0x1c */
    ENTRY mterp_op_const_class
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /* const/class vAA, type@BBBB */
    /* const/method-handle vAA, method_handle@BBBB */
    /* const/method-type vAA, proto@BBBB */
    /* const/string vAA, string@@BBBB */
    .extern MterpConstClass
    EXPORT_PC
    movzwq  2(rPC), OUT_ARG0                # eax <- OUT_ARG0
    movq    rINSTq, OUT_ARG1
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG2
    movq    rSELF, OUT_ARG3
    call    SYMBOL(MterpConstClass)                 # (index, tgt_reg, shadow_frame, self)
    testb   %al, %al
    jnz     MterpPossibleException
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_const_class

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_monitor_enter: /* 0x1d */
    ENTRY mterp_op_monitor_enter
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Synchronize on an object.
 */
    /* monitor-enter vAA */
    EXPORT_PC
    GET_VREG OUT_32_ARG0, rINSTq
    movq    rSELF, OUT_ARG1
    call    SYMBOL(artLockObjectFromCode)   # (object, self)
    testq   %rax, %rax
    jnz     MterpException
    ADVANCE_PC 1
    movq    rSELF, %rax
    cmpb    LITERAL(0), THREAD_USE_MTERP_OFFSET(%rax)
    jz      MterpFallback
    FETCH_INST
    GOTO_NEXT

    END mterp_op_monitor_enter

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_monitor_exit: /* 0x1e */
    ENTRY mterp_op_monitor_exit
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Unlock an object.
 *
 * Exceptions that occur when unlocking a monitor need to appear as
 * if they happened at the following instruction.  See the Dalvik
 * instruction spec.
 */
    /* monitor-exit vAA */
    EXPORT_PC
    GET_VREG OUT_32_ARG0, rINSTq
    movq    rSELF, OUT_ARG1
    call    SYMBOL(artUnlockObjectFromCode) # (object, self)
    testq   %rax, %rax
    jnz     MterpException
    ADVANCE_PC 1
    movq    rSELF, %rax
    cmpb    LITERAL(0), THREAD_USE_MTERP_OFFSET(%rax)
    jz      MterpFallback
    FETCH_INST
    GOTO_NEXT

    END mterp_op_monitor_exit

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_check_cast: /* 0x1f */
    ENTRY mterp_op_check_cast
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Check to see if a cast from one class to another is allowed.
 */
    /* check-cast vAA, class@BBBB */
    EXPORT_PC
    movzwq  2(rPC), OUT_ARG0                # OUT_ARG0 <- BBBB
    leaq    VREG_ADDRESS(rINSTq), OUT_ARG1
    movq    OFF_FP_METHOD(rFP), OUT_ARG2
    movq    rSELF, OUT_ARG3
    call    SYMBOL(MterpCheckCast)          # (index, &obj, method, self)
    testb   %al, %al
    jnz     MterpPossibleException
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_check_cast

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_instance_of: /* 0x20 */
    ENTRY mterp_op_instance_of
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Check to see if an object reference is an instance of a class.
 *
 * Most common situation is a non-null object, being compared against
 * an already-resolved class.
 */
    /* instance-of vA, vB, class@CCCC */
    EXPORT_PC
    movzwl  2(rPC), OUT_32_ARG0             # OUT_32_ARG0 <- CCCC
    movl    rINST, %eax                     # eax <- BA
    sarl    $4, %eax                       # eax <- B
    leaq    VREG_ADDRESS(%rax), OUT_ARG1    # Get object address
    movq    OFF_FP_METHOD(rFP), OUT_ARG2
    movq    rSELF, OUT_ARG3
    call    SYMBOL(MterpInstanceOf)         # (index, &obj, method, self)
    movsbl  %al, %eax
    movq    rSELF, %rcx
    cmpq    $0, THREAD_EXCEPTION_OFFSET(%rcx)
    jnz     MterpException
    andb    $0xf, rINSTbl                  # rINSTbl <- A
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_instance_of

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_array_length: /* 0x21 */
    ENTRY mterp_op_array_length
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Return the length of an array.
 */
    movl    rINST, %eax                     # eax <- BA
    sarl    $4, rINST                      # rINST <- B
    GET_VREG %ecx, rINSTq                   # ecx <- vB (object ref)
    testl   %ecx, %ecx                      # is null?
    je      common_errNullObject
    andb    $0xf, %al                      # eax <- A
    movl    MIRROR_ARRAY_LENGTH_OFFSET(%rcx), rINST
    SET_VREG rINST, %rax
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_array_length

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_new_instance: /* 0x22 */
    ENTRY mterp_op_new_instance
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Create a new instance of a class.
 */
    /* new-instance vAA, class@BBBB */
    EXPORT_PC
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG0
    movq    rSELF, OUT_ARG1
    REFRESH_INST 34
    movq    rINSTq, OUT_ARG2
    call    SYMBOL(MterpNewInstance)
    testb   %al, %al                        # 0 means an exception is thrown
    jz      MterpPossibleException
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_new_instance

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_new_array: /* 0x23 */
    ENTRY mterp_op_new_array
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Allocate an array of objects, specified with the array class
 * and a count.
 *
 * The verifier guarantees that this is an array class, so we don't
 * check for it here.
 */
    /* new-array vA, vB, class@CCCC */
    EXPORT_PC
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG0
    movq    rPC, OUT_ARG1
    REFRESH_INST 35
    movq    rINSTq, OUT_ARG2
    movq    rSELF, OUT_ARG3
    call    SYMBOL(MterpNewArray)
    testb   %al, %al                        # 0 means an exception is thrown
    jz      MterpPossibleException
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
    END mterp_op_new_array

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_filled_new_array: /* 0x24 */
    ENTRY mterp_op_filled_new_array
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Create a new array with elements filled from registers.
 *
 * for: filled-new-array, filled-new-array/range
 */
    /* op vB, {vD, vE, vF, vG, vA}, class@CCCC */
    /* op {vCCCC..v(CCCC+AA-1)}, type@BBBB */
    .extern MterpFilledNewArray
    EXPORT_PC
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG0
    movq    rPC, OUT_ARG1
    movq    rSELF, OUT_ARG2
    call    SYMBOL(MterpFilledNewArray)
    testb   %al, %al                        # 0 means an exception is thrown
    jz      MterpPossibleException
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 3

    END mterp_op_filled_new_array

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_filled_new_array_range: /* 0x25 */
    ENTRY mterp_op_filled_new_array_range
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Create a new array with elements filled from registers.
 *
 * for: filled-new-array, filled-new-array/range
 */
    /* op vB, {vD, vE, vF, vG, vA}, class@CCCC */
    /* op {vCCCC..v(CCCC+AA-1)}, type@BBBB */
    .extern MterpFilledNewArrayRange
    EXPORT_PC
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG0
    movq    rPC, OUT_ARG1
    movq    rSELF, OUT_ARG2
    call    SYMBOL(MterpFilledNewArrayRange)
    testb   %al, %al                        # 0 means an exception is thrown
    jz      MterpPossibleException
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 3

    END mterp_op_filled_new_array_range

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_fill_array_data: /* 0x26 */
    ENTRY mterp_op_fill_array_data
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /* fill-array-data vAA, +BBBBBBBB */
    EXPORT_PC
    movslq  2(rPC), %rcx                    # rcx <- ssssssssBBBBbbbb
    leaq    (rPC,%rcx,2), OUT_ARG1          # OUT_ARG1 <- PC + ssssssssBBBBbbbb*2
    GET_VREG OUT_32_ARG0, rINSTq            # OUT_ARG0 <- vAA (array object)
    call    SYMBOL(MterpFillArrayData)      # (obj, payload)
    testb   %al, %al                        # 0 means an exception is thrown
    jz      MterpPossibleException
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 3

    END mterp_op_fill_array_data

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_throw: /* 0x27 */
    ENTRY mterp_op_throw
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Throw an exception object in the current thread.
 */
    /* throw vAA */
    EXPORT_PC
    GET_VREG %eax, rINSTq                   # eax<- vAA (exception object)
    testb   %al, %al
    jz      common_errNullObject
    movq    rSELF, %rcx
    movq    %rax, THREAD_EXCEPTION_OFFSET(%rcx)
    jmp     MterpException
    END mterp_op_throw

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_goto: /* 0x28 */
    ENTRY mterp_op_goto
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Unconditional branch, 8-bit offset.
 *
 * The branch distance is a signed code-unit offset, which we need to
 * double to get a byte offset.
 */
    /* goto +AA */
    movsbq  rINSTbl, rINSTq                 # rINSTq <- ssssssAA
    testq   rINSTq, rINSTq
    jmp     MterpCommonTakenBranch

    END mterp_op_goto

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_goto_16: /* 0x29 */
    ENTRY mterp_op_goto_16
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Unconditional branch, 16-bit offset.
 *
 * The branch distance is a signed code-unit offset, which we need to
 * double to get a byte offset.
 */
    /* goto/16 +AAAA */
    movswq  2(rPC), rINSTq                  # rINSTq <- ssssAAAA
    testq   rINSTq, rINSTq
    jmp     MterpCommonTakenBranch

    END mterp_op_goto_16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_goto_32: /* 0x2a */
    ENTRY mterp_op_goto_32
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Unconditional branch, 32-bit offset.
 *
 * The branch distance is a signed code-unit offset, which we need to
 * double to get a byte offset.
 *
 *  Because we need the SF bit set, we'll use an adds
 * to convert from Dalvik offset to byte offset.
 */
    /* goto/32 +AAAAAAAA */
    movslq  2(rPC), rINSTq                  # rINSTq <- AAAAAAAA
    testq   rINSTq, rINSTq
    jmp     MterpCommonTakenBranch

    END mterp_op_goto_32

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_packed_switch: /* 0x2b */
    ENTRY mterp_op_packed_switch
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Handle a packed-switch or sparse-switch instruction.  In both cases
 * we decode it and hand it off to a helper function.
 *
 * We don't really expect backward branches in a switch statement, but
 * they're perfectly legal, so we check for them here.
 *
 * for: packed-switch, sparse-switch
 */
    /* op vAA, +BBBB */
    movslq  2(rPC), OUT_ARG0                # rcx <- ssssssssBBBBbbbb
    leaq    (rPC,OUT_ARG0,2), OUT_ARG0      # rcx <- PC + ssssssssBBBBbbbb*2
    GET_VREG OUT_32_ARG1, rINSTq            # eax <- vAA
    call    SYMBOL(MterpDoPackedSwitch)
    testl   %eax, %eax
    movslq  %eax, rINSTq
    jmp     MterpCommonTakenBranch

    END mterp_op_packed_switch

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sparse_switch: /* 0x2c */
    ENTRY mterp_op_sparse_switch
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Handle a packed-switch or sparse-switch instruction.  In both cases
 * we decode it and hand it off to a helper function.
 *
 * We don't really expect backward branches in a switch statement, but
 * they're perfectly legal, so we check for them here.
 *
 * for: packed-switch, sparse-switch
 */
    /* op vAA, +BBBB */
    movslq  2(rPC), OUT_ARG0                # rcx <- ssssssssBBBBbbbb
    leaq    (rPC,OUT_ARG0,2), OUT_ARG0      # rcx <- PC + ssssssssBBBBbbbb*2
    GET_VREG OUT_32_ARG1, rINSTq            # eax <- vAA
    call    SYMBOL(MterpDoSparseSwitch)
    testl   %eax, %eax
    movslq  %eax, rINSTq
    jmp     MterpCommonTakenBranch

    END mterp_op_sparse_switch

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_cmpl_float: /* 0x2d */
    ENTRY mterp_op_cmpl_float
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Compare two floating-point values.  Puts 0, 1, or -1 into the
 * destination register based on the results of the comparison.
 *
 * int compare(x, y) {
 *     if (x == y) {
 *         return 0;
 *     } else if (x < y) {
 *         return -1;
 *     } else if (x > y) {
 *         return 1;
 *     } else {
 *         return nanval ? 1 : -1;
 *     }
 * }
 */
    /* op vAA, vBB, vCC */
    movzbq  3(rPC), %rcx                    # ecx<- CC
    movzbq  2(rPC), %rax                    # eax<- BB
    GET_VREG_XMMs %xmm0, %rax
    xor     %eax, %eax
    ucomiss VREG_ADDRESS(%rcx), %xmm0
    jp      .Lop_cmpl_float_nan_is_neg
    je      .Lop_cmpl_float_finish
    jb      .Lop_cmpl_float_less
.Lop_cmpl_float_nan_is_pos:
    addb    $1, %al
    jmp     .Lop_cmpl_float_finish
.Lop_cmpl_float_nan_is_neg:
.Lop_cmpl_float_less:
    movl    $-1, %eax
.Lop_cmpl_float_finish:
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_cmpl_float

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_cmpg_float: /* 0x2e */
    ENTRY mterp_op_cmpg_float
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Compare two floating-point values.  Puts 0, 1, or -1 into the
 * destination register based on the results of the comparison.
 *
 * int compare(x, y) {
 *     if (x == y) {
 *         return 0;
 *     } else if (x < y) {
 *         return -1;
 *     } else if (x > y) {
 *         return 1;
 *     } else {
 *         return nanval ? 1 : -1;
 *     }
 * }
 */
    /* op vAA, vBB, vCC */
    movzbq  3(rPC), %rcx                    # ecx<- CC
    movzbq  2(rPC), %rax                    # eax<- BB
    GET_VREG_XMMs %xmm0, %rax
    xor     %eax, %eax
    ucomiss VREG_ADDRESS(%rcx), %xmm0
    jp      .Lop_cmpg_float_nan_is_pos
    je      .Lop_cmpg_float_finish
    jb      .Lop_cmpg_float_less
.Lop_cmpg_float_nan_is_pos:
    addb    $1, %al
    jmp     .Lop_cmpg_float_finish
.Lop_cmpg_float_nan_is_neg:
.Lop_cmpg_float_less:
    movl    $-1, %eax
.Lop_cmpg_float_finish:
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_cmpg_float

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_cmpl_double: /* 0x2f */
    ENTRY mterp_op_cmpl_double
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Compare two floating-point values.  Puts 0, 1, or -1 into the
 * destination register based on the results of the comparison.
 *
 * int compare(x, y) {
 *     if (x == y) {
 *         return 0;
 *     } else if (x < y) {
 *         return -1;
 *     } else if (x > y) {
 *         return 1;
 *     } else {
 *         return nanval ? 1 : -1;
 *     }
 * }
 */
    /* op vAA, vBB, vCC */
    movzbq  3(rPC), %rcx                    # ecx<- CC
    movzbq  2(rPC), %rax                    # eax<- BB
    GET_VREG_XMMd %xmm0, %rax
    xor     %eax, %eax
    ucomisd VREG_ADDRESS(%rcx), %xmm0
    jp      .Lop_cmpl_double_nan_is_neg
    je      .Lop_cmpl_double_finish
    jb      .Lop_cmpl_double_less
.Lop_cmpl_double_nan_is_pos:
    addb    $1, %al
    jmp     .Lop_cmpl_double_finish
.Lop_cmpl_double_nan_is_neg:
.Lop_cmpl_double_less:
    movl    $-1, %eax
.Lop_cmpl_double_finish:
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_cmpl_double

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_cmpg_double: /* 0x30 */
    ENTRY mterp_op_cmpg_double
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Compare two floating-point values.  Puts 0, 1, or -1 into the
 * destination register based on the results of the comparison.
 *
 * int compare(x, y) {
 *     if (x == y) {
 *         return 0;
 *     } else if (x < y) {
 *         return -1;
 *     } else if (x > y) {
 *         return 1;
 *     } else {
 *         return nanval ? 1 : -1;
 *     }
 * }
 */
    /* op vAA, vBB, vCC */
    movzbq  3(rPC), %rcx                    # ecx<- CC
    movzbq  2(rPC), %rax                    # eax<- BB
    GET_VREG_XMMd %xmm0, %rax
    xor     %eax, %eax
    ucomisd VREG_ADDRESS(%rcx), %xmm0
    jp      .Lop_cmpg_double_nan_is_pos
    je      .Lop_cmpg_double_finish
    jb      .Lop_cmpg_double_less
.Lop_cmpg_double_nan_is_pos:
    addb    $1, %al
    jmp     .Lop_cmpg_double_finish
.Lop_cmpg_double_nan_is_neg:
.Lop_cmpg_double_less:
    movl    $-1, %eax
.Lop_cmpg_double_finish:
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_cmpg_double

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_cmp_long: /* 0x31 */
    ENTRY mterp_op_cmp_long
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Compare two 64-bit values.  Puts 0, 1, or -1 into the destination
 * register based on the results of the comparison.
 */
    /* cmp-long vAA, vBB, vCC */
    movzbq  2(rPC), %rdx                    # edx <- BB
    movzbq  3(rPC), %rcx                    # ecx <- CC
    GET_WIDE_VREG %rdx, %rdx                # rdx <- v[BB]
    xorl    %eax, %eax
    xorl    %edi, %edi
    addb    $1, %al
    movl    $-1, %esi
    cmpq    VREG_ADDRESS(%rcx), %rdx
    cmovl   %esi, %edi
    cmovg   %eax, %edi
    SET_VREG %edi, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_cmp_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_if_eq: /* 0x32 */
    ENTRY mterp_op_if_eq
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic two-operand compare-and-branch operation.  Provide a "revcmp"
 * fragment that specifies the *reverse* comparison to perform, e.g.
 * for "if-le" you would use "gt".
 *
 * For: if-eq, if-ne, if-lt, if-ge, if-gt, if-le
 */
    /* if-cmp vA, vB, +CCCC */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # rcx <- A
    GET_VREG %eax, %rcx                     # eax <- vA
    cmpl    VREG_ADDRESS(rINSTq), %eax      # compare (vA, vB)
    jne   1f
    movswq  2(rPC), rINSTq                  # Get signed branch offset
    testq   rINSTq, rINSTq
    jmp     MterpCommonTakenBranch
1:
    cmpl    $JIT_CHECK_OSR, rPROFILE
    je      .L_check_not_taken_osr
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_if_eq

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_if_ne: /* 0x33 */
    ENTRY mterp_op_if_ne
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic two-operand compare-and-branch operation.  Provide a "revcmp"
 * fragment that specifies the *reverse* comparison to perform, e.g.
 * for "if-le" you would use "gt".
 *
 * For: if-eq, if-ne, if-lt, if-ge, if-gt, if-le
 */
    /* if-cmp vA, vB, +CCCC */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # rcx <- A
    GET_VREG %eax, %rcx                     # eax <- vA
    cmpl    VREG_ADDRESS(rINSTq), %eax      # compare (vA, vB)
    je   1f
    movswq  2(rPC), rINSTq                  # Get signed branch offset
    testq   rINSTq, rINSTq
    jmp     MterpCommonTakenBranch
1:
    cmpl    $JIT_CHECK_OSR, rPROFILE
    je      .L_check_not_taken_osr
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_if_ne

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_if_lt: /* 0x34 */
    ENTRY mterp_op_if_lt
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic two-operand compare-and-branch operation.  Provide a "revcmp"
 * fragment that specifies the *reverse* comparison to perform, e.g.
 * for "if-le" you would use "gt".
 *
 * For: if-eq, if-ne, if-lt, if-ge, if-gt, if-le
 */
    /* if-cmp vA, vB, +CCCC */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # rcx <- A
    GET_VREG %eax, %rcx                     # eax <- vA
    cmpl    VREG_ADDRESS(rINSTq), %eax      # compare (vA, vB)
    jge   1f
    movswq  2(rPC), rINSTq                  # Get signed branch offset
    testq   rINSTq, rINSTq
    jmp     MterpCommonTakenBranch
1:
    cmpl    $JIT_CHECK_OSR, rPROFILE
    je      .L_check_not_taken_osr
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_if_lt

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_if_ge: /* 0x35 */
    ENTRY mterp_op_if_ge
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic two-operand compare-and-branch operation.  Provide a "revcmp"
 * fragment that specifies the *reverse* comparison to perform, e.g.
 * for "if-le" you would use "gt".
 *
 * For: if-eq, if-ne, if-lt, if-ge, if-gt, if-le
 */
    /* if-cmp vA, vB, +CCCC */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # rcx <- A
    GET_VREG %eax, %rcx                     # eax <- vA
    cmpl    VREG_ADDRESS(rINSTq), %eax      # compare (vA, vB)
    jl   1f
    movswq  2(rPC), rINSTq                  # Get signed branch offset
    testq   rINSTq, rINSTq
    jmp     MterpCommonTakenBranch
1:
    cmpl    $JIT_CHECK_OSR, rPROFILE
    je      .L_check_not_taken_osr
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_if_ge

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_if_gt: /* 0x36 */
    ENTRY mterp_op_if_gt
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic two-operand compare-and-branch operation.  Provide a "revcmp"
 * fragment that specifies the *reverse* comparison to perform, e.g.
 * for "if-le" you would use "gt".
 *
 * For: if-eq, if-ne, if-lt, if-ge, if-gt, if-le
 */
    /* if-cmp vA, vB, +CCCC */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # rcx <- A
    GET_VREG %eax, %rcx                     # eax <- vA
    cmpl    VREG_ADDRESS(rINSTq), %eax      # compare (vA, vB)
    jle   1f
    movswq  2(rPC), rINSTq                  # Get signed branch offset
    testq   rINSTq, rINSTq
    jmp     MterpCommonTakenBranch
1:
    cmpl    $JIT_CHECK_OSR, rPROFILE
    je      .L_check_not_taken_osr
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_if_gt

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_if_le: /* 0x37 */
    ENTRY mterp_op_if_le
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic two-operand compare-and-branch operation.  Provide a "revcmp"
 * fragment that specifies the *reverse* comparison to perform, e.g.
 * for "if-le" you would use "gt".
 *
 * For: if-eq, if-ne, if-lt, if-ge, if-gt, if-le
 */
    /* if-cmp vA, vB, +CCCC */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # rcx <- A
    GET_VREG %eax, %rcx                     # eax <- vA
    cmpl    VREG_ADDRESS(rINSTq), %eax      # compare (vA, vB)
    jg   1f
    movswq  2(rPC), rINSTq                  # Get signed branch offset
    testq   rINSTq, rINSTq
    jmp     MterpCommonTakenBranch
1:
    cmpl    $JIT_CHECK_OSR, rPROFILE
    je      .L_check_not_taken_osr
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_if_le

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_if_eqz: /* 0x38 */
    ENTRY mterp_op_if_eqz
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic one-operand compare-and-branch operation.  Provide a "revcmp"
 * fragment that specifies the *reverse* comparison to perform, e.g.
 * for "if-le" you would use "gt".
 *
 * for: if-eqz, if-nez, if-ltz, if-gez, if-gtz, if-lez
 */
    /* if-cmp vAA, +BBBB */
    cmpl    $0, VREG_ADDRESS(rINSTq)       # compare (vA, 0)
    jne   1f
    movswq  2(rPC), rINSTq                  # fetch signed displacement
    testq   rINSTq, rINSTq
    jmp     MterpCommonTakenBranch
1:
    cmpl    $JIT_CHECK_OSR, rPROFILE
    je      .L_check_not_taken_osr
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_if_eqz

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_if_nez: /* 0x39 */
    ENTRY mterp_op_if_nez
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic one-operand compare-and-branch operation.  Provide a "revcmp"
 * fragment that specifies the *reverse* comparison to perform, e.g.
 * for "if-le" you would use "gt".
 *
 * for: if-eqz, if-nez, if-ltz, if-gez, if-gtz, if-lez
 */
    /* if-cmp vAA, +BBBB */
    cmpl    $0, VREG_ADDRESS(rINSTq)       # compare (vA, 0)
    je   1f
    movswq  2(rPC), rINSTq                  # fetch signed displacement
    testq   rINSTq, rINSTq
    jmp     MterpCommonTakenBranch
1:
    cmpl    $JIT_CHECK_OSR, rPROFILE
    je      .L_check_not_taken_osr
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_if_nez

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_if_ltz: /* 0x3a */
    ENTRY mterp_op_if_ltz
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic one-operand compare-and-branch operation.  Provide a "revcmp"
 * fragment that specifies the *reverse* comparison to perform, e.g.
 * for "if-le" you would use "gt".
 *
 * for: if-eqz, if-nez, if-ltz, if-gez, if-gtz, if-lez
 */
    /* if-cmp vAA, +BBBB */
    cmpl    $0, VREG_ADDRESS(rINSTq)       # compare (vA, 0)
    jge   1f
    movswq  2(rPC), rINSTq                  # fetch signed displacement
    testq   rINSTq, rINSTq
    jmp     MterpCommonTakenBranch
1:
    cmpl    $JIT_CHECK_OSR, rPROFILE
    je      .L_check_not_taken_osr
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_if_ltz

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_if_gez: /* 0x3b */
    ENTRY mterp_op_if_gez
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic one-operand compare-and-branch operation.  Provide a "revcmp"
 * fragment that specifies the *reverse* comparison to perform, e.g.
 * for "if-le" you would use "gt".
 *
 * for: if-eqz, if-nez, if-ltz, if-gez, if-gtz, if-lez
 */
    /* if-cmp vAA, +BBBB */
    cmpl    $0, VREG_ADDRESS(rINSTq)       # compare (vA, 0)
    jl   1f
    movswq  2(rPC), rINSTq                  # fetch signed displacement
    testq   rINSTq, rINSTq
    jmp     MterpCommonTakenBranch
1:
    cmpl    $JIT_CHECK_OSR, rPROFILE
    je      .L_check_not_taken_osr
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_if_gez

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_if_gtz: /* 0x3c */
    ENTRY mterp_op_if_gtz
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic one-operand compare-and-branch operation.  Provide a "revcmp"
 * fragment that specifies the *reverse* comparison to perform, e.g.
 * for "if-le" you would use "gt".
 *
 * for: if-eqz, if-nez, if-ltz, if-gez, if-gtz, if-lez
 */
    /* if-cmp vAA, +BBBB */
    cmpl    $0, VREG_ADDRESS(rINSTq)       # compare (vA, 0)
    jle   1f
    movswq  2(rPC), rINSTq                  # fetch signed displacement
    testq   rINSTq, rINSTq
    jmp     MterpCommonTakenBranch
1:
    cmpl    $JIT_CHECK_OSR, rPROFILE
    je      .L_check_not_taken_osr
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_if_gtz

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_if_lez: /* 0x3d */
    ENTRY mterp_op_if_lez
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic one-operand compare-and-branch operation.  Provide a "revcmp"
 * fragment that specifies the *reverse* comparison to perform, e.g.
 * for "if-le" you would use "gt".
 *
 * for: if-eqz, if-nez, if-ltz, if-gez, if-gtz, if-lez
 */
    /* if-cmp vAA, +BBBB */
    cmpl    $0, VREG_ADDRESS(rINSTq)       # compare (vA, 0)
    jg   1f
    movswq  2(rPC), rINSTq                  # fetch signed displacement
    testq   rINSTq, rINSTq
    jmp     MterpCommonTakenBranch
1:
    cmpl    $JIT_CHECK_OSR, rPROFILE
    je      .L_check_not_taken_osr
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_if_lez

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_3e: /* 0x3e */
    ENTRY mterp_op_unused_3e
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Bail to reference interpreter to throw.
 */
    jmp     MterpFallback

    END mterp_op_unused_3e

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_3f: /* 0x3f */
    ENTRY mterp_op_unused_3f
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Bail to reference interpreter to throw.
 */
    jmp     MterpFallback

    END mterp_op_unused_3f

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_40: /* 0x40 */
    ENTRY mterp_op_unused_40
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Bail to reference interpreter to throw.
 */
    jmp     MterpFallback

    END mterp_op_unused_40

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_41: /* 0x41 */
    ENTRY mterp_op_unused_41
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Bail to reference interpreter to throw.
 */
    jmp     MterpFallback

    END mterp_op_unused_41

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_42: /* 0x42 */
    ENTRY mterp_op_unused_42
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Bail to reference interpreter to throw.
 */
    jmp     MterpFallback

    END mterp_op_unused_42

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_43: /* 0x43 */
    ENTRY mterp_op_unused_43
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Bail to reference interpreter to throw.
 */
    jmp     MterpFallback

    END mterp_op_unused_43

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aget: /* 0x44 */
    ENTRY mterp_op_aget
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Array get, 32 bits or less.  vAA <- vBB[vCC].
 *
 * for: aget, aget-boolean, aget-byte, aget-char, aget-short, aget-wide
 *
 */
    /* op vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # eax <- BB
    movzbq  3(rPC), %rcx                    # ecx <- CC
    GET_VREG %eax, %rax                     # eax <- vBB (array object)
    GET_VREG %ecx, %rcx                     # ecx <- vCC (requested index)
    testl   %eax, %eax                      # null array object?
    je      common_errNullObject            # bail if so
    cmpl    MIRROR_ARRAY_LENGTH_OFFSET(%eax), %ecx
    jae     common_errArrayIndex            # index >= length, bail.
    .if 0
    movq    MIRROR_INT_ARRAY_DATA_OFFSET(%rax,%rcx,8), %rax
    SET_WIDE_VREG %rax, rINSTq
    .else
    movl   MIRROR_INT_ARRAY_DATA_OFFSET(%rax,%rcx,4), %eax
    SET_VREG %eax, rINSTq
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_aget

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aget_wide: /* 0x45 */
    ENTRY mterp_op_aget_wide
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Array get, 32 bits or less.  vAA <- vBB[vCC].
 *
 * for: aget, aget-boolean, aget-byte, aget-char, aget-short, aget-wide
 *
 */
    /* op vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # eax <- BB
    movzbq  3(rPC), %rcx                    # ecx <- CC
    GET_VREG %eax, %rax                     # eax <- vBB (array object)
    GET_VREG %ecx, %rcx                     # ecx <- vCC (requested index)
    testl   %eax, %eax                      # null array object?
    je      common_errNullObject            # bail if so
    cmpl    MIRROR_ARRAY_LENGTH_OFFSET(%eax), %ecx
    jae     common_errArrayIndex            # index >= length, bail.
    .if 1
    movq    MIRROR_WIDE_ARRAY_DATA_OFFSET(%rax,%rcx,8), %rax
    SET_WIDE_VREG %rax, rINSTq
    .else
    movq   MIRROR_WIDE_ARRAY_DATA_OFFSET(%rax,%rcx,8), %eax
    SET_VREG %eax, rINSTq
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_aget_wide

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aget_object: /* 0x46 */
    ENTRY mterp_op_aget_object
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Array object get.  vAA <- vBB[vCC].
 *
 * for: aget-object
 */
    /* op vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # rax <- BB
    movzbq  3(rPC), %rcx                    # rcx <- CC
    GET_VREG OUT_32_ARG0, %rax              # eax <- vBB (array object)
    GET_VREG OUT_32_ARG1, %rcx              # ecx <- vCC (requested index)
    EXPORT_PC
    call    SYMBOL(artAGetObjectFromMterp)  # (array, index)
    movq    rSELF, %rcx
    cmpq    $0, THREAD_EXCEPTION_OFFSET(%rcx)
    jnz     MterpException
    SET_VREG_OBJECT %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_aget_object

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aget_boolean: /* 0x47 */
    ENTRY mterp_op_aget_boolean
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Array get, 32 bits or less.  vAA <- vBB[vCC].
 *
 * for: aget, aget-boolean, aget-byte, aget-char, aget-short, aget-wide
 *
 */
    /* op vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # eax <- BB
    movzbq  3(rPC), %rcx                    # ecx <- CC
    GET_VREG %eax, %rax                     # eax <- vBB (array object)
    GET_VREG %ecx, %rcx                     # ecx <- vCC (requested index)
    testl   %eax, %eax                      # null array object?
    je      common_errNullObject            # bail if so
    cmpl    MIRROR_ARRAY_LENGTH_OFFSET(%eax), %ecx
    jae     common_errArrayIndex            # index >= length, bail.
    .if 0
    movq    MIRROR_BOOLEAN_ARRAY_DATA_OFFSET(%rax,%rcx,8), %rax
    SET_WIDE_VREG %rax, rINSTq
    .else
    movzbl   MIRROR_BOOLEAN_ARRAY_DATA_OFFSET(%rax,%rcx,1), %eax
    SET_VREG %eax, rINSTq
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_aget_boolean

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aget_byte: /* 0x48 */
    ENTRY mterp_op_aget_byte
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Array get, 32 bits or less.  vAA <- vBB[vCC].
 *
 * for: aget, aget-boolean, aget-byte, aget-char, aget-short, aget-wide
 *
 */
    /* op vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # eax <- BB
    movzbq  3(rPC), %rcx                    # ecx <- CC
    GET_VREG %eax, %rax                     # eax <- vBB (array object)
    GET_VREG %ecx, %rcx                     # ecx <- vCC (requested index)
    testl   %eax, %eax                      # null array object?
    je      common_errNullObject            # bail if so
    cmpl    MIRROR_ARRAY_LENGTH_OFFSET(%eax), %ecx
    jae     common_errArrayIndex            # index >= length, bail.
    .if 0
    movq    MIRROR_BYTE_ARRAY_DATA_OFFSET(%rax,%rcx,8), %rax
    SET_WIDE_VREG %rax, rINSTq
    .else
    movsbl   MIRROR_BYTE_ARRAY_DATA_OFFSET(%rax,%rcx,1), %eax
    SET_VREG %eax, rINSTq
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_aget_byte

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aget_char: /* 0x49 */
    ENTRY mterp_op_aget_char
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Array get, 32 bits or less.  vAA <- vBB[vCC].
 *
 * for: aget, aget-boolean, aget-byte, aget-char, aget-short, aget-wide
 *
 */
    /* op vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # eax <- BB
    movzbq  3(rPC), %rcx                    # ecx <- CC
    GET_VREG %eax, %rax                     # eax <- vBB (array object)
    GET_VREG %ecx, %rcx                     # ecx <- vCC (requested index)
    testl   %eax, %eax                      # null array object?
    je      common_errNullObject            # bail if so
    cmpl    MIRROR_ARRAY_LENGTH_OFFSET(%eax), %ecx
    jae     common_errArrayIndex            # index >= length, bail.
    .if 0
    movq    MIRROR_CHAR_ARRAY_DATA_OFFSET(%rax,%rcx,8), %rax
    SET_WIDE_VREG %rax, rINSTq
    .else
    movzwl   MIRROR_CHAR_ARRAY_DATA_OFFSET(%rax,%rcx,2), %eax
    SET_VREG %eax, rINSTq
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_aget_char

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aget_short: /* 0x4a */
    ENTRY mterp_op_aget_short
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Array get, 32 bits or less.  vAA <- vBB[vCC].
 *
 * for: aget, aget-boolean, aget-byte, aget-char, aget-short, aget-wide
 *
 */
    /* op vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # eax <- BB
    movzbq  3(rPC), %rcx                    # ecx <- CC
    GET_VREG %eax, %rax                     # eax <- vBB (array object)
    GET_VREG %ecx, %rcx                     # ecx <- vCC (requested index)
    testl   %eax, %eax                      # null array object?
    je      common_errNullObject            # bail if so
    cmpl    MIRROR_ARRAY_LENGTH_OFFSET(%eax), %ecx
    jae     common_errArrayIndex            # index >= length, bail.
    .if 0
    movq    MIRROR_SHORT_ARRAY_DATA_OFFSET(%rax,%rcx,8), %rax
    SET_WIDE_VREG %rax, rINSTq
    .else
    movswl   MIRROR_SHORT_ARRAY_DATA_OFFSET(%rax,%rcx,2), %eax
    SET_VREG %eax, rINSTq
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_aget_short

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aput: /* 0x4b */
    ENTRY mterp_op_aput
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Array put, 32 bits or less.  vBB[vCC] <- vAA.
 *
 * for: aput, aput-boolean, aput-byte, aput-char, aput-short, aput-wide
 *
 */
    /* op vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # rax <- BB
    movzbq  3(rPC), %rcx                    # rcx <- CC
    GET_VREG %eax, %rax                     # eax <- vBB (array object)
    GET_VREG %ecx, %rcx                     # ecx <- vCC (requested index)
    testl   %eax, %eax                      # null array object?
    je      common_errNullObject            # bail if so
    cmpl    MIRROR_ARRAY_LENGTH_OFFSET(%eax), %ecx
    jae     common_errArrayIndex            # index >= length, bail.
    .if 0
    GET_WIDE_VREG rINSTq, rINSTq
    .else
    GET_VREG rINST, rINSTq
    .endif
    movl    rINST, MIRROR_INT_ARRAY_DATA_OFFSET(%rax,%rcx,4)
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_aput

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aput_wide: /* 0x4c */
    ENTRY mterp_op_aput_wide
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Array put, 32 bits or less.  vBB[vCC] <- vAA.
 *
 * for: aput, aput-boolean, aput-byte, aput-char, aput-short, aput-wide
 *
 */
    /* op vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # rax <- BB
    movzbq  3(rPC), %rcx                    # rcx <- CC
    GET_VREG %eax, %rax                     # eax <- vBB (array object)
    GET_VREG %ecx, %rcx                     # ecx <- vCC (requested index)
    testl   %eax, %eax                      # null array object?
    je      common_errNullObject            # bail if so
    cmpl    MIRROR_ARRAY_LENGTH_OFFSET(%eax), %ecx
    jae     common_errArrayIndex            # index >= length, bail.
    .if 1
    GET_WIDE_VREG rINSTq, rINSTq
    .else
    GET_VREG rINST, rINSTq
    .endif
    movq    rINSTq, MIRROR_WIDE_ARRAY_DATA_OFFSET(%rax,%rcx,8)
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_aput_wide

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aput_object: /* 0x4d */
    ENTRY mterp_op_aput_object
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Store an object into an array.  vBB[vCC] <- vAA.
 */
    /* op vAA, vBB, vCC */
    EXPORT_PC
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG0
    movq    rPC, OUT_ARG1
    REFRESH_INST 77
    movq    rINSTq, OUT_ARG2
    call    SYMBOL(MterpAputObject)         # (array, index)
    testb   %al, %al
    jz      MterpPossibleException
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_aput_object

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aput_boolean: /* 0x4e */
    ENTRY mterp_op_aput_boolean
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Array put, 32 bits or less.  vBB[vCC] <- vAA.
 *
 * for: aput, aput-boolean, aput-byte, aput-char, aput-short, aput-wide
 *
 */
    /* op vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # rax <- BB
    movzbq  3(rPC), %rcx                    # rcx <- CC
    GET_VREG %eax, %rax                     # eax <- vBB (array object)
    GET_VREG %ecx, %rcx                     # ecx <- vCC (requested index)
    testl   %eax, %eax                      # null array object?
    je      common_errNullObject            # bail if so
    cmpl    MIRROR_ARRAY_LENGTH_OFFSET(%eax), %ecx
    jae     common_errArrayIndex            # index >= length, bail.
    .if 0
    GET_WIDE_VREG rINSTq, rINSTq
    .else
    GET_VREG rINST, rINSTq
    .endif
    movb    rINSTbl, MIRROR_BOOLEAN_ARRAY_DATA_OFFSET(%rax,%rcx,1)
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_aput_boolean

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aput_byte: /* 0x4f */
    ENTRY mterp_op_aput_byte
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Array put, 32 bits or less.  vBB[vCC] <- vAA.
 *
 * for: aput, aput-boolean, aput-byte, aput-char, aput-short, aput-wide
 *
 */
    /* op vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # rax <- BB
    movzbq  3(rPC), %rcx                    # rcx <- CC
    GET_VREG %eax, %rax                     # eax <- vBB (array object)
    GET_VREG %ecx, %rcx                     # ecx <- vCC (requested index)
    testl   %eax, %eax                      # null array object?
    je      common_errNullObject            # bail if so
    cmpl    MIRROR_ARRAY_LENGTH_OFFSET(%eax), %ecx
    jae     common_errArrayIndex            # index >= length, bail.
    .if 0
    GET_WIDE_VREG rINSTq, rINSTq
    .else
    GET_VREG rINST, rINSTq
    .endif
    movb    rINSTbl, MIRROR_BYTE_ARRAY_DATA_OFFSET(%rax,%rcx,1)
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_aput_byte

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aput_char: /* 0x50 */
    ENTRY mterp_op_aput_char
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Array put, 32 bits or less.  vBB[vCC] <- vAA.
 *
 * for: aput, aput-boolean, aput-byte, aput-char, aput-short, aput-wide
 *
 */
    /* op vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # rax <- BB
    movzbq  3(rPC), %rcx                    # rcx <- CC
    GET_VREG %eax, %rax                     # eax <- vBB (array object)
    GET_VREG %ecx, %rcx                     # ecx <- vCC (requested index)
    testl   %eax, %eax                      # null array object?
    je      common_errNullObject            # bail if so
    cmpl    MIRROR_ARRAY_LENGTH_OFFSET(%eax), %ecx
    jae     common_errArrayIndex            # index >= length, bail.
    .if 0
    GET_WIDE_VREG rINSTq, rINSTq
    .else
    GET_VREG rINST, rINSTq
    .endif
    movw    rINSTw, MIRROR_CHAR_ARRAY_DATA_OFFSET(%rax,%rcx,2)
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_aput_char

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aput_short: /* 0x51 */
    ENTRY mterp_op_aput_short
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Array put, 32 bits or less.  vBB[vCC] <- vAA.
 *
 * for: aput, aput-boolean, aput-byte, aput-char, aput-short, aput-wide
 *
 */
    /* op vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # rax <- BB
    movzbq  3(rPC), %rcx                    # rcx <- CC
    GET_VREG %eax, %rax                     # eax <- vBB (array object)
    GET_VREG %ecx, %rcx                     # ecx <- vCC (requested index)
    testl   %eax, %eax                      # null array object?
    je      common_errNullObject            # bail if so
    cmpl    MIRROR_ARRAY_LENGTH_OFFSET(%eax), %ecx
    jae     common_errArrayIndex            # index >= length, bail.
    .if 0
    GET_WIDE_VREG rINSTq, rINSTq
    .else
    GET_VREG rINST, rINSTq
    .endif
    movw    rINSTw, MIRROR_SHORT_ARRAY_DATA_OFFSET(%rax,%rcx,2)
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_aput_short

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iget: /* 0x52 */
    ENTRY mterp_op_iget
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /*
     * General field read / write (iget-* iput-* sget-* sput-*).
     */
    .extern MterpIGetU32
    REFRESH_INST 82                      # fix rINST to include opcode
    movq    rPC, OUT_ARG0                      # arg0: Instruction* inst
    movl    rINST, OUT_32_ARG1                 # arg1: uint16_t inst_data
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG2  # arg2: ShadowFrame* sf
    movq    rSELF, OUT_ARG3                    # arg3: Thread* self
    call    SYMBOL(MterpIGetU32)
    testb   %al, %al
    jz      MterpPossibleException
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_iget

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iget_wide: /* 0x53 */
    ENTRY mterp_op_iget_wide
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /*
     * General field read / write (iget-* iput-* sget-* sput-*).
     */
    .extern MterpIGetU64
    REFRESH_INST 83                      # fix rINST to include opcode
    movq    rPC, OUT_ARG0                      # arg0: Instruction* inst
    movl    rINST, OUT_32_ARG1                 # arg1: uint16_t inst_data
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG2  # arg2: ShadowFrame* sf
    movq    rSELF, OUT_ARG3                    # arg3: Thread* self
    call    SYMBOL(MterpIGetU64)
    testb   %al, %al
    jz      MterpPossibleException
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_iget_wide

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iget_object: /* 0x54 */
    ENTRY mterp_op_iget_object
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /*
     * General field read / write (iget-* iput-* sget-* sput-*).
     */
    .extern MterpIGetObj
    REFRESH_INST 84                      # fix rINST to include opcode
    movq    rPC, OUT_ARG0                      # arg0: Instruction* inst
    movl    rINST, OUT_32_ARG1                 # arg1: uint16_t inst_data
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG2  # arg2: ShadowFrame* sf
    movq    rSELF, OUT_ARG3                    # arg3: Thread* self
    call    SYMBOL(MterpIGetObj)
    testb   %al, %al
    jz      MterpPossibleException
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_iget_object

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iget_boolean: /* 0x55 */
    ENTRY mterp_op_iget_boolean
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /*
     * General field read / write (iget-* iput-* sget-* sput-*).
     */
    .extern MterpIGetU8
    REFRESH_INST 85                      # fix rINST to include opcode
    movq    rPC, OUT_ARG0                      # arg0: Instruction* inst
    movl    rINST, OUT_32_ARG1                 # arg1: uint16_t inst_data
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG2  # arg2: ShadowFrame* sf
    movq    rSELF, OUT_ARG3                    # arg3: Thread* self
    call    SYMBOL(MterpIGetU8)
    testb   %al, %al
    jz      MterpPossibleException
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_iget_boolean

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iget_byte: /* 0x56 */
    ENTRY mterp_op_iget_byte
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /*
     * General field read / write (iget-* iput-* sget-* sput-*).
     */
    .extern MterpIGetI8
    REFRESH_INST 86                      # fix rINST to include opcode
    movq    rPC, OUT_ARG0                      # arg0: Instruction* inst
    movl    rINST, OUT_32_ARG1                 # arg1: uint16_t inst_data
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG2  # arg2: ShadowFrame* sf
    movq    rSELF, OUT_ARG3                    # arg3: Thread* self
    call    SYMBOL(MterpIGetI8)
    testb   %al, %al
    jz      MterpPossibleException
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_iget_byte

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iget_char: /* 0x57 */
    ENTRY mterp_op_iget_char
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /*
     * General field read / write (iget-* iput-* sget-* sput-*).
     */
    .extern MterpIGetU16
    REFRESH_INST 87                      # fix rINST to include opcode
    movq    rPC, OUT_ARG0                      # arg0: Instruction* inst
    movl    rINST, OUT_32_ARG1                 # arg1: uint16_t inst_data
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG2  # arg2: ShadowFrame* sf
    movq    rSELF, OUT_ARG3                    # arg3: Thread* self
    call    SYMBOL(MterpIGetU16)
    testb   %al, %al
    jz      MterpPossibleException
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_iget_char

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iget_short: /* 0x58 */
    ENTRY mterp_op_iget_short
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /*
     * General field read / write (iget-* iput-* sget-* sput-*).
     */
    .extern MterpIGetI16
    REFRESH_INST 88                      # fix rINST to include opcode
    movq    rPC, OUT_ARG0                      # arg0: Instruction* inst
    movl    rINST, OUT_32_ARG1                 # arg1: uint16_t inst_data
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG2  # arg2: ShadowFrame* sf
    movq    rSELF, OUT_ARG3                    # arg3: Thread* self
    call    SYMBOL(MterpIGetI16)
    testb   %al, %al
    jz      MterpPossibleException
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_iget_short

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iput: /* 0x59 */
    ENTRY mterp_op_iput
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /*
     * General field read / write (iget-* iput-* sget-* sput-*).
     */
    .extern MterpIPutU32
    REFRESH_INST 89                      # fix rINST to include opcode
    movq    rPC, OUT_ARG0                      # arg0: Instruction* inst
    movl    rINST, OUT_32_ARG1                 # arg1: uint16_t inst_data
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG2  # arg2: ShadowFrame* sf
    movq    rSELF, OUT_ARG3                    # arg3: Thread* self
    call    SYMBOL(MterpIPutU32)
    testb   %al, %al
    jz      MterpPossibleException
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_iput

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iput_wide: /* 0x5a */
    ENTRY mterp_op_iput_wide
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /*
     * General field read / write (iget-* iput-* sget-* sput-*).
     */
    .extern MterpIPutU64
    REFRESH_INST 90                      # fix rINST to include opcode
    movq    rPC, OUT_ARG0                      # arg0: Instruction* inst
    movl    rINST, OUT_32_ARG1                 # arg1: uint16_t inst_data
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG2  # arg2: ShadowFrame* sf
    movq    rSELF, OUT_ARG3                    # arg3: Thread* self
    call    SYMBOL(MterpIPutU64)
    testb   %al, %al
    jz      MterpPossibleException
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_iput_wide

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iput_object: /* 0x5b */
    ENTRY mterp_op_iput_object
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /*
     * General field read / write (iget-* iput-* sget-* sput-*).
     */
    .extern MterpIPutObj
    REFRESH_INST 91                      # fix rINST to include opcode
    movq    rPC, OUT_ARG0                      # arg0: Instruction* inst
    movl    rINST, OUT_32_ARG1                 # arg1: uint16_t inst_data
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG2  # arg2: ShadowFrame* sf
    movq    rSELF, OUT_ARG3                    # arg3: Thread* self
    call    SYMBOL(MterpIPutObj)
    testb   %al, %al
    jz      MterpPossibleException
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_iput_object

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iput_boolean: /* 0x5c */
    ENTRY mterp_op_iput_boolean
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /*
     * General field read / write (iget-* iput-* sget-* sput-*).
     */
    .extern MterpIPutU8
    REFRESH_INST 92                      # fix rINST to include opcode
    movq    rPC, OUT_ARG0                      # arg0: Instruction* inst
    movl    rINST, OUT_32_ARG1                 # arg1: uint16_t inst_data
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG2  # arg2: ShadowFrame* sf
    movq    rSELF, OUT_ARG3                    # arg3: Thread* self
    call    SYMBOL(MterpIPutU8)
    testb   %al, %al
    jz      MterpPossibleException
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_iput_boolean

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iput_byte: /* 0x5d */
    ENTRY mterp_op_iput_byte
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /*
     * General field read / write (iget-* iput-* sget-* sput-*).
     */
    .extern MterpIPutI8
    REFRESH_INST 93                      # fix rINST to include opcode
    movq    rPC, OUT_ARG0                      # arg0: Instruction* inst
    movl    rINST, OUT_32_ARG1                 # arg1: uint16_t inst_data
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG2  # arg2: ShadowFrame* sf
    movq    rSELF, OUT_ARG3                    # arg3: Thread* self
    call    SYMBOL(MterpIPutI8)
    testb   %al, %al
    jz      MterpPossibleException
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_iput_byte

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iput_char: /* 0x5e */
    ENTRY mterp_op_iput_char
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /*
     * General field read / write (iget-* iput-* sget-* sput-*).
     */
    .extern MterpIPutU16
    REFRESH_INST 94                      # fix rINST to include opcode
    movq    rPC, OUT_ARG0                      # arg0: Instruction* inst
    movl    rINST, OUT_32_ARG1                 # arg1: uint16_t inst_data
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG2  # arg2: ShadowFrame* sf
    movq    rSELF, OUT_ARG3                    # arg3: Thread* self
    call    SYMBOL(MterpIPutU16)
    testb   %al, %al
    jz      MterpPossibleException
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_iput_char

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iput_short: /* 0x5f */
    ENTRY mterp_op_iput_short
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /*
     * General field read / write (iget-* iput-* sget-* sput-*).
     */
    .extern MterpIPutI16
    REFRESH_INST 95                      # fix rINST to include opcode
    movq    rPC, OUT_ARG0                      # arg0: Instruction* inst
    movl    rINST, OUT_32_ARG1                 # arg1: uint16_t inst_data
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG2  # arg2: ShadowFrame* sf
    movq    rSELF, OUT_ARG3                    # arg3: Thread* self
    call    SYMBOL(MterpIPutI16)
    testb   %al, %al
    jz      MterpPossibleException
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_iput_short

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sget: /* 0x60 */
    ENTRY mterp_op_sget
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /*
     * General field read / write (iget-* iput-* sget-* sput-*).
     */
    .extern MterpSGetU32
    REFRESH_INST 96                      # fix rINST to include opcode
    movq    rPC, OUT_ARG0                      # arg0: Instruction* inst
    movl    rINST, OUT_32_ARG1                 # arg1: uint16_t inst_data
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG2  # arg2: ShadowFrame* sf
    movq    rSELF, OUT_ARG3                    # arg3: Thread* self
    call    SYMBOL(MterpSGetU32)
    testb   %al, %al
    jz      MterpPossibleException
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_sget

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sget_wide: /* 0x61 */
    ENTRY mterp_op_sget_wide
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /*
     * General field read / write (iget-* iput-* sget-* sput-*).
     */
    .extern MterpSGetU64
    REFRESH_INST 97                      # fix rINST to include opcode
    movq    rPC, OUT_ARG0                      # arg0: Instruction* inst
    movl    rINST, OUT_32_ARG1                 # arg1: uint16_t inst_data
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG2  # arg2: ShadowFrame* sf
    movq    rSELF, OUT_ARG3                    # arg3: Thread* self
    call    SYMBOL(MterpSGetU64)
    testb   %al, %al
    jz      MterpPossibleException
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_sget_wide

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sget_object: /* 0x62 */
    ENTRY mterp_op_sget_object
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /*
     * General field read / write (iget-* iput-* sget-* sput-*).
     */
    .extern MterpSGetObj
    REFRESH_INST 98                      # fix rINST to include opcode
    movq    rPC, OUT_ARG0                      # arg0: Instruction* inst
    movl    rINST, OUT_32_ARG1                 # arg1: uint16_t inst_data
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG2  # arg2: ShadowFrame* sf
    movq    rSELF, OUT_ARG3                    # arg3: Thread* self
    call    SYMBOL(MterpSGetObj)
    testb   %al, %al
    jz      MterpPossibleException
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_sget_object

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sget_boolean: /* 0x63 */
    ENTRY mterp_op_sget_boolean
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /*
     * General field read / write (iget-* iput-* sget-* sput-*).
     */
    .extern MterpSGetU8
    REFRESH_INST 99                      # fix rINST to include opcode
    movq    rPC, OUT_ARG0                      # arg0: Instruction* inst
    movl    rINST, OUT_32_ARG1                 # arg1: uint16_t inst_data
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG2  # arg2: ShadowFrame* sf
    movq    rSELF, OUT_ARG3                    # arg3: Thread* self
    call    SYMBOL(MterpSGetU8)
    testb   %al, %al
    jz      MterpPossibleException
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_sget_boolean

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sget_byte: /* 0x64 */
    ENTRY mterp_op_sget_byte
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /*
     * General field read / write (iget-* iput-* sget-* sput-*).
     */
    .extern MterpSGetI8
    REFRESH_INST 100                      # fix rINST to include opcode
    movq    rPC, OUT_ARG0                      # arg0: Instruction* inst
    movl    rINST, OUT_32_ARG1                 # arg1: uint16_t inst_data
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG2  # arg2: ShadowFrame* sf
    movq    rSELF, OUT_ARG3                    # arg3: Thread* self
    call    SYMBOL(MterpSGetI8)
    testb   %al, %al
    jz      MterpPossibleException
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_sget_byte

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sget_char: /* 0x65 */
    ENTRY mterp_op_sget_char
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /*
     * General field read / write (iget-* iput-* sget-* sput-*).
     */
    .extern MterpSGetU16
    REFRESH_INST 101                      # fix rINST to include opcode
    movq    rPC, OUT_ARG0                      # arg0: Instruction* inst
    movl    rINST, OUT_32_ARG1                 # arg1: uint16_t inst_data
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG2  # arg2: ShadowFrame* sf
    movq    rSELF, OUT_ARG3                    # arg3: Thread* self
    call    SYMBOL(MterpSGetU16)
    testb   %al, %al
    jz      MterpPossibleException
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_sget_char

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sget_short: /* 0x66 */
    ENTRY mterp_op_sget_short
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /*
     * General field read / write (iget-* iput-* sget-* sput-*).
     */
    .extern MterpSGetI16
    REFRESH_INST 102                      # fix rINST to include opcode
    movq    rPC, OUT_ARG0                      # arg0: Instruction* inst
    movl    rINST, OUT_32_ARG1                 # arg1: uint16_t inst_data
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG2  # arg2: ShadowFrame* sf
    movq    rSELF, OUT_ARG3                    # arg3: Thread* self
    call    SYMBOL(MterpSGetI16)
    testb   %al, %al
    jz      MterpPossibleException
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_sget_short

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sput: /* 0x67 */
    ENTRY mterp_op_sput
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /*
     * General field read / write (iget-* iput-* sget-* sput-*).
     */
    .extern MterpSPutU32
    REFRESH_INST 103                      # fix rINST to include opcode
    movq    rPC, OUT_ARG0                      # arg0: Instruction* inst
    movl    rINST, OUT_32_ARG1                 # arg1: uint16_t inst_data
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG2  # arg2: ShadowFrame* sf
    movq    rSELF, OUT_ARG3                    # arg3: Thread* self
    call    SYMBOL(MterpSPutU32)
    testb   %al, %al
    jz      MterpPossibleException
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_sput

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sput_wide: /* 0x68 */
    ENTRY mterp_op_sput_wide
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /*
     * General field read / write (iget-* iput-* sget-* sput-*).
     */
    .extern MterpSPutU64
    REFRESH_INST 104                      # fix rINST to include opcode
    movq    rPC, OUT_ARG0                      # arg0: Instruction* inst
    movl    rINST, OUT_32_ARG1                 # arg1: uint16_t inst_data
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG2  # arg2: ShadowFrame* sf
    movq    rSELF, OUT_ARG3                    # arg3: Thread* self
    call    SYMBOL(MterpSPutU64)
    testb   %al, %al
    jz      MterpPossibleException
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_sput_wide

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sput_object: /* 0x69 */
    ENTRY mterp_op_sput_object
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /*
     * General field read / write (iget-* iput-* sget-* sput-*).
     */
    .extern MterpSPutObj
    REFRESH_INST 105                      # fix rINST to include opcode
    movq    rPC, OUT_ARG0                      # arg0: Instruction* inst
    movl    rINST, OUT_32_ARG1                 # arg1: uint16_t inst_data
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG2  # arg2: ShadowFrame* sf
    movq    rSELF, OUT_ARG3                    # arg3: Thread* self
    call    SYMBOL(MterpSPutObj)
    testb   %al, %al
    jz      MterpPossibleException
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_sput_object

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sput_boolean: /* 0x6a */
    ENTRY mterp_op_sput_boolean
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /*
     * General field read / write (iget-* iput-* sget-* sput-*).
     */
    .extern MterpSPutU8
    REFRESH_INST 106                      # fix rINST to include opcode
    movq    rPC, OUT_ARG0                      # arg0: Instruction* inst
    movl    rINST, OUT_32_ARG1                 # arg1: uint16_t inst_data
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG2  # arg2: ShadowFrame* sf
    movq    rSELF, OUT_ARG3                    # arg3: Thread* self
    call    SYMBOL(MterpSPutU8)
    testb   %al, %al
    jz      MterpPossibleException
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_sput_boolean

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sput_byte: /* 0x6b */
    ENTRY mterp_op_sput_byte
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /*
     * General field read / write (iget-* iput-* sget-* sput-*).
     */
    .extern MterpSPutI8
    REFRESH_INST 107                      # fix rINST to include opcode
    movq    rPC, OUT_ARG0                      # arg0: Instruction* inst
    movl    rINST, OUT_32_ARG1                 # arg1: uint16_t inst_data
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG2  # arg2: ShadowFrame* sf
    movq    rSELF, OUT_ARG3                    # arg3: Thread* self
    call    SYMBOL(MterpSPutI8)
    testb   %al, %al
    jz      MterpPossibleException
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_sput_byte

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sput_char: /* 0x6c */
    ENTRY mterp_op_sput_char
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /*
     * General field read / write (iget-* iput-* sget-* sput-*).
     */
    .extern MterpSPutU16
    REFRESH_INST 108                      # fix rINST to include opcode
    movq    rPC, OUT_ARG0                      # arg0: Instruction* inst
    movl    rINST, OUT_32_ARG1                 # arg1: uint16_t inst_data
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG2  # arg2: ShadowFrame* sf
    movq    rSELF, OUT_ARG3                    # arg3: Thread* self
    call    SYMBOL(MterpSPutU16)
    testb   %al, %al
    jz      MterpPossibleException
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_sput_char

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sput_short: /* 0x6d */
    ENTRY mterp_op_sput_short
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /*
     * General field read / write (iget-* iput-* sget-* sput-*).
     */
    .extern MterpSPutI16
    REFRESH_INST 109                      # fix rINST to include opcode
    movq    rPC, OUT_ARG0                      # arg0: Instruction* inst
    movl    rINST, OUT_32_ARG1                 # arg1: uint16_t inst_data
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG2  # arg2: ShadowFrame* sf
    movq    rSELF, OUT_ARG3                    # arg3: Thread* self
    call    SYMBOL(MterpSPutI16)
    testb   %al, %al
    jz      MterpPossibleException
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_sput_short

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_virtual: /* 0x6e */
    ENTRY mterp_op_invoke_virtual
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic invoke handler wrapper.
 */
    /* op vB, {vD, vE, vF, vG, vA}, class@CCCC */
    /* op {vCCCC..v(CCCC+AA-1)}, meth@BBBB */
    .extern MterpInvokeVirtual
    EXPORT_PC
    movq    rSELF, OUT_ARG0
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG1
    movq    rPC, OUT_ARG2
    REFRESH_INST 110
    movl    rINST, OUT_32_ARG3
    call    SYMBOL(MterpInvokeVirtual)
    testb   %al, %al
    jz      MterpException
    ADVANCE_PC 3
    movq    rSELF, %rax
    cmpb    LITERAL(0), THREAD_USE_MTERP_OFFSET(%rax)
    jz      MterpFallback
    FETCH_INST
    GOTO_NEXT

/*
 * Handle a virtual method call.
 *
 * for: invoke-virtual, invoke-virtual/range
 */
    /* op vB, {vD, vE, vF, vG, vA}, class@CCCC */
    /* op vAA, {vCCCC..v(CCCC+AA-1)}, meth@BBBB */

    END mterp_op_invoke_virtual

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_super: /* 0x6f */
    ENTRY mterp_op_invoke_super
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic invoke handler wrapper.
 */
    /* op vB, {vD, vE, vF, vG, vA}, class@CCCC */
    /* op {vCCCC..v(CCCC+AA-1)}, meth@BBBB */
    .extern MterpInvokeSuper
    EXPORT_PC
    movq    rSELF, OUT_ARG0
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG1
    movq    rPC, OUT_ARG2
    REFRESH_INST 111
    movl    rINST, OUT_32_ARG3
    call    SYMBOL(MterpInvokeSuper)
    testb   %al, %al
    jz      MterpException
    ADVANCE_PC 3
    movq    rSELF, %rax
    cmpb    LITERAL(0), THREAD_USE_MTERP_OFFSET(%rax)
    jz      MterpFallback
    FETCH_INST
    GOTO_NEXT

/*
 * Handle a "super" method call.
 *
 * for: invoke-super, invoke-super/range
 */
    /* op vB, {vD, vE, vF, vG, vA}, class@CCCC */
    /* op vAA, {vCCCC..v(CCCC+AA-1)}, meth@BBBB */

    END mterp_op_invoke_super

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_direct: /* 0x70 */
    ENTRY mterp_op_invoke_direct
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic invoke handler wrapper.
 */
    /* op vB, {vD, vE, vF, vG, vA}, class@CCCC */
    /* op {vCCCC..v(CCCC+AA-1)}, meth@BBBB */
    .extern MterpInvokeDirect
    EXPORT_PC
    movq    rSELF, OUT_ARG0
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG1
    movq    rPC, OUT_ARG2
    REFRESH_INST 112
    movl    rINST, OUT_32_ARG3
    call    SYMBOL(MterpInvokeDirect)
    testb   %al, %al
    jz      MterpException
    ADVANCE_PC 3
    movq    rSELF, %rax
    cmpb    LITERAL(0), THREAD_USE_MTERP_OFFSET(%rax)
    jz      MterpFallback
    FETCH_INST
    GOTO_NEXT

    END mterp_op_invoke_direct

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_static: /* 0x71 */
    ENTRY mterp_op_invoke_static
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic invoke handler wrapper.
 */
    /* op vB, {vD, vE, vF, vG, vA}, class@CCCC */
    /* op {vCCCC..v(CCCC+AA-1)}, meth@BBBB */
    .extern MterpInvokeStatic
    EXPORT_PC
    movq    rSELF, OUT_ARG0
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG1
    movq    rPC, OUT_ARG2
    REFRESH_INST 113
    movl    rINST, OUT_32_ARG3
    call    SYMBOL(MterpInvokeStatic)
    testb   %al, %al
    jz      MterpException
    ADVANCE_PC 3
    movq    rSELF, %rax
    cmpb    LITERAL(0), THREAD_USE_MTERP_OFFSET(%rax)
    jz      MterpFallback
    FETCH_INST
    GOTO_NEXT

    END mterp_op_invoke_static

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_interface: /* 0x72 */
    ENTRY mterp_op_invoke_interface
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic invoke handler wrapper.
 */
    /* op vB, {vD, vE, vF, vG, vA}, class@CCCC */
    /* op {vCCCC..v(CCCC+AA-1)}, meth@BBBB */
    .extern MterpInvokeInterface
    EXPORT_PC
    movq    rSELF, OUT_ARG0
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG1
    movq    rPC, OUT_ARG2
    REFRESH_INST 114
    movl    rINST, OUT_32_ARG3
    call    SYMBOL(MterpInvokeInterface)
    testb   %al, %al
    jz      MterpException
    ADVANCE_PC 3
    movq    rSELF, %rax
    cmpb    LITERAL(0), THREAD_USE_MTERP_OFFSET(%rax)
    jz      MterpFallback
    FETCH_INST
    GOTO_NEXT

/*
 * Handle an interface method call.
 *
 * for: invoke-interface, invoke-interface/range
 */
    /* op vB, {vD, vE, vF, vG, vA}, class@CCCC */
    /* op {vCCCC..v(CCCC+AA-1)}, meth@BBBB */

    END mterp_op_invoke_interface

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_73: /* 0x73 */
    ENTRY mterp_op_unused_73
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Bail to reference interpreter to throw.
 */
    jmp     MterpFallback

    END mterp_op_unused_73

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_virtual_range: /* 0x74 */
    ENTRY mterp_op_invoke_virtual_range
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic invoke handler wrapper.
 */
    /* op vB, {vD, vE, vF, vG, vA}, class@CCCC */
    /* op {vCCCC..v(CCCC+AA-1)}, meth@BBBB */
    .extern MterpInvokeVirtualRange
    EXPORT_PC
    movq    rSELF, OUT_ARG0
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG1
    movq    rPC, OUT_ARG2
    REFRESH_INST 116
    movl    rINST, OUT_32_ARG3
    call    SYMBOL(MterpInvokeVirtualRange)
    testb   %al, %al
    jz      MterpException
    ADVANCE_PC 3
    movq    rSELF, %rax
    cmpb    LITERAL(0), THREAD_USE_MTERP_OFFSET(%rax)
    jz      MterpFallback
    FETCH_INST
    GOTO_NEXT

    END mterp_op_invoke_virtual_range

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_super_range: /* 0x75 */
    ENTRY mterp_op_invoke_super_range
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic invoke handler wrapper.
 */
    /* op vB, {vD, vE, vF, vG, vA}, class@CCCC */
    /* op {vCCCC..v(CCCC+AA-1)}, meth@BBBB */
    .extern MterpInvokeSuperRange
    EXPORT_PC
    movq    rSELF, OUT_ARG0
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG1
    movq    rPC, OUT_ARG2
    REFRESH_INST 117
    movl    rINST, OUT_32_ARG3
    call    SYMBOL(MterpInvokeSuperRange)
    testb   %al, %al
    jz      MterpException
    ADVANCE_PC 3
    movq    rSELF, %rax
    cmpb    LITERAL(0), THREAD_USE_MTERP_OFFSET(%rax)
    jz      MterpFallback
    FETCH_INST
    GOTO_NEXT

    END mterp_op_invoke_super_range

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_direct_range: /* 0x76 */
    ENTRY mterp_op_invoke_direct_range
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic invoke handler wrapper.
 */
    /* op vB, {vD, vE, vF, vG, vA}, class@CCCC */
    /* op {vCCCC..v(CCCC+AA-1)}, meth@BBBB */
    .extern MterpInvokeDirectRange
    EXPORT_PC
    movq    rSELF, OUT_ARG0
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG1
    movq    rPC, OUT_ARG2
    REFRESH_INST 118
    movl    rINST, OUT_32_ARG3
    call    SYMBOL(MterpInvokeDirectRange)
    testb   %al, %al
    jz      MterpException
    ADVANCE_PC 3
    movq    rSELF, %rax
    cmpb    LITERAL(0), THREAD_USE_MTERP_OFFSET(%rax)
    jz      MterpFallback
    FETCH_INST
    GOTO_NEXT

    END mterp_op_invoke_direct_range

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_static_range: /* 0x77 */
    ENTRY mterp_op_invoke_static_range
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic invoke handler wrapper.
 */
    /* op vB, {vD, vE, vF, vG, vA}, class@CCCC */
    /* op {vCCCC..v(CCCC+AA-1)}, meth@BBBB */
    .extern MterpInvokeStaticRange
    EXPORT_PC
    movq    rSELF, OUT_ARG0
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG1
    movq    rPC, OUT_ARG2
    REFRESH_INST 119
    movl    rINST, OUT_32_ARG3
    call    SYMBOL(MterpInvokeStaticRange)
    testb   %al, %al
    jz      MterpException
    ADVANCE_PC 3
    movq    rSELF, %rax
    cmpb    LITERAL(0), THREAD_USE_MTERP_OFFSET(%rax)
    jz      MterpFallback
    FETCH_INST
    GOTO_NEXT

    END mterp_op_invoke_static_range

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_interface_range: /* 0x78 */
    ENTRY mterp_op_invoke_interface_range
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic invoke handler wrapper.
 */
    /* op vB, {vD, vE, vF, vG, vA}, class@CCCC */
    /* op {vCCCC..v(CCCC+AA-1)}, meth@BBBB */
    .extern MterpInvokeInterfaceRange
    EXPORT_PC
    movq    rSELF, OUT_ARG0
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG1
    movq    rPC, OUT_ARG2
    REFRESH_INST 120
    movl    rINST, OUT_32_ARG3
    call    SYMBOL(MterpInvokeInterfaceRange)
    testb   %al, %al
    jz      MterpException
    ADVANCE_PC 3
    movq    rSELF, %rax
    cmpb    LITERAL(0), THREAD_USE_MTERP_OFFSET(%rax)
    jz      MterpFallback
    FETCH_INST
    GOTO_NEXT

    END mterp_op_invoke_interface_range

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_79: /* 0x79 */
    ENTRY mterp_op_unused_79
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Bail to reference interpreter to throw.
 */
    jmp     MterpFallback

    END mterp_op_unused_79

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_7a: /* 0x7a */
    ENTRY mterp_op_unused_7a
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Bail to reference interpreter to throw.
 */
    jmp     MterpFallback

    END mterp_op_unused_7a

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_neg_int: /* 0x7b */
    ENTRY mterp_op_neg_int
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 32/64-bit unary operation.  Provide an "instr" line that
 * specifies an instruction that performs "result = op eax".
 */
    /* unop vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4,rINST                       # rINST <- B
    .if 0
    GET_WIDE_VREG %rax, rINSTq              # rax <- vB
    .else
    GET_VREG %eax, rINSTq                   # eax <- vB
    .endif
    andb    $0xf,%cl                       # ecx <- A

    negl    %eax
    .if 0
    SET_WIDE_VREG %rax, %rcx
    .else
    SET_VREG %eax, %rcx
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_neg_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_not_int: /* 0x7c */
    ENTRY mterp_op_not_int
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 32/64-bit unary operation.  Provide an "instr" line that
 * specifies an instruction that performs "result = op eax".
 */
    /* unop vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4,rINST                       # rINST <- B
    .if 0
    GET_WIDE_VREG %rax, rINSTq              # rax <- vB
    .else
    GET_VREG %eax, rINSTq                   # eax <- vB
    .endif
    andb    $0xf,%cl                       # ecx <- A

    notl    %eax
    .if 0
    SET_WIDE_VREG %rax, %rcx
    .else
    SET_VREG %eax, %rcx
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_not_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_neg_long: /* 0x7d */
    ENTRY mterp_op_neg_long
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 32/64-bit unary operation.  Provide an "instr" line that
 * specifies an instruction that performs "result = op eax".
 */
    /* unop vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4,rINST                       # rINST <- B
    .if 1
    GET_WIDE_VREG %rax, rINSTq              # rax <- vB
    .else
    GET_VREG %eax, rINSTq                   # eax <- vB
    .endif
    andb    $0xf,%cl                       # ecx <- A

    negq    %rax
    .if 1
    SET_WIDE_VREG %rax, %rcx
    .else
    SET_VREG %eax, %rcx
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_neg_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_not_long: /* 0x7e */
    ENTRY mterp_op_not_long
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 32/64-bit unary operation.  Provide an "instr" line that
 * specifies an instruction that performs "result = op eax".
 */
    /* unop vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4,rINST                       # rINST <- B
    .if 1
    GET_WIDE_VREG %rax, rINSTq              # rax <- vB
    .else
    GET_VREG %eax, rINSTq                   # eax <- vB
    .endif
    andb    $0xf,%cl                       # ecx <- A

    notq    %rax
    .if 1
    SET_WIDE_VREG %rax, %rcx
    .else
    SET_VREG %eax, %rcx
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_not_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_neg_float: /* 0x7f */
    ENTRY mterp_op_neg_float
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 32/64-bit unary operation.  Provide an "instr" line that
 * specifies an instruction that performs "result = op eax".
 */
    /* unop vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4,rINST                       # rINST <- B
    .if 0
    GET_WIDE_VREG %rax, rINSTq              # rax <- vB
    .else
    GET_VREG %eax, rINSTq                   # eax <- vB
    .endif
    andb    $0xf,%cl                       # ecx <- A

    xorl    $0x80000000, %eax
    .if 0
    SET_WIDE_VREG %rax, %rcx
    .else
    SET_VREG %eax, %rcx
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_neg_float

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_neg_double: /* 0x80 */
    ENTRY mterp_op_neg_double
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 32/64-bit unary operation.  Provide an "instr" line that
 * specifies an instruction that performs "result = op eax".
 */
    /* unop vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4,rINST                       # rINST <- B
    .if 1
    GET_WIDE_VREG %rax, rINSTq              # rax <- vB
    .else
    GET_VREG %eax, rINSTq                   # eax <- vB
    .endif
    andb    $0xf,%cl                       # ecx <- A
    movq    $0x8000000000000000, %rsi
    xorq    %rsi, %rax
    .if 1
    SET_WIDE_VREG %rax, %rcx
    .else
    SET_VREG %eax, %rcx
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_neg_double

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_int_to_long: /* 0x81 */
    ENTRY mterp_op_int_to_long
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /* int to long vA, vB */
    movzbq  rINSTbl, %rax                   # rax <- +A
    sarl    $4, %eax                       # eax <- B
    andb    $0xf, rINSTbl                  # rINST <- A
    movslq  VREG_ADDRESS(%rax), %rax
    SET_WIDE_VREG %rax, rINSTq              # v[A] <- %rax
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_int_to_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_int_to_float: /* 0x82 */
    ENTRY mterp_op_int_to_float
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 32-bit FP conversion operation.
 */
    /* unop vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # ecx <- A
    cvtsi2ssl    VREG_ADDRESS(rINSTq), %xmm0
    .if 0
    SET_VREG_XMMd %xmm0, %rcx
    CLEAR_WIDE_REF %rcx
    .else
    SET_VREG_XMMs %xmm0, %rcx
    CLEAR_REF %rcx
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_int_to_float

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_int_to_double: /* 0x83 */
    ENTRY mterp_op_int_to_double
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 32-bit FP conversion operation.
 */
    /* unop vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # ecx <- A
    cvtsi2sdl    VREG_ADDRESS(rINSTq), %xmm0
    .if 1
    SET_VREG_XMMd %xmm0, %rcx
    CLEAR_WIDE_REF %rcx
    .else
    SET_VREG_XMMs %xmm0, %rcx
    CLEAR_REF %rcx
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_int_to_double

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_long_to_int: /* 0x84 */
    ENTRY mterp_op_long_to_int
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/* we ignore the high word, making this equivalent to a 32-bit reg move */
    /* for move, move-object, long-to-int */
    /* op vA, vB */
    movl    rINST, %eax                     # eax <- BA
    andb    $0xf, %al                      # eax <- A
    shrl    $4, rINST                      # rINST <- B
    GET_VREG %edx, rINSTq
    .if 0
    SET_VREG_OBJECT %edx, %rax              # fp[A] <- fp[B]
    .else
    SET_VREG %edx, %rax                     # fp[A] <- fp[B]
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_long_to_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_long_to_float: /* 0x85 */
    ENTRY mterp_op_long_to_float
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 32-bit FP conversion operation.
 */
    /* unop vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # ecx <- A
    cvtsi2ssq    VREG_ADDRESS(rINSTq), %xmm0
    .if 0
    SET_VREG_XMMd %xmm0, %rcx
    CLEAR_WIDE_REF %rcx
    .else
    SET_VREG_XMMs %xmm0, %rcx
    CLEAR_REF %rcx
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_long_to_float

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_long_to_double: /* 0x86 */
    ENTRY mterp_op_long_to_double
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 32-bit FP conversion operation.
 */
    /* unop vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # ecx <- A
    cvtsi2sdq    VREG_ADDRESS(rINSTq), %xmm0
    .if 1
    SET_VREG_XMMd %xmm0, %rcx
    CLEAR_WIDE_REF %rcx
    .else
    SET_VREG_XMMs %xmm0, %rcx
    CLEAR_REF %rcx
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_long_to_double

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_float_to_int: /* 0x87 */
    ENTRY mterp_op_float_to_int
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/* On fp to int conversions, Java requires that
 * if the result > maxint, it should be clamped to maxint.  If it is less
 * than minint, it should be clamped to minint.  If it is a nan, the result
 * should be zero.  Further, the rounding mode is to truncate.
 */
    /* float/double to int/long vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # ecx <- A
    GET_VREG_XMMs %xmm0, rINSTq
    movl  $0x7fffffff, %eax
    cvtsi2ssl %eax, %xmm1
    comiss    %xmm1, %xmm0
    jae     1f
    jp      2f
    cvttss2sil  %xmm0, %eax
    jmp     1f
2:
    xorl    %eax, %eax
1:
    .if 0
    SET_WIDE_VREG %eax, %rcx
    .else
    SET_VREG %eax, %rcx
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_float_to_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_float_to_long: /* 0x88 */
    ENTRY mterp_op_float_to_long
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/* On fp to int conversions, Java requires that
 * if the result > maxint, it should be clamped to maxint.  If it is less
 * than minint, it should be clamped to minint.  If it is a nan, the result
 * should be zero.  Further, the rounding mode is to truncate.
 */
    /* float/double to int/long vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # ecx <- A
    GET_VREG_XMMs %xmm0, rINSTq
    movq  $0x7fffffffffffffff, %rax
    cvtsi2ssq %rax, %xmm1
    comiss    %xmm1, %xmm0
    jae     1f
    jp      2f
    cvttss2siq  %xmm0, %rax
    jmp     1f
2:
    xorq    %rax, %rax
1:
    .if 1
    SET_WIDE_VREG %rax, %rcx
    .else
    SET_VREG %rax, %rcx
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_float_to_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_float_to_double: /* 0x89 */
    ENTRY mterp_op_float_to_double
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 32-bit FP conversion operation.
 */
    /* unop vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # ecx <- A
    cvtss2sd    VREG_ADDRESS(rINSTq), %xmm0
    .if 1
    SET_VREG_XMMd %xmm0, %rcx
    CLEAR_WIDE_REF %rcx
    .else
    SET_VREG_XMMs %xmm0, %rcx
    CLEAR_REF %rcx
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_float_to_double

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_double_to_int: /* 0x8a */
    ENTRY mterp_op_double_to_int
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/* On fp to int conversions, Java requires that
 * if the result > maxint, it should be clamped to maxint.  If it is less
 * than minint, it should be clamped to minint.  If it is a nan, the result
 * should be zero.  Further, the rounding mode is to truncate.
 */
    /* float/double to int/long vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # ecx <- A
    GET_VREG_XMMd %xmm0, rINSTq
    movl  $0x7fffffff, %eax
    cvtsi2sdl %eax, %xmm1
    comisd    %xmm1, %xmm0
    jae     1f
    jp      2f
    cvttsd2sil  %xmm0, %eax
    jmp     1f
2:
    xorl    %eax, %eax
1:
    .if 0
    SET_WIDE_VREG %eax, %rcx
    .else
    SET_VREG %eax, %rcx
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_double_to_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_double_to_long: /* 0x8b */
    ENTRY mterp_op_double_to_long
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/* On fp to int conversions, Java requires that
 * if the result > maxint, it should be clamped to maxint.  If it is less
 * than minint, it should be clamped to minint.  If it is a nan, the result
 * should be zero.  Further, the rounding mode is to truncate.
 */
    /* float/double to int/long vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # ecx <- A
    GET_VREG_XMMd %xmm0, rINSTq
    movq  $0x7fffffffffffffff, %rax
    cvtsi2sdq %rax, %xmm1
    comisd    %xmm1, %xmm0
    jae     1f
    jp      2f
    cvttsd2siq  %xmm0, %rax
    jmp     1f
2:
    xorq    %rax, %rax
1:
    .if 1
    SET_WIDE_VREG %rax, %rcx
    .else
    SET_VREG %rax, %rcx
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_double_to_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_double_to_float: /* 0x8c */
    ENTRY mterp_op_double_to_float
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 32-bit FP conversion operation.
 */
    /* unop vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # ecx <- A
    cvtsd2ss    VREG_ADDRESS(rINSTq), %xmm0
    .if 0
    SET_VREG_XMMd %xmm0, %rcx
    CLEAR_WIDE_REF %rcx
    .else
    SET_VREG_XMMs %xmm0, %rcx
    CLEAR_REF %rcx
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_double_to_float

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_int_to_byte: /* 0x8d */
    ENTRY mterp_op_int_to_byte
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 32/64-bit unary operation.  Provide an "instr" line that
 * specifies an instruction that performs "result = op eax".
 */
    /* unop vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4,rINST                       # rINST <- B
    .if 0
    GET_WIDE_VREG %rax, rINSTq              # rax <- vB
    .else
    GET_VREG %eax, rINSTq                   # eax <- vB
    .endif
    andb    $0xf,%cl                       # ecx <- A

movsbl  %al, %eax
    .if 0
    SET_WIDE_VREG %rax, %rcx
    .else
    SET_VREG %eax, %rcx
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_int_to_byte

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_int_to_char: /* 0x8e */
    ENTRY mterp_op_int_to_char
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 32/64-bit unary operation.  Provide an "instr" line that
 * specifies an instruction that performs "result = op eax".
 */
    /* unop vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4,rINST                       # rINST <- B
    .if 0
    GET_WIDE_VREG %rax, rINSTq              # rax <- vB
    .else
    GET_VREG %eax, rINSTq                   # eax <- vB
    .endif
    andb    $0xf,%cl                       # ecx <- A

movzwl  %ax,%eax
    .if 0
    SET_WIDE_VREG %rax, %rcx
    .else
    SET_VREG %eax, %rcx
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_int_to_char

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_int_to_short: /* 0x8f */
    ENTRY mterp_op_int_to_short
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 32/64-bit unary operation.  Provide an "instr" line that
 * specifies an instruction that performs "result = op eax".
 */
    /* unop vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4,rINST                       # rINST <- B
    .if 0
    GET_WIDE_VREG %rax, rINSTq              # rax <- vB
    .else
    GET_VREG %eax, rINSTq                   # eax <- vB
    .endif
    andb    $0xf,%cl                       # ecx <- A

movswl %ax, %eax
    .if 0
    SET_WIDE_VREG %rax, %rcx
    .else
    SET_VREG %eax, %rcx
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_int_to_short

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_add_int: /* 0x90 */
    ENTRY mterp_op_add_int
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 32-bit binary operation.  Provide an "instr" line that
 * specifies an instruction that performs "result = eax op (rFP,%ecx,4)".
 * This could be an x86 instruction or a function call.  (If the result
 * comes back in a register other than eax, you can override "result".)
 *
 * For: add-int, sub-int, and-int, or-int,
 *      xor-int, shl-int, shr-int, ushr-int
 */
    /* binop vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # rax <- BB
    movzbq  3(rPC), %rcx                    # rcx <- CC
    GET_VREG %eax, %rax                     # eax <- vBB
    addl VREG_ADDRESS(%rcx),%eax
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_add_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sub_int: /* 0x91 */
    ENTRY mterp_op_sub_int
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 32-bit binary operation.  Provide an "instr" line that
 * specifies an instruction that performs "result = eax op (rFP,%ecx,4)".
 * This could be an x86 instruction or a function call.  (If the result
 * comes back in a register other than eax, you can override "result".)
 *
 * For: add-int, sub-int, and-int, or-int,
 *      xor-int, shl-int, shr-int, ushr-int
 */
    /* binop vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # rax <- BB
    movzbq  3(rPC), %rcx                    # rcx <- CC
    GET_VREG %eax, %rax                     # eax <- vBB
    subl VREG_ADDRESS(%rcx),%eax
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_sub_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_mul_int: /* 0x92 */
    ENTRY mterp_op_mul_int
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 32-bit binary operation.  Provide an "instr" line that
 * specifies an instruction that performs "result = eax op (rFP,%ecx,4)".
 * This could be an x86 instruction or a function call.  (If the result
 * comes back in a register other than eax, you can override "result".)
 *
 * For: add-int, sub-int, and-int, or-int,
 *      xor-int, shl-int, shr-int, ushr-int
 */
    /* binop vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # rax <- BB
    movzbq  3(rPC), %rcx                    # rcx <- CC
    GET_VREG %eax, %rax                     # eax <- vBB
    imull VREG_ADDRESS(%rcx),%eax
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_mul_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_div_int: /* 0x93 */
    ENTRY mterp_op_div_int
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * 32-bit binary div/rem operation.  Handles special case of op1=-1.
 */
    /* div/rem vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # rax <- BB
    movzbq  3(rPC), %rcx                    # rcx <- CC
    .if 0
    GET_WIDE_VREG %rax, %rax                # eax <- vBB
    GET_WIDE_VREG %ecx, %rcx             # ecx <- vCC
    .else
    GET_VREG %eax, %rax                     # eax <- vBB
    GET_VREG %ecx, %rcx                  # ecx <- vCC
    .endif
    testl   %ecx, %ecx
    jz      common_errDivideByZero
    cmpl  $-1, %ecx
    je      2f
    cmpl  $2, %ecx
    je 3f
    cdq                                    # rdx:rax <- sign-extended of rax
    idivl   %ecx
1:
    .if 0
    SET_WIDE_VREG %eax, rINSTq           # eax <- vBB
    .else
    SET_VREG %eax, rINSTq                # eax <- vBB
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
2:
    .if 0
    xorl %eax, %eax
    .else
    negl %eax
    .endif
    jmp     1b
3:
    .if 0
    movl %edx, %eax
    .if 0
    shrl $63, %eax
    .else
    shrl $31, %eax
    .endif
    addl %edx, %eax
    andl $-2, %eax
    subl %eax, %edx
    movl %edx, %eax
    .else
    movl %eax, %edx
    .if 0
    shrl $63, %edx
    .else
    shrl $31, %edx
    .endif
    addl %edx, %eax
    sarl %eax
    .endif
    jmp     1b

    END mterp_op_div_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_rem_int: /* 0x94 */
    ENTRY mterp_op_rem_int
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * 32-bit binary div/rem operation.  Handles special case of op1=-1.
 */
    /* div/rem vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # rax <- BB
    movzbq  3(rPC), %rcx                    # rcx <- CC
    .if 0
    GET_WIDE_VREG %rax, %rax                # eax <- vBB
    GET_WIDE_VREG %ecx, %rcx             # ecx <- vCC
    .else
    GET_VREG %eax, %rax                     # eax <- vBB
    GET_VREG %ecx, %rcx                  # ecx <- vCC
    .endif
    testl   %ecx, %ecx
    jz      common_errDivideByZero
    cmpl  $-1, %ecx
    je      2f
    cmpl  $2, %ecx
    je 3f
    cdq                                    # rdx:rax <- sign-extended of rax
    idivl   %ecx
1:
    .if 0
    SET_WIDE_VREG %edx, rINSTq           # eax <- vBB
    .else
    SET_VREG %edx, rINSTq                # eax <- vBB
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
2:
    .if 1
    xorl %edx, %edx
    .else
    negl %edx
    .endif
    jmp     1b
3:
    .if 1
    movl %eax, %edx
    .if 0
    shrl $63, %edx
    .else
    shrl $31, %edx
    .endif
    addl %eax, %edx
    andl $-2, %edx
    subl %edx, %eax
    movl %eax, %edx
    .else
    movl %edx, %eax
    .if 0
    shrl $63, %eax
    .else
    shrl $31, %eax
    .endif
    addl %eax, %edx
    sarl %edx
    .endif
    jmp     1b

    END mterp_op_rem_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_and_int: /* 0x95 */
    ENTRY mterp_op_and_int
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 32-bit binary operation.  Provide an "instr" line that
 * specifies an instruction that performs "result = eax op (rFP,%ecx,4)".
 * This could be an x86 instruction or a function call.  (If the result
 * comes back in a register other than eax, you can override "result".)
 *
 * For: add-int, sub-int, and-int, or-int,
 *      xor-int, shl-int, shr-int, ushr-int
 */
    /* binop vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # rax <- BB
    movzbq  3(rPC), %rcx                    # rcx <- CC
    GET_VREG %eax, %rax                     # eax <- vBB
    andl VREG_ADDRESS(%rcx),%eax
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_and_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_or_int: /* 0x96 */
    ENTRY mterp_op_or_int
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 32-bit binary operation.  Provide an "instr" line that
 * specifies an instruction that performs "result = eax op (rFP,%ecx,4)".
 * This could be an x86 instruction or a function call.  (If the result
 * comes back in a register other than eax, you can override "result".)
 *
 * For: add-int, sub-int, and-int, or-int,
 *      xor-int, shl-int, shr-int, ushr-int
 */
    /* binop vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # rax <- BB
    movzbq  3(rPC), %rcx                    # rcx <- CC
    GET_VREG %eax, %rax                     # eax <- vBB
    orl VREG_ADDRESS(%rcx),%eax
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_or_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_xor_int: /* 0x97 */
    ENTRY mterp_op_xor_int
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 32-bit binary operation.  Provide an "instr" line that
 * specifies an instruction that performs "result = eax op (rFP,%ecx,4)".
 * This could be an x86 instruction or a function call.  (If the result
 * comes back in a register other than eax, you can override "result".)
 *
 * For: add-int, sub-int, and-int, or-int,
 *      xor-int, shl-int, shr-int, ushr-int
 */
    /* binop vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # rax <- BB
    movzbq  3(rPC), %rcx                    # rcx <- CC
    GET_VREG %eax, %rax                     # eax <- vBB
    xorl VREG_ADDRESS(%rcx),%eax
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_xor_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_shl_int: /* 0x98 */
    ENTRY mterp_op_shl_int
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 32-bit binary operation in which both operands loaded to
 * registers (op0 in eax, op1 in ecx).
 */
    /* binop vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # eax <- BB
    movzbq  3(rPC), %rcx                    # ecx <- CC
    GET_VREG %ecx, %rcx                     # eax <- vCC
    .if 0
    GET_WIDE_VREG %rax, %rax                # rax <- vBB
    sall    %cl, %eax                                  # ex: addl    %ecx,%eax
    SET_WIDE_VREG %rax, rINSTq
    .else
    GET_VREG %eax, %rax                     # eax <- vBB
    sall    %cl, %eax                                  # ex: addl    %ecx,%eax
    SET_VREG %eax, rINSTq
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_shl_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_shr_int: /* 0x99 */
    ENTRY mterp_op_shr_int
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 32-bit binary operation in which both operands loaded to
 * registers (op0 in eax, op1 in ecx).
 */
    /* binop vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # eax <- BB
    movzbq  3(rPC), %rcx                    # ecx <- CC
    GET_VREG %ecx, %rcx                     # eax <- vCC
    .if 0
    GET_WIDE_VREG %rax, %rax                # rax <- vBB
    sarl    %cl, %eax                                  # ex: addl    %ecx,%eax
    SET_WIDE_VREG %rax, rINSTq
    .else
    GET_VREG %eax, %rax                     # eax <- vBB
    sarl    %cl, %eax                                  # ex: addl    %ecx,%eax
    SET_VREG %eax, rINSTq
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_shr_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_ushr_int: /* 0x9a */
    ENTRY mterp_op_ushr_int
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 32-bit binary operation in which both operands loaded to
 * registers (op0 in eax, op1 in ecx).
 */
    /* binop vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # eax <- BB
    movzbq  3(rPC), %rcx                    # ecx <- CC
    GET_VREG %ecx, %rcx                     # eax <- vCC
    .if 0
    GET_WIDE_VREG %rax, %rax                # rax <- vBB
    shrl    %cl, %eax                                  # ex: addl    %ecx,%eax
    SET_WIDE_VREG %rax, rINSTq
    .else
    GET_VREG %eax, %rax                     # eax <- vBB
    shrl    %cl, %eax                                  # ex: addl    %ecx,%eax
    SET_VREG %eax, rINSTq
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_ushr_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_add_long: /* 0x9b */
    ENTRY mterp_op_add_long
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 64-bit binary operation.
 */
    /* binop vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # eax <- BB
    movzbq  3(rPC), %rcx                    # ecx <- CC
    GET_WIDE_VREG %rax, %rax                # rax <- v[BB]
    addq VREG_ADDRESS(%rcx),%rax
    SET_WIDE_VREG %rax, rINSTq              # v[AA] <- rax
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_add_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sub_long: /* 0x9c */
    ENTRY mterp_op_sub_long
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 64-bit binary operation.
 */
    /* binop vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # eax <- BB
    movzbq  3(rPC), %rcx                    # ecx <- CC
    GET_WIDE_VREG %rax, %rax                # rax <- v[BB]
    subq VREG_ADDRESS(%rcx),%rax
    SET_WIDE_VREG %rax, rINSTq              # v[AA] <- rax
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_sub_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_mul_long: /* 0x9d */
    ENTRY mterp_op_mul_long
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 64-bit binary operation.
 */
    /* binop vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # eax <- BB
    movzbq  3(rPC), %rcx                    # ecx <- CC
    GET_WIDE_VREG %rax, %rax                # rax <- v[BB]
    imulq VREG_ADDRESS(%rcx),%rax
    SET_WIDE_VREG %rax, rINSTq              # v[AA] <- rax
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_mul_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_div_long: /* 0x9e */
    ENTRY mterp_op_div_long
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * 32-bit binary div/rem operation.  Handles special case of op1=-1.
 */
    /* div/rem vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # rax <- BB
    movzbq  3(rPC), %rcx                    # rcx <- CC
    .if 1
    GET_WIDE_VREG %rax, %rax                # eax <- vBB
    GET_WIDE_VREG %rcx, %rcx             # ecx <- vCC
    .else
    GET_VREG %eax, %rax                     # eax <- vBB
    GET_VREG %rcx, %rcx                  # ecx <- vCC
    .endif
    testq   %rcx, %rcx
    jz      common_errDivideByZero
    cmpq  $-1, %rcx
    je      2f
    cmpq  $2, %rcx
    je 3f
    cqo                                    # rdx:rax <- sign-extended of rax
    idivq   %rcx
1:
    .if 1
    SET_WIDE_VREG %rax, rINSTq           # eax <- vBB
    .else
    SET_VREG %rax, rINSTq                # eax <- vBB
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
2:
    .if 0
    xorq %rax, %rax
    .else
    negq %rax
    .endif
    jmp     1b
3:
    .if 0
    movq %rdx, %rax
    .if 1
    shrq $63, %rax
    .else
    shrq $31, %rax
    .endif
    addq %rdx, %rax
    andq $-2, %rax
    subq %rax, %rdx
    movq %rdx, %rax
    .else
    movq %rax, %rdx
    .if 1
    shrq $63, %rdx
    .else
    shrq $31, %rdx
    .endif
    addq %rdx, %rax
    sarq %rax
    .endif
    jmp     1b

    END mterp_op_div_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_rem_long: /* 0x9f */
    ENTRY mterp_op_rem_long
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * 32-bit binary div/rem operation.  Handles special case of op1=-1.
 */
    /* div/rem vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # rax <- BB
    movzbq  3(rPC), %rcx                    # rcx <- CC
    .if 1
    GET_WIDE_VREG %rax, %rax                # eax <- vBB
    GET_WIDE_VREG %rcx, %rcx             # ecx <- vCC
    .else
    GET_VREG %eax, %rax                     # eax <- vBB
    GET_VREG %rcx, %rcx                  # ecx <- vCC
    .endif
    testq   %rcx, %rcx
    jz      common_errDivideByZero
    cmpq  $-1, %rcx
    je      2f
    cmpq  $2, %rcx
    je 3f
    cqo                                    # rdx:rax <- sign-extended of rax
    idivq   %rcx
1:
    .if 1
    SET_WIDE_VREG %rdx, rINSTq           # eax <- vBB
    .else
    SET_VREG %rdx, rINSTq                # eax <- vBB
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
2:
    .if 1
    xorq %rdx, %rdx
    .else
    negq %rdx
    .endif
    jmp     1b
3:
    .if 1
    movq %rax, %rdx
    .if 1
    shrq $63, %rdx
    .else
    shrq $31, %rdx
    .endif
    addq %rax, %rdx
    andq $-2, %rdx
    subq %rdx, %rax
    movq %rax, %rdx
    .else
    movq %rdx, %rax
    .if 1
    shrq $63, %rax
    .else
    shrq $31, %rax
    .endif
    addq %rax, %rdx
    sarq %rdx
    .endif
    jmp     1b

    END mterp_op_rem_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_and_long: /* 0xa0 */
    ENTRY mterp_op_and_long
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 64-bit binary operation.
 */
    /* binop vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # eax <- BB
    movzbq  3(rPC), %rcx                    # ecx <- CC
    GET_WIDE_VREG %rax, %rax                # rax <- v[BB]
    andq VREG_ADDRESS(%rcx),%rax
    SET_WIDE_VREG %rax, rINSTq              # v[AA] <- rax
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_and_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_or_long: /* 0xa1 */
    ENTRY mterp_op_or_long
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 64-bit binary operation.
 */
    /* binop vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # eax <- BB
    movzbq  3(rPC), %rcx                    # ecx <- CC
    GET_WIDE_VREG %rax, %rax                # rax <- v[BB]
    orq VREG_ADDRESS(%rcx),%rax
    SET_WIDE_VREG %rax, rINSTq              # v[AA] <- rax
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_or_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_xor_long: /* 0xa2 */
    ENTRY mterp_op_xor_long
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 64-bit binary operation.
 */
    /* binop vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # eax <- BB
    movzbq  3(rPC), %rcx                    # ecx <- CC
    GET_WIDE_VREG %rax, %rax                # rax <- v[BB]
    xorq VREG_ADDRESS(%rcx),%rax
    SET_WIDE_VREG %rax, rINSTq              # v[AA] <- rax
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_xor_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_shl_long: /* 0xa3 */
    ENTRY mterp_op_shl_long
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 32-bit binary operation in which both operands loaded to
 * registers (op0 in eax, op1 in ecx).
 */
    /* binop vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # eax <- BB
    movzbq  3(rPC), %rcx                    # ecx <- CC
    GET_VREG %ecx, %rcx                     # eax <- vCC
    .if 1
    GET_WIDE_VREG %rax, %rax                # rax <- vBB
    salq    %cl, %rax                                  # ex: addl    %ecx,%eax
    SET_WIDE_VREG %rax, rINSTq
    .else
    GET_VREG %eax, %rax                     # eax <- vBB
    salq    %cl, %rax                                  # ex: addl    %ecx,%eax
    SET_VREG %eax, rINSTq
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_shl_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_shr_long: /* 0xa4 */
    ENTRY mterp_op_shr_long
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 32-bit binary operation in which both operands loaded to
 * registers (op0 in eax, op1 in ecx).
 */
    /* binop vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # eax <- BB
    movzbq  3(rPC), %rcx                    # ecx <- CC
    GET_VREG %ecx, %rcx                     # eax <- vCC
    .if 1
    GET_WIDE_VREG %rax, %rax                # rax <- vBB
    sarq    %cl, %rax                                  # ex: addl    %ecx,%eax
    SET_WIDE_VREG %rax, rINSTq
    .else
    GET_VREG %eax, %rax                     # eax <- vBB
    sarq    %cl, %rax                                  # ex: addl    %ecx,%eax
    SET_VREG %eax, rINSTq
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_shr_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_ushr_long: /* 0xa5 */
    ENTRY mterp_op_ushr_long
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 32-bit binary operation in which both operands loaded to
 * registers (op0 in eax, op1 in ecx).
 */
    /* binop vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # eax <- BB
    movzbq  3(rPC), %rcx                    # ecx <- CC
    GET_VREG %ecx, %rcx                     # eax <- vCC
    .if 1
    GET_WIDE_VREG %rax, %rax                # rax <- vBB
    shrq    %cl, %rax                                  # ex: addl    %ecx,%eax
    SET_WIDE_VREG %rax, rINSTq
    .else
    GET_VREG %eax, %rax                     # eax <- vBB
    shrq    %cl, %rax                                  # ex: addl    %ecx,%eax
    SET_VREG %eax, rINSTq
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_ushr_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_add_float: /* 0xa6 */
    ENTRY mterp_op_add_float
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    movzbq  2(rPC), %rcx                    # ecx <- BB
    movzbq  3(rPC), %rax                    # eax <- CC
    GET_VREG_XMMs %xmm0, %rcx         # %xmm0 <- 1st src
#ifdef MTERP_USE_AVX
    vaddss VREG_ADDRESS(%rax), %xmm0, %xmm0
    SET_VREG_XMMs %xmm0, rINSTq       # vAA <- %xmm0
    vpxor    %xmm0, %xmm0, %xmm0
    vmovss   %xmm0, VREG_REF_ADDRESS(rINSTq) # clear ref
#else
    addss VREG_ADDRESS(%rax), %xmm0
    SET_VREG_XMMs %xmm0, rINSTq       # vAA <- %xmm0
    pxor    %xmm0, %xmm0
    movss   %xmm0, VREG_REF_ADDRESS(rINSTq) # clear ref
#endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_add_float

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sub_float: /* 0xa7 */
    ENTRY mterp_op_sub_float
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    movzbq  2(rPC), %rcx                    # ecx <- BB
    movzbq  3(rPC), %rax                    # eax <- CC
    GET_VREG_XMMs %xmm0, %rcx         # %xmm0 <- 1st src
#ifdef MTERP_USE_AVX
    vsubss VREG_ADDRESS(%rax), %xmm0, %xmm0
    SET_VREG_XMMs %xmm0, rINSTq       # vAA <- %xmm0
    vpxor    %xmm0, %xmm0, %xmm0
    vmovss   %xmm0, VREG_REF_ADDRESS(rINSTq) # clear ref
#else
    subss VREG_ADDRESS(%rax), %xmm0
    SET_VREG_XMMs %xmm0, rINSTq       # vAA <- %xmm0
    pxor    %xmm0, %xmm0
    movss   %xmm0, VREG_REF_ADDRESS(rINSTq) # clear ref
#endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_sub_float

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_mul_float: /* 0xa8 */
    ENTRY mterp_op_mul_float
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    movzbq  2(rPC), %rcx                    # ecx <- BB
    movzbq  3(rPC), %rax                    # eax <- CC
    GET_VREG_XMMs %xmm0, %rcx         # %xmm0 <- 1st src
#ifdef MTERP_USE_AVX
    vmulss VREG_ADDRESS(%rax), %xmm0, %xmm0
    SET_VREG_XMMs %xmm0, rINSTq       # vAA <- %xmm0
    vpxor    %xmm0, %xmm0, %xmm0
    vmovss   %xmm0, VREG_REF_ADDRESS(rINSTq) # clear ref
#else
    mulss VREG_ADDRESS(%rax), %xmm0
    SET_VREG_XMMs %xmm0, rINSTq       # vAA <- %xmm0
    pxor    %xmm0, %xmm0
    movss   %xmm0, VREG_REF_ADDRESS(rINSTq) # clear ref
#endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_mul_float

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_div_float: /* 0xa9 */
    ENTRY mterp_op_div_float
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    movzbq  2(rPC), %rcx                    # ecx <- BB
    movzbq  3(rPC), %rax                    # eax <- CC
    GET_VREG_XMMs %xmm0, %rcx         # %xmm0 <- 1st src
#ifdef MTERP_USE_AVX
    vdivss VREG_ADDRESS(%rax), %xmm0, %xmm0
    SET_VREG_XMMs %xmm0, rINSTq       # vAA <- %xmm0
    vpxor    %xmm0, %xmm0, %xmm0
    vmovss   %xmm0, VREG_REF_ADDRESS(rINSTq) # clear ref
#else
    divss VREG_ADDRESS(%rax), %xmm0
    SET_VREG_XMMs %xmm0, rINSTq       # vAA <- %xmm0
    pxor    %xmm0, %xmm0
    movss   %xmm0, VREG_REF_ADDRESS(rINSTq) # clear ref
#endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_div_float

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_rem_float: /* 0xaa */
    ENTRY mterp_op_rem_float
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /* rem_float vAA, vBB, vCC */
    movzbq  3(rPC), %rcx                    # ecx <- BB
    movzbq  2(rPC), %rax                    # eax <- CC
    flds    VREG_ADDRESS(%rcx)              # vBB to fp stack
    flds    VREG_ADDRESS(%rax)              # vCC to fp stack
1:
    fprem
    fstsw   %ax
    sahf
    jp      1b
    fstp    %st(1)
    fstps   VREG_ADDRESS(rINSTq)            # %st to vAA
    CLEAR_REF rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_rem_float

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_add_double: /* 0xab */
    ENTRY mterp_op_add_double
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    movzbq  2(rPC), %rcx                    # ecx <- BB
    movzbq  3(rPC), %rax                    # eax <- CC
    GET_VREG_XMMd %xmm0, %rcx         # %xmm0 <- 1st src
#ifdef MTERP_USE_AVX
    vaddsd VREG_ADDRESS(%rax), %xmm0, %xmm0
    SET_VREG_XMMd %xmm0, rINSTq       # vAA <- %xmm0
    vpxor    %xmm0, %xmm0, %xmm0
    vmovsd   %xmm0, VREG_REF_ADDRESS(rINSTq) # clear ref
#else
    addsd VREG_ADDRESS(%rax), %xmm0
    SET_VREG_XMMd %xmm0, rINSTq       # vAA <- %xmm0
    pxor    %xmm0, %xmm0
    movsd   %xmm0, VREG_REF_ADDRESS(rINSTq) # clear ref
#endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_add_double

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sub_double: /* 0xac */
    ENTRY mterp_op_sub_double
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    movzbq  2(rPC), %rcx                    # ecx <- BB
    movzbq  3(rPC), %rax                    # eax <- CC
    GET_VREG_XMMd %xmm0, %rcx         # %xmm0 <- 1st src
#ifdef MTERP_USE_AVX
    vsubsd VREG_ADDRESS(%rax), %xmm0, %xmm0
    SET_VREG_XMMd %xmm0, rINSTq       # vAA <- %xmm0
    vpxor    %xmm0, %xmm0, %xmm0
    vmovsd   %xmm0, VREG_REF_ADDRESS(rINSTq) # clear ref
#else
    subsd VREG_ADDRESS(%rax), %xmm0
    SET_VREG_XMMd %xmm0, rINSTq       # vAA <- %xmm0
    pxor    %xmm0, %xmm0
    movsd   %xmm0, VREG_REF_ADDRESS(rINSTq) # clear ref
#endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_sub_double

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_mul_double: /* 0xad */
    ENTRY mterp_op_mul_double
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    movzbq  2(rPC), %rcx                    # ecx <- BB
    movzbq  3(rPC), %rax                    # eax <- CC
    GET_VREG_XMMd %xmm0, %rcx         # %xmm0 <- 1st src
#ifdef MTERP_USE_AVX
    vmulsd VREG_ADDRESS(%rax), %xmm0, %xmm0
    SET_VREG_XMMd %xmm0, rINSTq       # vAA <- %xmm0
    vpxor    %xmm0, %xmm0, %xmm0
    vmovsd   %xmm0, VREG_REF_ADDRESS(rINSTq) # clear ref
#else
    mulsd VREG_ADDRESS(%rax), %xmm0
    SET_VREG_XMMd %xmm0, rINSTq       # vAA <- %xmm0
    pxor    %xmm0, %xmm0
    movsd   %xmm0, VREG_REF_ADDRESS(rINSTq) # clear ref
#endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_mul_double

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_div_double: /* 0xae */
    ENTRY mterp_op_div_double
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    movzbq  2(rPC), %rcx                    # ecx <- BB
    movzbq  3(rPC), %rax                    # eax <- CC
    GET_VREG_XMMd %xmm0, %rcx         # %xmm0 <- 1st src
#ifdef MTERP_USE_AVX
    vdivsd VREG_ADDRESS(%rax), %xmm0, %xmm0
    SET_VREG_XMMd %xmm0, rINSTq       # vAA <- %xmm0
    vpxor    %xmm0, %xmm0, %xmm0
    vmovsd   %xmm0, VREG_REF_ADDRESS(rINSTq) # clear ref
#else
    divsd VREG_ADDRESS(%rax), %xmm0
    SET_VREG_XMMd %xmm0, rINSTq       # vAA <- %xmm0
    pxor    %xmm0, %xmm0
    movsd   %xmm0, VREG_REF_ADDRESS(rINSTq) # clear ref
#endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_div_double

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_rem_double: /* 0xaf */
    ENTRY mterp_op_rem_double
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /* rem_double vAA, vBB, vCC */
    movzbq  3(rPC), %rcx                    # ecx <- BB
    movzbq  2(rPC), %rax                    # eax <- CC
    fldl    VREG_ADDRESS(%rcx)              # %st1 <- fp[vBB]
    fldl    VREG_ADDRESS(%rax)              # %st0 <- fp[vCC]
1:
    fprem
    fstsw   %ax
    sahf
    jp      1b
    fstp    %st(1)
    fstpl   VREG_ADDRESS(rINSTq)            # fp[vAA] <- %st
    CLEAR_WIDE_REF rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_rem_double

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_add_int_2addr: /* 0xb0 */
    ENTRY mterp_op_add_int_2addr
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 32-bit "/2addr" binary operation.  Provide an "instr" line
 * that specifies an instruction that performs "result = r0 op r1".
 * This could be an instruction or a function call.
 *
 * For: add-int/2addr, sub-int/2addr, mul-int/2addr, div-int/2addr,
 *      rem-int/2addr, and-int/2addr, or-int/2addr, xor-int/2addr,
 *      shl-int/2addr, shr-int/2addr, ushr-int/2addr, add-float/2addr,
 *      sub-float/2addr, mul-float/2addr, div-float/2addr, rem-float/2addr
 */
    /* binop/2addr vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # ecx <- A
    GET_VREG %eax, rINSTq                   # eax <- vB
    addl %eax, VREG_ADDRESS(%rcx)
    CLEAR_REF %rcx
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_add_int_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sub_int_2addr: /* 0xb1 */
    ENTRY mterp_op_sub_int_2addr
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 32-bit "/2addr" binary operation.  Provide an "instr" line
 * that specifies an instruction that performs "result = r0 op r1".
 * This could be an instruction or a function call.
 *
 * For: add-int/2addr, sub-int/2addr, mul-int/2addr, div-int/2addr,
 *      rem-int/2addr, and-int/2addr, or-int/2addr, xor-int/2addr,
 *      shl-int/2addr, shr-int/2addr, ushr-int/2addr, add-float/2addr,
 *      sub-float/2addr, mul-float/2addr, div-float/2addr, rem-float/2addr
 */
    /* binop/2addr vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # ecx <- A
    GET_VREG %eax, rINSTq                   # eax <- vB
    subl %eax, VREG_ADDRESS(%rcx)
    CLEAR_REF %rcx
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_sub_int_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_mul_int_2addr: /* 0xb2 */
    ENTRY mterp_op_mul_int_2addr
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /* mul vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # ecx <- A
    GET_VREG %eax, %rcx                     # eax <- vA
    imull   (rFP,rINSTq,4), %eax
    SET_VREG %eax, %rcx
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_mul_int_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_div_int_2addr: /* 0xb3 */
    ENTRY mterp_op_div_int_2addr
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * 32-bit binary div/rem operation.  Handles special case of op1=-1.
 */
    /* div/rem/2addr vA, vB */
    movl    rINST, %ecx                     # rcx <- BA
    sarl    $4, %ecx                       # rcx <- B
    andb    $0xf, rINSTbl                  # rINST <- A
    .if 0
    GET_WIDE_VREG %rax, rINSTq              # eax <- vA
    GET_WIDE_VREG %ecx, %rcx             # ecx <- vB
    .else
    GET_VREG %eax, rINSTq                   # eax <- vA
    GET_VREG %ecx, %rcx                  # ecx <- vB
    .endif
    testl   %ecx, %ecx
    jz      common_errDivideByZero
    cmpl  $-1, %ecx
    je      2f
    cmpl  $2, %ecx
    je      3f
    cdq                                    # rdx:rax <- sign-extended of rax
    idivl   %ecx
1:
    .if 0
    SET_WIDE_VREG %eax, rINSTq           # vA <- result
    .else
    SET_VREG %eax, rINSTq                # vA <- result
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1
2:
    .if 0
    xorl %eax, %eax
    .else
    negl %eax
    .endif
    jmp     1b
3:
    .if 0
    movl %edx, %eax
    .if 0
    shrl $63, %eax
    .else
    shrl $31, %eax
    .endif
    addl %edx, %eax
    andl $-2, %eax
    subl %eax, %edx
    movl %edx, %eax
    .else
    movl %eax, %edx
    .if 0
    shrl $63, %edx
    .else
    shrl $31, %edx
    .endif
    addl %edx, %eax
    sarl %eax
    .endif
    jmp     1b

    END mterp_op_div_int_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_rem_int_2addr: /* 0xb4 */
    ENTRY mterp_op_rem_int_2addr
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * 32-bit binary div/rem operation.  Handles special case of op1=-1.
 */
    /* div/rem/2addr vA, vB */
    movl    rINST, %ecx                     # rcx <- BA
    sarl    $4, %ecx                       # rcx <- B
    andb    $0xf, rINSTbl                  # rINST <- A
    .if 0
    GET_WIDE_VREG %rax, rINSTq              # eax <- vA
    GET_WIDE_VREG %ecx, %rcx             # ecx <- vB
    .else
    GET_VREG %eax, rINSTq                   # eax <- vA
    GET_VREG %ecx, %rcx                  # ecx <- vB
    .endif
    testl   %ecx, %ecx
    jz      common_errDivideByZero
    cmpl  $-1, %ecx
    je      2f
    cmpl  $2, %ecx
    je      3f
    cdq                                    # rdx:rax <- sign-extended of rax
    idivl   %ecx
1:
    .if 0
    SET_WIDE_VREG %edx, rINSTq           # vA <- result
    .else
    SET_VREG %edx, rINSTq                # vA <- result
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1
2:
    .if 1
    xorl %edx, %edx
    .else
    negl %edx
    .endif
    jmp     1b
3:
    .if 1
    movl %eax, %edx
    .if 0
    shrl $63, %edx
    .else
    shrl $31, %edx
    .endif
    addl %eax, %edx
    andl $-2, %edx
    subl %edx, %eax
    movl %eax, %edx
    .else
    movl %edx, %eax
    .if 0
    shrl $63, %eax
    .else
    shrl $31, %eax
    .endif
    addl %eax, %edx
    sarl %edx
    .endif
    jmp     1b

    END mterp_op_rem_int_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_and_int_2addr: /* 0xb5 */
    ENTRY mterp_op_and_int_2addr
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 32-bit "/2addr" binary operation.  Provide an "instr" line
 * that specifies an instruction that performs "result = r0 op r1".
 * This could be an instruction or a function call.
 *
 * For: add-int/2addr, sub-int/2addr, mul-int/2addr, div-int/2addr,
 *      rem-int/2addr, and-int/2addr, or-int/2addr, xor-int/2addr,
 *      shl-int/2addr, shr-int/2addr, ushr-int/2addr, add-float/2addr,
 *      sub-float/2addr, mul-float/2addr, div-float/2addr, rem-float/2addr
 */
    /* binop/2addr vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # ecx <- A
    GET_VREG %eax, rINSTq                   # eax <- vB
    andl %eax, VREG_ADDRESS(%rcx)
    CLEAR_REF %rcx
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_and_int_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_or_int_2addr: /* 0xb6 */
    ENTRY mterp_op_or_int_2addr
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 32-bit "/2addr" binary operation.  Provide an "instr" line
 * that specifies an instruction that performs "result = r0 op r1".
 * This could be an instruction or a function call.
 *
 * For: add-int/2addr, sub-int/2addr, mul-int/2addr, div-int/2addr,
 *      rem-int/2addr, and-int/2addr, or-int/2addr, xor-int/2addr,
 *      shl-int/2addr, shr-int/2addr, ushr-int/2addr, add-float/2addr,
 *      sub-float/2addr, mul-float/2addr, div-float/2addr, rem-float/2addr
 */
    /* binop/2addr vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # ecx <- A
    GET_VREG %eax, rINSTq                   # eax <- vB
    orl %eax, VREG_ADDRESS(%rcx)
    CLEAR_REF %rcx
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_or_int_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_xor_int_2addr: /* 0xb7 */
    ENTRY mterp_op_xor_int_2addr
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 32-bit "/2addr" binary operation.  Provide an "instr" line
 * that specifies an instruction that performs "result = r0 op r1".
 * This could be an instruction or a function call.
 *
 * For: add-int/2addr, sub-int/2addr, mul-int/2addr, div-int/2addr,
 *      rem-int/2addr, and-int/2addr, or-int/2addr, xor-int/2addr,
 *      shl-int/2addr, shr-int/2addr, ushr-int/2addr, add-float/2addr,
 *      sub-float/2addr, mul-float/2addr, div-float/2addr, rem-float/2addr
 */
    /* binop/2addr vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # ecx <- A
    GET_VREG %eax, rINSTq                   # eax <- vB
    xorl %eax, VREG_ADDRESS(%rcx)
    CLEAR_REF %rcx
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_xor_int_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_shl_int_2addr: /* 0xb8 */
    ENTRY mterp_op_shl_int_2addr
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 32-bit "shift/2addr" operation.
 */
    /* shift/2addr vA, vB */
    movl    rINST, %ecx                     # ecx <- BA
    sarl    $4, %ecx                       # ecx <- B
    GET_VREG %ecx, %rcx                     # ecx <- vBB
    andb    $0xf, rINSTbl                  # rINST <- A
    .if 0
    GET_WIDE_VREG %rax, rINSTq              # rax <- vAA
    sall    %cl, %eax                                  # ex: sarl %cl, %eax
    SET_WIDE_VREG %rax, rINSTq
    .else
    GET_VREG %eax, rINSTq                   # eax <- vAA
    sall    %cl, %eax                                  # ex: sarl %cl, %eax
    SET_VREG %eax, rINSTq
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_shl_int_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_shr_int_2addr: /* 0xb9 */
    ENTRY mterp_op_shr_int_2addr
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 32-bit "shift/2addr" operation.
 */
    /* shift/2addr vA, vB */
    movl    rINST, %ecx                     # ecx <- BA
    sarl    $4, %ecx                       # ecx <- B
    GET_VREG %ecx, %rcx                     # ecx <- vBB
    andb    $0xf, rINSTbl                  # rINST <- A
    .if 0
    GET_WIDE_VREG %rax, rINSTq              # rax <- vAA
    sarl    %cl, %eax                                  # ex: sarl %cl, %eax
    SET_WIDE_VREG %rax, rINSTq
    .else
    GET_VREG %eax, rINSTq                   # eax <- vAA
    sarl    %cl, %eax                                  # ex: sarl %cl, %eax
    SET_VREG %eax, rINSTq
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_shr_int_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_ushr_int_2addr: /* 0xba */
    ENTRY mterp_op_ushr_int_2addr
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 32-bit "shift/2addr" operation.
 */
    /* shift/2addr vA, vB */
    movl    rINST, %ecx                     # ecx <- BA
    sarl    $4, %ecx                       # ecx <- B
    GET_VREG %ecx, %rcx                     # ecx <- vBB
    andb    $0xf, rINSTbl                  # rINST <- A
    .if 0
    GET_WIDE_VREG %rax, rINSTq              # rax <- vAA
    shrl    %cl, %eax                                  # ex: sarl %cl, %eax
    SET_WIDE_VREG %rax, rINSTq
    .else
    GET_VREG %eax, rINSTq                   # eax <- vAA
    shrl    %cl, %eax                                  # ex: sarl %cl, %eax
    SET_VREG %eax, rINSTq
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_ushr_int_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_add_long_2addr: /* 0xbb */
    ENTRY mterp_op_add_long_2addr
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 64-bit binary operation.
 */
    /* binop/2addr vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # ecx <- A
    GET_WIDE_VREG %rax, rINSTq              # rax <- vB
    addq %rax,VREG_ADDRESS(%rcx)
    CLEAR_WIDE_REF %rcx
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_add_long_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sub_long_2addr: /* 0xbc */
    ENTRY mterp_op_sub_long_2addr
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 64-bit binary operation.
 */
    /* binop/2addr vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # ecx <- A
    GET_WIDE_VREG %rax, rINSTq              # rax <- vB
    subq %rax,VREG_ADDRESS(%rcx)
    CLEAR_WIDE_REF %rcx
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_sub_long_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_mul_long_2addr: /* 0xbd */
    ENTRY mterp_op_mul_long_2addr
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /* mul vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # ecx <- A
    GET_WIDE_VREG %rax, %rcx                # rax <- vA
    imulq   (rFP,rINSTq,4), %rax
    SET_WIDE_VREG %rax, %rcx
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_mul_long_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_div_long_2addr: /* 0xbe */
    ENTRY mterp_op_div_long_2addr
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * 32-bit binary div/rem operation.  Handles special case of op1=-1.
 */
    /* div/rem/2addr vA, vB */
    movl    rINST, %ecx                     # rcx <- BA
    sarl    $4, %ecx                       # rcx <- B
    andb    $0xf, rINSTbl                  # rINST <- A
    .if 1
    GET_WIDE_VREG %rax, rINSTq              # eax <- vA
    GET_WIDE_VREG %rcx, %rcx             # ecx <- vB
    .else
    GET_VREG %eax, rINSTq                   # eax <- vA
    GET_VREG %rcx, %rcx                  # ecx <- vB
    .endif
    testq   %rcx, %rcx
    jz      common_errDivideByZero
    cmpq  $-1, %rcx
    je      2f
    cmpq  $2, %rcx
    je      3f
    cqo                                    # rdx:rax <- sign-extended of rax
    idivq   %rcx
1:
    .if 1
    SET_WIDE_VREG %rax, rINSTq           # vA <- result
    .else
    SET_VREG %rax, rINSTq                # vA <- result
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1
2:
    .if 0
    xorq %rax, %rax
    .else
    negq %rax
    .endif
    jmp     1b
3:
    .if 0
    movq %rdx, %rax
    .if 1
    shrq $63, %rax
    .else
    shrq $31, %rax
    .endif
    addq %rdx, %rax
    andq $-2, %rax
    subq %rax, %rdx
    movq %rdx, %rax
    .else
    movq %rax, %rdx
    .if 1
    shrq $63, %rdx
    .else
    shrq $31, %rdx
    .endif
    addq %rdx, %rax
    sarq %rax
    .endif
    jmp     1b

    END mterp_op_div_long_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_rem_long_2addr: /* 0xbf */
    ENTRY mterp_op_rem_long_2addr
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * 32-bit binary div/rem operation.  Handles special case of op1=-1.
 */
    /* div/rem/2addr vA, vB */
    movl    rINST, %ecx                     # rcx <- BA
    sarl    $4, %ecx                       # rcx <- B
    andb    $0xf, rINSTbl                  # rINST <- A
    .if 1
    GET_WIDE_VREG %rax, rINSTq              # eax <- vA
    GET_WIDE_VREG %rcx, %rcx             # ecx <- vB
    .else
    GET_VREG %eax, rINSTq                   # eax <- vA
    GET_VREG %rcx, %rcx                  # ecx <- vB
    .endif
    testq   %rcx, %rcx
    jz      common_errDivideByZero
    cmpq  $-1, %rcx
    je      2f
    cmpq  $2, %rcx
    je      3f
    cqo                                    # rdx:rax <- sign-extended of rax
    idivq   %rcx
1:
    .if 1
    SET_WIDE_VREG %rdx, rINSTq           # vA <- result
    .else
    SET_VREG %rdx, rINSTq                # vA <- result
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1
2:
    .if 1
    xorq %rdx, %rdx
    .else
    negq %rdx
    .endif
    jmp     1b
3:
    .if 1
    movq %rax, %rdx
    .if 1
    shrq $63, %rdx
    .else
    shrq $31, %rdx
    .endif
    addq %rax, %rdx
    andq $-2, %rdx
    subq %rdx, %rax
    movq %rax, %rdx
    .else
    movq %rdx, %rax
    .if 1
    shrq $63, %rax
    .else
    shrq $31, %rax
    .endif
    addq %rax, %rdx
    sarq %rdx
    .endif
    jmp     1b

    END mterp_op_rem_long_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_and_long_2addr: /* 0xc0 */
    ENTRY mterp_op_and_long_2addr
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 64-bit binary operation.
 */
    /* binop/2addr vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # ecx <- A
    GET_WIDE_VREG %rax, rINSTq              # rax <- vB
    andq %rax,VREG_ADDRESS(%rcx)
    CLEAR_WIDE_REF %rcx
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_and_long_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_or_long_2addr: /* 0xc1 */
    ENTRY mterp_op_or_long_2addr
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 64-bit binary operation.
 */
    /* binop/2addr vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # ecx <- A
    GET_WIDE_VREG %rax, rINSTq              # rax <- vB
    orq %rax,VREG_ADDRESS(%rcx)
    CLEAR_WIDE_REF %rcx
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_or_long_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_xor_long_2addr: /* 0xc2 */
    ENTRY mterp_op_xor_long_2addr
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 64-bit binary operation.
 */
    /* binop/2addr vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # ecx <- A
    GET_WIDE_VREG %rax, rINSTq              # rax <- vB
    xorq %rax,VREG_ADDRESS(%rcx)
    CLEAR_WIDE_REF %rcx
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_xor_long_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_shl_long_2addr: /* 0xc3 */
    ENTRY mterp_op_shl_long_2addr
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 32-bit "shift/2addr" operation.
 */
    /* shift/2addr vA, vB */
    movl    rINST, %ecx                     # ecx <- BA
    sarl    $4, %ecx                       # ecx <- B
    GET_VREG %ecx, %rcx                     # ecx <- vBB
    andb    $0xf, rINSTbl                  # rINST <- A
    .if 1
    GET_WIDE_VREG %rax, rINSTq              # rax <- vAA
    salq    %cl, %rax                                  # ex: sarl %cl, %eax
    SET_WIDE_VREG %rax, rINSTq
    .else
    GET_VREG %eax, rINSTq                   # eax <- vAA
    salq    %cl, %rax                                  # ex: sarl %cl, %eax
    SET_VREG %eax, rINSTq
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_shl_long_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_shr_long_2addr: /* 0xc4 */
    ENTRY mterp_op_shr_long_2addr
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 32-bit "shift/2addr" operation.
 */
    /* shift/2addr vA, vB */
    movl    rINST, %ecx                     # ecx <- BA
    sarl    $4, %ecx                       # ecx <- B
    GET_VREG %ecx, %rcx                     # ecx <- vBB
    andb    $0xf, rINSTbl                  # rINST <- A
    .if 1
    GET_WIDE_VREG %rax, rINSTq              # rax <- vAA
    sarq    %cl, %rax                                  # ex: sarl %cl, %eax
    SET_WIDE_VREG %rax, rINSTq
    .else
    GET_VREG %eax, rINSTq                   # eax <- vAA
    sarq    %cl, %rax                                  # ex: sarl %cl, %eax
    SET_VREG %eax, rINSTq
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_shr_long_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_ushr_long_2addr: /* 0xc5 */
    ENTRY mterp_op_ushr_long_2addr
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 32-bit "shift/2addr" operation.
 */
    /* shift/2addr vA, vB */
    movl    rINST, %ecx                     # ecx <- BA
    sarl    $4, %ecx                       # ecx <- B
    GET_VREG %ecx, %rcx                     # ecx <- vBB
    andb    $0xf, rINSTbl                  # rINST <- A
    .if 1
    GET_WIDE_VREG %rax, rINSTq              # rax <- vAA
    shrq    %cl, %rax                                  # ex: sarl %cl, %eax
    SET_WIDE_VREG %rax, rINSTq
    .else
    GET_VREG %eax, rINSTq                   # eax <- vAA
    shrq    %cl, %rax                                  # ex: sarl %cl, %eax
    SET_VREG %eax, rINSTq
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_ushr_long_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_add_float_2addr: /* 0xc6 */
    ENTRY mterp_op_add_float_2addr
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    movl    rINST, %ecx                     # ecx <- A+
    andl    $0xf, %ecx                     # ecx <- A
    GET_VREG_XMMs %xmm0, %rcx         # %xmm0 <- 1st src
    sarl    $4, rINST                      # rINST<- B
#ifdef MTERP_USE_AVX
    vaddss VREG_ADDRESS(rINSTq), %xmm0, %xmm0
    SET_VREG_XMMs %xmm0, %rcx         # vAA <- %xmm0
    vpxor    %xmm0, %xmm0, %xmm0
    vmovss %xmm0, VREG_REF_ADDRESS(rINSTq)  # clear ref
#else
    addss VREG_ADDRESS(rINSTq), %xmm0
    SET_VREG_XMMs %xmm0, %rcx         # vAA <- %xmm0
    pxor    %xmm0, %xmm0
    movss %xmm0, VREG_REF_ADDRESS(rINSTq)  # clear ref
#endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_add_float_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sub_float_2addr: /* 0xc7 */
    ENTRY mterp_op_sub_float_2addr
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    movl    rINST, %ecx                     # ecx <- A+
    andl    $0xf, %ecx                     # ecx <- A
    GET_VREG_XMMs %xmm0, %rcx         # %xmm0 <- 1st src
    sarl    $4, rINST                      # rINST<- B
#ifdef MTERP_USE_AVX
    vsubss VREG_ADDRESS(rINSTq), %xmm0, %xmm0
    SET_VREG_XMMs %xmm0, %rcx         # vAA <- %xmm0
    vpxor    %xmm0, %xmm0, %xmm0
    vmovss %xmm0, VREG_REF_ADDRESS(rINSTq)  # clear ref
#else
    subss VREG_ADDRESS(rINSTq), %xmm0
    SET_VREG_XMMs %xmm0, %rcx         # vAA <- %xmm0
    pxor    %xmm0, %xmm0
    movss %xmm0, VREG_REF_ADDRESS(rINSTq)  # clear ref
#endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_sub_float_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_mul_float_2addr: /* 0xc8 */
    ENTRY mterp_op_mul_float_2addr
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    movl    rINST, %ecx                     # ecx <- A+
    andl    $0xf, %ecx                     # ecx <- A
    GET_VREG_XMMs %xmm0, %rcx         # %xmm0 <- 1st src
    sarl    $4, rINST                      # rINST<- B
#ifdef MTERP_USE_AVX
    vmulss VREG_ADDRESS(rINSTq), %xmm0, %xmm0
    SET_VREG_XMMs %xmm0, %rcx         # vAA <- %xmm0
    vpxor    %xmm0, %xmm0, %xmm0
    vmovss %xmm0, VREG_REF_ADDRESS(rINSTq)  # clear ref
#else
    mulss VREG_ADDRESS(rINSTq), %xmm0
    SET_VREG_XMMs %xmm0, %rcx         # vAA <- %xmm0
    pxor    %xmm0, %xmm0
    movss %xmm0, VREG_REF_ADDRESS(rINSTq)  # clear ref
#endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_mul_float_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_div_float_2addr: /* 0xc9 */
    ENTRY mterp_op_div_float_2addr
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    movl    rINST, %ecx                     # ecx <- A+
    andl    $0xf, %ecx                     # ecx <- A
    GET_VREG_XMMs %xmm0, %rcx         # %xmm0 <- 1st src
    sarl    $4, rINST                      # rINST<- B
#ifdef MTERP_USE_AVX
    vdivss VREG_ADDRESS(rINSTq), %xmm0, %xmm0
    SET_VREG_XMMs %xmm0, %rcx         # vAA <- %xmm0
    vpxor    %xmm0, %xmm0, %xmm0
    vmovss %xmm0, VREG_REF_ADDRESS(rINSTq)  # clear ref
#else
    divss VREG_ADDRESS(rINSTq), %xmm0
    SET_VREG_XMMs %xmm0, %rcx         # vAA <- %xmm0
    pxor    %xmm0, %xmm0
    movss %xmm0, VREG_REF_ADDRESS(rINSTq)  # clear ref
#endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_div_float_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_rem_float_2addr: /* 0xca */
    ENTRY mterp_op_rem_float_2addr
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /* rem_float/2addr vA, vB */
    movzbq  rINSTbl, %rcx                   # ecx <- A+
    sarl    $4, rINST                      # rINST <- B
    flds    VREG_ADDRESS(rINSTq)            # vB to fp stack
    andb    $0xf, %cl                      # ecx <- A
    flds    VREG_ADDRESS(%rcx)              # vA to fp stack
1:
    fprem
    fstsw   %ax
    sahf
    jp      1b
    fstp    %st(1)
    fstps   VREG_ADDRESS(%rcx)              # %st to vA
    CLEAR_REF %rcx
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_rem_float_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_add_double_2addr: /* 0xcb */
    ENTRY mterp_op_add_double_2addr
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    movl    rINST, %ecx                     # ecx <- A+
    andl    $0xf, %ecx                     # ecx <- A
    GET_VREG_XMMd %xmm0, %rcx         # %xmm0 <- 1st src
    sarl    $4, rINST                      # rINST<- B
#ifdef MTERP_USE_AVX
    vaddsd VREG_ADDRESS(rINSTq), %xmm0, %xmm0
    SET_VREG_XMMd %xmm0, %rcx         # vAA <- %xmm0
    vpxor    %xmm0, %xmm0, %xmm0
    vmovsd %xmm0, VREG_REF_ADDRESS(rINSTq)  # clear ref
#else
    addsd VREG_ADDRESS(rINSTq), %xmm0
    SET_VREG_XMMd %xmm0, %rcx         # vAA <- %xmm0
    pxor    %xmm0, %xmm0
    movsd %xmm0, VREG_REF_ADDRESS(rINSTq)  # clear ref
#endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_add_double_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sub_double_2addr: /* 0xcc */
    ENTRY mterp_op_sub_double_2addr
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    movl    rINST, %ecx                     # ecx <- A+
    andl    $0xf, %ecx                     # ecx <- A
    GET_VREG_XMMd %xmm0, %rcx         # %xmm0 <- 1st src
    sarl    $4, rINST                      # rINST<- B
#ifdef MTERP_USE_AVX
    vsubsd VREG_ADDRESS(rINSTq), %xmm0, %xmm0
    SET_VREG_XMMd %xmm0, %rcx         # vAA <- %xmm0
    vpxor    %xmm0, %xmm0, %xmm0
    vmovsd %xmm0, VREG_REF_ADDRESS(rINSTq)  # clear ref
#else
    subsd VREG_ADDRESS(rINSTq), %xmm0
    SET_VREG_XMMd %xmm0, %rcx         # vAA <- %xmm0
    pxor    %xmm0, %xmm0
    movsd %xmm0, VREG_REF_ADDRESS(rINSTq)  # clear ref
#endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_sub_double_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_mul_double_2addr: /* 0xcd */
    ENTRY mterp_op_mul_double_2addr
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    movl    rINST, %ecx                     # ecx <- A+
    andl    $0xf, %ecx                     # ecx <- A
    GET_VREG_XMMd %xmm0, %rcx         # %xmm0 <- 1st src
    sarl    $4, rINST                      # rINST<- B
#ifdef MTERP_USE_AVX
    vmulsd VREG_ADDRESS(rINSTq), %xmm0, %xmm0
    SET_VREG_XMMd %xmm0, %rcx         # vAA <- %xmm0
    vpxor    %xmm0, %xmm0, %xmm0
    vmovsd %xmm0, VREG_REF_ADDRESS(rINSTq)  # clear ref
#else
    mulsd VREG_ADDRESS(rINSTq), %xmm0
    SET_VREG_XMMd %xmm0, %rcx         # vAA <- %xmm0
    pxor    %xmm0, %xmm0
    movsd %xmm0, VREG_REF_ADDRESS(rINSTq)  # clear ref
#endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_mul_double_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_div_double_2addr: /* 0xce */
    ENTRY mterp_op_div_double_2addr
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    movl    rINST, %ecx                     # ecx <- A+
    andl    $0xf, %ecx                     # ecx <- A
    GET_VREG_XMMd %xmm0, %rcx         # %xmm0 <- 1st src
    sarl    $4, rINST                      # rINST<- B
#ifdef MTERP_USE_AVX
    vdivsd VREG_ADDRESS(rINSTq), %xmm0, %xmm0
    SET_VREG_XMMd %xmm0, %rcx         # vAA <- %xmm0
    vpxor    %xmm0, %xmm0, %xmm0
    vmovsd %xmm0, VREG_REF_ADDRESS(rINSTq)  # clear ref
#else
    divsd VREG_ADDRESS(rINSTq), %xmm0
    SET_VREG_XMMd %xmm0, %rcx         # vAA <- %xmm0
    pxor    %xmm0, %xmm0
    movsd %xmm0, VREG_REF_ADDRESS(rINSTq)  # clear ref
#endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_div_double_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_rem_double_2addr: /* 0xcf */
    ENTRY mterp_op_rem_double_2addr
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /* rem_double/2addr vA, vB */
    movzbq  rINSTbl, %rcx                   # ecx <- A+
    sarl    $4, rINST                      # rINST <- B
    fldl    VREG_ADDRESS(rINSTq)            # vB to fp stack
    andb    $0xf, %cl                      # ecx <- A
    fldl    VREG_ADDRESS(%rcx)              # vA to fp stack
1:
    fprem
    fstsw   %ax
    sahf
    jp      1b
    fstp    %st(1)
    fstpl   VREG_ADDRESS(%rcx)              # %st to vA
    CLEAR_WIDE_REF %rcx
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END mterp_op_rem_double_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_add_int_lit16: /* 0xd0 */
    ENTRY mterp_op_add_int_lit16
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 32-bit "lit16" binary operation.  Provide an "instr" line
 * that specifies an instruction that performs "result = eax op ecx".
 * This could be an x86 instruction or a function call.  (If the result
 * comes back in a register other than eax, you can override "result".)
 *
 * For: add-int/lit16, rsub-int,
 *      and-int/lit16, or-int/lit16, xor-int/lit16
 */
    /* binop/lit16 vA, vB, #+CCCC */
    movl    rINST, %eax                     # rax <- 000000BA
    sarl    $4, %eax                       # eax <- B
    GET_VREG %eax, %rax                     # eax <- vB
    andb    $0xf, rINSTbl                  # rINST <- A
    movswl  2(rPC), %ecx                    # ecx <- ssssCCCC
    addl    %ecx, %eax                                  # for example: addl %ecx, %eax
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_add_int_lit16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_rsub_int: /* 0xd1 */
    ENTRY mterp_op_rsub_int
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/* this op is "rsub-int", but can be thought of as "rsub-int/lit16" */
/*
 * Generic 32-bit "lit16" binary operation.  Provide an "instr" line
 * that specifies an instruction that performs "result = eax op ecx".
 * This could be an x86 instruction or a function call.  (If the result
 * comes back in a register other than eax, you can override "result".)
 *
 * For: add-int/lit16, rsub-int,
 *      and-int/lit16, or-int/lit16, xor-int/lit16
 */
    /* binop/lit16 vA, vB, #+CCCC */
    movl    rINST, %eax                     # rax <- 000000BA
    sarl    $4, %eax                       # eax <- B
    GET_VREG %eax, %rax                     # eax <- vB
    andb    $0xf, rINSTbl                  # rINST <- A
    movswl  2(rPC), %ecx                    # ecx <- ssssCCCC
    subl    %eax, %ecx                                  # for example: addl %ecx, %eax
    SET_VREG %ecx, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_rsub_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_mul_int_lit16: /* 0xd2 */
    ENTRY mterp_op_mul_int_lit16
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 32-bit "lit16" binary operation.  Provide an "instr" line
 * that specifies an instruction that performs "result = eax op ecx".
 * This could be an x86 instruction or a function call.  (If the result
 * comes back in a register other than eax, you can override "result".)
 *
 * For: add-int/lit16, rsub-int,
 *      and-int/lit16, or-int/lit16, xor-int/lit16
 */
    /* binop/lit16 vA, vB, #+CCCC */
    movl    rINST, %eax                     # rax <- 000000BA
    sarl    $4, %eax                       # eax <- B
    GET_VREG %eax, %rax                     # eax <- vB
    andb    $0xf, rINSTbl                  # rINST <- A
    movswl  2(rPC), %ecx                    # ecx <- ssssCCCC
    imull   %ecx, %eax                                  # for example: addl %ecx, %eax
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_mul_int_lit16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_div_int_lit16: /* 0xd3 */
    ENTRY mterp_op_div_int_lit16
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * 32-bit binary div/rem operation.  Handles special case of op1=-1.
 */
    /* div/rem/lit16 vA, vB, #+CCCC */
    /* Need A in rINST, ssssCCCC in ecx, vB in eax */
    movl    rINST, %eax                     # rax <- 000000BA
    sarl    $4, %eax                       # eax <- B
    GET_VREG %eax, %rax                     # eax <- vB
    movswl  2(rPC), %ecx                    # ecx <- ssssCCCC
    andb    $0xf, rINSTbl                  # rINST <- A
    testl   %ecx, %ecx
    jz      common_errDivideByZero
    cmpl    $-1, %ecx
    je      2f
    cdq                                     # rax <- sign-extended of eax
    idivl   %ecx
1:
    SET_VREG %eax, rINSTq                # vA <- result
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
2:
    .if 0
    xorl    %eax, %eax
    .else
    negl    %eax
    .endif
    jmp     1b

    END mterp_op_div_int_lit16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_rem_int_lit16: /* 0xd4 */
    ENTRY mterp_op_rem_int_lit16
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * 32-bit binary div/rem operation.  Handles special case of op1=-1.
 */
    /* div/rem/lit16 vA, vB, #+CCCC */
    /* Need A in rINST, ssssCCCC in ecx, vB in eax */
    movl    rINST, %eax                     # rax <- 000000BA
    sarl    $4, %eax                       # eax <- B
    GET_VREG %eax, %rax                     # eax <- vB
    movswl  2(rPC), %ecx                    # ecx <- ssssCCCC
    andb    $0xf, rINSTbl                  # rINST <- A
    testl   %ecx, %ecx
    jz      common_errDivideByZero
    cmpl    $-1, %ecx
    je      2f
    cdq                                     # rax <- sign-extended of eax
    idivl   %ecx
1:
    SET_VREG %edx, rINSTq                # vA <- result
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
2:
    .if 1
    xorl    %edx, %edx
    .else
    negl    %edx
    .endif
    jmp     1b

    END mterp_op_rem_int_lit16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_and_int_lit16: /* 0xd5 */
    ENTRY mterp_op_and_int_lit16
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 32-bit "lit16" binary operation.  Provide an "instr" line
 * that specifies an instruction that performs "result = eax op ecx".
 * This could be an x86 instruction or a function call.  (If the result
 * comes back in a register other than eax, you can override "result".)
 *
 * For: add-int/lit16, rsub-int,
 *      and-int/lit16, or-int/lit16, xor-int/lit16
 */
    /* binop/lit16 vA, vB, #+CCCC */
    movl    rINST, %eax                     # rax <- 000000BA
    sarl    $4, %eax                       # eax <- B
    GET_VREG %eax, %rax                     # eax <- vB
    andb    $0xf, rINSTbl                  # rINST <- A
    movswl  2(rPC), %ecx                    # ecx <- ssssCCCC
    andl    %ecx, %eax                                  # for example: addl %ecx, %eax
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_and_int_lit16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_or_int_lit16: /* 0xd6 */
    ENTRY mterp_op_or_int_lit16
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 32-bit "lit16" binary operation.  Provide an "instr" line
 * that specifies an instruction that performs "result = eax op ecx".
 * This could be an x86 instruction or a function call.  (If the result
 * comes back in a register other than eax, you can override "result".)
 *
 * For: add-int/lit16, rsub-int,
 *      and-int/lit16, or-int/lit16, xor-int/lit16
 */
    /* binop/lit16 vA, vB, #+CCCC */
    movl    rINST, %eax                     # rax <- 000000BA
    sarl    $4, %eax                       # eax <- B
    GET_VREG %eax, %rax                     # eax <- vB
    andb    $0xf, rINSTbl                  # rINST <- A
    movswl  2(rPC), %ecx                    # ecx <- ssssCCCC
    orl     %ecx, %eax                                  # for example: addl %ecx, %eax
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_or_int_lit16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_xor_int_lit16: /* 0xd7 */
    ENTRY mterp_op_xor_int_lit16
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 32-bit "lit16" binary operation.  Provide an "instr" line
 * that specifies an instruction that performs "result = eax op ecx".
 * This could be an x86 instruction or a function call.  (If the result
 * comes back in a register other than eax, you can override "result".)
 *
 * For: add-int/lit16, rsub-int,
 *      and-int/lit16, or-int/lit16, xor-int/lit16
 */
    /* binop/lit16 vA, vB, #+CCCC */
    movl    rINST, %eax                     # rax <- 000000BA
    sarl    $4, %eax                       # eax <- B
    GET_VREG %eax, %rax                     # eax <- vB
    andb    $0xf, rINSTbl                  # rINST <- A
    movswl  2(rPC), %ecx                    # ecx <- ssssCCCC
    xorl    %ecx, %eax                                  # for example: addl %ecx, %eax
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_xor_int_lit16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_add_int_lit8: /* 0xd8 */
    ENTRY mterp_op_add_int_lit8
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 32-bit "lit8" binary operation.  Provide an "instr" line
 * that specifies an instruction that performs "result = eax op ecx".
 * This could be an x86 instruction or a function call.  (If the result
 * comes back in a register other than r0, you can override "result".)
 *
 * For: add-int/lit8, rsub-int/lit8
 *      and-int/lit8, or-int/lit8, xor-int/lit8,
 *      shl-int/lit8, shr-int/lit8, ushr-int/lit8
 */
    /* binop/lit8 vAA, vBB, #+CC */
    movzbq  2(rPC), %rax                    # rax <- BB
    movsbl  3(rPC), %ecx                    # rcx <- ssssssCC
    GET_VREG %eax, %rax                     # eax <- rBB
    addl    %ecx, %eax                                  # ex: addl %ecx,%eax
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_add_int_lit8

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_rsub_int_lit8: /* 0xd9 */
    ENTRY mterp_op_rsub_int_lit8
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 32-bit "lit8" binary operation.  Provide an "instr" line
 * that specifies an instruction that performs "result = eax op ecx".
 * This could be an x86 instruction or a function call.  (If the result
 * comes back in a register other than r0, you can override "result".)
 *
 * For: add-int/lit8, rsub-int/lit8
 *      and-int/lit8, or-int/lit8, xor-int/lit8,
 *      shl-int/lit8, shr-int/lit8, ushr-int/lit8
 */
    /* binop/lit8 vAA, vBB, #+CC */
    movzbq  2(rPC), %rax                    # rax <- BB
    movsbl  3(rPC), %ecx                    # rcx <- ssssssCC
    GET_VREG %eax, %rax                     # eax <- rBB
    subl    %eax, %ecx                                  # ex: addl %ecx,%eax
    SET_VREG %ecx, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_rsub_int_lit8

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_mul_int_lit8: /* 0xda */
    ENTRY mterp_op_mul_int_lit8
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 32-bit "lit8" binary operation.  Provide an "instr" line
 * that specifies an instruction that performs "result = eax op ecx".
 * This could be an x86 instruction or a function call.  (If the result
 * comes back in a register other than r0, you can override "result".)
 *
 * For: add-int/lit8, rsub-int/lit8
 *      and-int/lit8, or-int/lit8, xor-int/lit8,
 *      shl-int/lit8, shr-int/lit8, ushr-int/lit8
 */
    /* binop/lit8 vAA, vBB, #+CC */
    movzbq  2(rPC), %rax                    # rax <- BB
    movsbl  3(rPC), %ecx                    # rcx <- ssssssCC
    GET_VREG %eax, %rax                     # eax <- rBB
    imull   %ecx, %eax                                  # ex: addl %ecx,%eax
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_mul_int_lit8

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_div_int_lit8: /* 0xdb */
    ENTRY mterp_op_div_int_lit8
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * 32-bit div/rem "lit8" binary operation.  Handles special case of
 * op0=minint & op1=-1
 */
    /* div/rem/lit8 vAA, vBB, #+CC */
    movzbq  2(rPC), %rax                    # eax <- BB
    movsbl  3(rPC), %ecx                    # ecx <- ssssssCC
    GET_VREG  %eax, %rax                    # eax <- rBB
    testl   %ecx, %ecx
    je      common_errDivideByZero
    cmpl    $-1, %ecx
    je      2f
    cdq                                     # rax <- sign-extended of eax
    idivl   %ecx
1:
    SET_VREG %eax, rINSTq                # vA <- result
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
2:
    .if 0
    xorl    %eax, %eax
    .else
    negl    %eax
    .endif
    jmp     1b

    END mterp_op_div_int_lit8

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_rem_int_lit8: /* 0xdc */
    ENTRY mterp_op_rem_int_lit8
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * 32-bit div/rem "lit8" binary operation.  Handles special case of
 * op0=minint & op1=-1
 */
    /* div/rem/lit8 vAA, vBB, #+CC */
    movzbq  2(rPC), %rax                    # eax <- BB
    movsbl  3(rPC), %ecx                    # ecx <- ssssssCC
    GET_VREG  %eax, %rax                    # eax <- rBB
    testl   %ecx, %ecx
    je      common_errDivideByZero
    cmpl    $-1, %ecx
    je      2f
    cdq                                     # rax <- sign-extended of eax
    idivl   %ecx
1:
    SET_VREG %edx, rINSTq                # vA <- result
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
2:
    .if 1
    xorl    %edx, %edx
    .else
    negl    %edx
    .endif
    jmp     1b

    END mterp_op_rem_int_lit8

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_and_int_lit8: /* 0xdd */
    ENTRY mterp_op_and_int_lit8
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 32-bit "lit8" binary operation.  Provide an "instr" line
 * that specifies an instruction that performs "result = eax op ecx".
 * This could be an x86 instruction or a function call.  (If the result
 * comes back in a register other than r0, you can override "result".)
 *
 * For: add-int/lit8, rsub-int/lit8
 *      and-int/lit8, or-int/lit8, xor-int/lit8,
 *      shl-int/lit8, shr-int/lit8, ushr-int/lit8
 */
    /* binop/lit8 vAA, vBB, #+CC */
    movzbq  2(rPC), %rax                    # rax <- BB
    movsbl  3(rPC), %ecx                    # rcx <- ssssssCC
    GET_VREG %eax, %rax                     # eax <- rBB
    andl    %ecx, %eax                                  # ex: addl %ecx,%eax
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_and_int_lit8

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_or_int_lit8: /* 0xde */
    ENTRY mterp_op_or_int_lit8
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 32-bit "lit8" binary operation.  Provide an "instr" line
 * that specifies an instruction that performs "result = eax op ecx".
 * This could be an x86 instruction or a function call.  (If the result
 * comes back in a register other than r0, you can override "result".)
 *
 * For: add-int/lit8, rsub-int/lit8
 *      and-int/lit8, or-int/lit8, xor-int/lit8,
 *      shl-int/lit8, shr-int/lit8, ushr-int/lit8
 */
    /* binop/lit8 vAA, vBB, #+CC */
    movzbq  2(rPC), %rax                    # rax <- BB
    movsbl  3(rPC), %ecx                    # rcx <- ssssssCC
    GET_VREG %eax, %rax                     # eax <- rBB
    orl     %ecx, %eax                                  # ex: addl %ecx,%eax
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_or_int_lit8

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_xor_int_lit8: /* 0xdf */
    ENTRY mterp_op_xor_int_lit8
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 32-bit "lit8" binary operation.  Provide an "instr" line
 * that specifies an instruction that performs "result = eax op ecx".
 * This could be an x86 instruction or a function call.  (If the result
 * comes back in a register other than r0, you can override "result".)
 *
 * For: add-int/lit8, rsub-int/lit8
 *      and-int/lit8, or-int/lit8, xor-int/lit8,
 *      shl-int/lit8, shr-int/lit8, ushr-int/lit8
 */
    /* binop/lit8 vAA, vBB, #+CC */
    movzbq  2(rPC), %rax                    # rax <- BB
    movsbl  3(rPC), %ecx                    # rcx <- ssssssCC
    GET_VREG %eax, %rax                     # eax <- rBB
    xorl    %ecx, %eax                                  # ex: addl %ecx,%eax
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_xor_int_lit8

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_shl_int_lit8: /* 0xe0 */
    ENTRY mterp_op_shl_int_lit8
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 32-bit "lit8" binary operation.  Provide an "instr" line
 * that specifies an instruction that performs "result = eax op ecx".
 * This could be an x86 instruction or a function call.  (If the result
 * comes back in a register other than r0, you can override "result".)
 *
 * For: add-int/lit8, rsub-int/lit8
 *      and-int/lit8, or-int/lit8, xor-int/lit8,
 *      shl-int/lit8, shr-int/lit8, ushr-int/lit8
 */
    /* binop/lit8 vAA, vBB, #+CC */
    movzbq  2(rPC), %rax                    # rax <- BB
    movsbl  3(rPC), %ecx                    # rcx <- ssssssCC
    GET_VREG %eax, %rax                     # eax <- rBB
    sall    %cl, %eax                                  # ex: addl %ecx,%eax
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_shl_int_lit8

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_shr_int_lit8: /* 0xe1 */
    ENTRY mterp_op_shr_int_lit8
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 32-bit "lit8" binary operation.  Provide an "instr" line
 * that specifies an instruction that performs "result = eax op ecx".
 * This could be an x86 instruction or a function call.  (If the result
 * comes back in a register other than r0, you can override "result".)
 *
 * For: add-int/lit8, rsub-int/lit8
 *      and-int/lit8, or-int/lit8, xor-int/lit8,
 *      shl-int/lit8, shr-int/lit8, ushr-int/lit8
 */
    /* binop/lit8 vAA, vBB, #+CC */
    movzbq  2(rPC), %rax                    # rax <- BB
    movsbl  3(rPC), %ecx                    # rcx <- ssssssCC
    GET_VREG %eax, %rax                     # eax <- rBB
    sarl    %cl, %eax                                  # ex: addl %ecx,%eax
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_shr_int_lit8

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_ushr_int_lit8: /* 0xe2 */
    ENTRY mterp_op_ushr_int_lit8
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic 32-bit "lit8" binary operation.  Provide an "instr" line
 * that specifies an instruction that performs "result = eax op ecx".
 * This could be an x86 instruction or a function call.  (If the result
 * comes back in a register other than r0, you can override "result".)
 *
 * For: add-int/lit8, rsub-int/lit8
 *      and-int/lit8, or-int/lit8, xor-int/lit8,
 *      shl-int/lit8, shr-int/lit8, ushr-int/lit8
 */
    /* binop/lit8 vAA, vBB, #+CC */
    movzbq  2(rPC), %rax                    # rax <- BB
    movsbl  3(rPC), %ecx                    # rcx <- ssssssCC
    GET_VREG %eax, %rax                     # eax <- rBB
    shrl    %cl, %eax                                  # ex: addl %ecx,%eax
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_ushr_int_lit8

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_e3: /* 0xe3 */
    ENTRY mterp_op_unused_e3
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Bail to reference interpreter to throw.
 */
    jmp     MterpFallback

    END mterp_op_unused_e3

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_e4: /* 0xe4 */
    ENTRY mterp_op_unused_e4
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Bail to reference interpreter to throw.
 */
    jmp     MterpFallback

    END mterp_op_unused_e4

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_e5: /* 0xe5 */
    ENTRY mterp_op_unused_e5
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Bail to reference interpreter to throw.
 */
    jmp     MterpFallback

    END mterp_op_unused_e5

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_e6: /* 0xe6 */
    ENTRY mterp_op_unused_e6
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Bail to reference interpreter to throw.
 */
    jmp     MterpFallback

    END mterp_op_unused_e6

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_e7: /* 0xe7 */
    ENTRY mterp_op_unused_e7
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Bail to reference interpreter to throw.
 */
    jmp     MterpFallback

    END mterp_op_unused_e7

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_e8: /* 0xe8 */
    ENTRY mterp_op_unused_e8
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Bail to reference interpreter to throw.
 */
    jmp     MterpFallback

    END mterp_op_unused_e8

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_e9: /* 0xe9 */
    ENTRY mterp_op_unused_e9
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Bail to reference interpreter to throw.
 */
    jmp     MterpFallback

    END mterp_op_unused_e9

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_ea: /* 0xea */
    ENTRY mterp_op_unused_ea
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Bail to reference interpreter to throw.
 */
    jmp     MterpFallback

    END mterp_op_unused_ea

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_eb: /* 0xeb */
    ENTRY mterp_op_unused_eb
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Bail to reference interpreter to throw.
 */
    jmp     MterpFallback

    END mterp_op_unused_eb

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_ec: /* 0xec */
    ENTRY mterp_op_unused_ec
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Bail to reference interpreter to throw.
 */
    jmp     MterpFallback

    END mterp_op_unused_ec

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_ed: /* 0xed */
    ENTRY mterp_op_unused_ed
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Bail to reference interpreter to throw.
 */
    jmp     MterpFallback

    END mterp_op_unused_ed

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_ee: /* 0xee */
    ENTRY mterp_op_unused_ee
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Bail to reference interpreter to throw.
 */
    jmp     MterpFallback

    END mterp_op_unused_ee

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_ef: /* 0xef */
    ENTRY mterp_op_unused_ef
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Bail to reference interpreter to throw.
 */
    jmp     MterpFallback

    END mterp_op_unused_ef

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_f0: /* 0xf0 */
    ENTRY mterp_op_unused_f0
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Bail to reference interpreter to throw.
 */
    jmp     MterpFallback

    END mterp_op_unused_f0

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_f1: /* 0xf1 */
    ENTRY mterp_op_unused_f1
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Bail to reference interpreter to throw.
 */
    jmp     MterpFallback

    END mterp_op_unused_f1

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_f2: /* 0xf2 */
    ENTRY mterp_op_unused_f2
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Bail to reference interpreter to throw.
 */
    jmp     MterpFallback

    END mterp_op_unused_f2

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_f3: /* 0xf3 */
    ENTRY mterp_op_unused_f3
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Bail to reference interpreter to throw.
 */
    jmp     MterpFallback

    END mterp_op_unused_f3

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_f4: /* 0xf4 */
    ENTRY mterp_op_unused_f4
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Bail to reference interpreter to throw.
 */
    jmp     MterpFallback

    END mterp_op_unused_f4

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_f5: /* 0xf5 */
    ENTRY mterp_op_unused_f5
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Bail to reference interpreter to throw.
 */
    jmp     MterpFallback

    END mterp_op_unused_f5

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_f6: /* 0xf6 */
    ENTRY mterp_op_unused_f6
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Bail to reference interpreter to throw.
 */
    jmp     MterpFallback

    END mterp_op_unused_f6

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_f7: /* 0xf7 */
    ENTRY mterp_op_unused_f7
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Bail to reference interpreter to throw.
 */
    jmp     MterpFallback

    END mterp_op_unused_f7

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_f8: /* 0xf8 */
    ENTRY mterp_op_unused_f8
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Bail to reference interpreter to throw.
 */
    jmp     MterpFallback

    END mterp_op_unused_f8

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_f9: /* 0xf9 */
    ENTRY mterp_op_unused_f9
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Bail to reference interpreter to throw.
 */
    jmp     MterpFallback

    END mterp_op_unused_f9

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_polymorphic: /* 0xfa */
    ENTRY mterp_op_invoke_polymorphic
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /*
     * invoke-polymorphic handler wrapper.
     */
    /* op {vC, vD, vE, vF, vG}, meth@BBBB, proto@HHHH */
    /* op {vCCCC..v(CCCC+AA-1)}, meth@BBBB, proto@HHHH */
    .extern MterpInvokePolymorphic
    EXPORT_PC
    movq    rSELF, OUT_ARG0
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG1
    movq    rPC, OUT_ARG2
    REFRESH_INST 250
    movl    rINST, OUT_32_ARG3
    call    SYMBOL(MterpInvokePolymorphic)
    testb   %al, %al
    jz      MterpException
    ADVANCE_PC 4
    movq    rSELF, %rax
    cmpb    LITERAL(0), THREAD_USE_MTERP_OFFSET(%rax)
    jz      MterpFallback
    FETCH_INST
    GOTO_NEXT

    END mterp_op_invoke_polymorphic

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_polymorphic_range: /* 0xfb */
    ENTRY mterp_op_invoke_polymorphic_range
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /*
     * invoke-polymorphic handler wrapper.
     */
    /* op {vC, vD, vE, vF, vG}, meth@BBBB, proto@HHHH */
    /* op {vCCCC..v(CCCC+AA-1)}, meth@BBBB, proto@HHHH */
    .extern MterpInvokePolymorphicRange
    EXPORT_PC
    movq    rSELF, OUT_ARG0
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG1
    movq    rPC, OUT_ARG2
    REFRESH_INST 251
    movl    rINST, OUT_32_ARG3
    call    SYMBOL(MterpInvokePolymorphicRange)
    testb   %al, %al
    jz      MterpException
    ADVANCE_PC 4
    movq    rSELF, %rax
    cmpb    LITERAL(0), THREAD_USE_MTERP_OFFSET(%rax)
    jz      MterpFallback
    FETCH_INST
    GOTO_NEXT

    END mterp_op_invoke_polymorphic_range

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_custom: /* 0xfc */
    ENTRY mterp_op_invoke_custom
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic invoke handler wrapper.
 */
    /* op vB, {vD, vE, vF, vG, vA}, class@CCCC */
    /* op {vCCCC..v(CCCC+AA-1)}, meth@BBBB */
    .extern MterpInvokeCustom
    EXPORT_PC
    movq    rSELF, OUT_ARG0
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG1
    movq    rPC, OUT_ARG2
    REFRESH_INST 252
    movl    rINST, OUT_32_ARG3
    call    SYMBOL(MterpInvokeCustom)
    testb   %al, %al
    jz      MterpException
    ADVANCE_PC 3
    movq    rSELF, %rax
    cmpb    LITERAL(0), THREAD_USE_MTERP_OFFSET(%rax)
    jz      MterpFallback
    FETCH_INST
    GOTO_NEXT

    END mterp_op_invoke_custom

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_custom_range: /* 0xfd */
    ENTRY mterp_op_invoke_custom_range
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

/*
 * Generic invoke handler wrapper.
 */
    /* op vB, {vD, vE, vF, vG, vA}, class@CCCC */
    /* op {vCCCC..v(CCCC+AA-1)}, meth@BBBB */
    .extern MterpInvokeCustomRange
    EXPORT_PC
    movq    rSELF, OUT_ARG0
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG1
    movq    rPC, OUT_ARG2
    REFRESH_INST 253
    movl    rINST, OUT_32_ARG3
    call    SYMBOL(MterpInvokeCustomRange)
    testb   %al, %al
    jz      MterpException
    ADVANCE_PC 3
    movq    rSELF, %rax
    cmpb    LITERAL(0), THREAD_USE_MTERP_OFFSET(%rax)
    jz      MterpFallback
    FETCH_INST
    GOTO_NEXT

    END mterp_op_invoke_custom_range

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_const_method_handle: /* 0xfe */
    ENTRY mterp_op_const_method_handle
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /* const/class vAA, type@BBBB */
    /* const/method-handle vAA, method_handle@BBBB */
    /* const/method-type vAA, proto@BBBB */
    /* const/string vAA, string@@BBBB */
    .extern MterpConstMethodHandle
    EXPORT_PC
    movzwq  2(rPC), OUT_ARG0                # eax <- OUT_ARG0
    movq    rINSTq, OUT_ARG1
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG2
    movq    rSELF, OUT_ARG3
    call    SYMBOL(MterpConstMethodHandle)                 # (index, tgt_reg, shadow_frame, self)
    testb   %al, %al
    jnz     MterpPossibleException
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_const_method_handle

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_const_method_type: /* 0xff */
    ENTRY mterp_op_const_method_type
    #if !defined(NDEBUG)
    call    SYMBOL(mterp_dchecks_before_helper)
    #endif

    /* const/class vAA, type@BBBB */
    /* const/method-handle vAA, method_handle@BBBB */
    /* const/method-type vAA, proto@BBBB */
    /* const/string vAA, string@@BBBB */
    .extern MterpConstMethodType
    EXPORT_PC
    movzwq  2(rPC), OUT_ARG0                # eax <- OUT_ARG0
    movq    rINSTq, OUT_ARG1
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG2
    movq    rSELF, OUT_ARG3
    call    SYMBOL(MterpConstMethodType)                 # (index, tgt_reg, shadow_frame, self)
    testb   %al, %al
    jnz     MterpPossibleException
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END mterp_op_const_method_type

    .balign MTERP_HANDLER_SIZE

    OBJECT_TYPE(artMterpAsmInstructionEnd)
    ASM_HIDDEN SYMBOL(artMterpAsmInstructionEnd)
    .global SYMBOL(artMterpAsmInstructionEnd)
SYMBOL(artMterpAsmInstructionEnd):

    ENTRY mterp_dchecks_before_helper
    // Call C++ to do debug checks and return to the handler using tail call.
    .extern MterpCheckBefore
    popq    %rax                     # Return address (the instuction handler).
    REFRESH_IBASE
    movq    rSELF, OUT_ARG0
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG1
    movq    rPC, OUT_ARG2
    pushq   %rax                     # Return address for the tail call.
    jmp     SYMBOL(MterpCheckBefore) # (self, shadow_frame, dex_pc_ptr)

    END mterp_dchecks_before_helper
    ENTRY MterpHelpers

/*
 * ===========================================================================
 *  Common subroutines and data
 * ===========================================================================
 */

    .text
    .align  2

/*
 * We've detected a condition that will result in an exception, but the exception
 * has not yet been thrown.  Just bail out to the reference interpreter to deal with it.
 * TUNING: for consistency, we may want to just go ahead and handle these here.
 */
common_errDivideByZero:
    EXPORT_PC
#if MTERP_LOGGING
    movq    rSELF, OUT_ARG0
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG1
    call    SYMBOL(MterpLogDivideByZeroException)
#endif
    jmp     MterpCommonFallback

common_errArrayIndex:
    EXPORT_PC
#if MTERP_LOGGING
    movq    rSELF, OUT_ARG0
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG1
    call    SYMBOL(MterpLogArrayIndexException)
#endif
    jmp     MterpCommonFallback

common_errNegativeArraySize:
    EXPORT_PC
#if MTERP_LOGGING
    movq    rSELF, OUT_ARG0
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG1
    call    SYMBOL(MterpLogNegativeArraySizeException)
#endif
    jmp     MterpCommonFallback

common_errNoSuchMethod:
    EXPORT_PC
#if MTERP_LOGGING
    movq    rSELF, OUT_ARG0
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG1
    call    SYMBOL(MterpLogNoSuchMethodException)
#endif
    jmp     MterpCommonFallback

common_errNullObject:
    EXPORT_PC
#if MTERP_LOGGING
    movq    rSELF, OUT_ARG0
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG1
    call    SYMBOL(MterpLogNullObjectException)
#endif
    jmp     MterpCommonFallback

common_exceptionThrown:
    EXPORT_PC
#if MTERP_LOGGING
    movq    rSELF, OUT_ARG0
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG1
    call    SYMBOL(MterpLogExceptionThrownException)
#endif
    jmp     MterpCommonFallback

MterpSuspendFallback:
    EXPORT_PC
#if MTERP_LOGGING
    movq    rSELF, OUT_ARG0
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG1
    movl    THREAD_FLAGS_OFFSET(OUT_ARG0), OUT_32_ARG2
    call    SYMBOL(MterpLogSuspendFallback)
#endif
    jmp     MterpCommonFallback

/*
 * If we're here, something is out of the ordinary.  If there is a pending
 * exception, handle it.  Otherwise, roll back and retry with the reference
 * interpreter.
 */
MterpPossibleException:
    movq    rSELF, %rcx
    cmpq    $0, THREAD_EXCEPTION_OFFSET(%rcx)
    jz      MterpFallback
    /* intentional fallthrough - handle pending exception. */

/*
 * On return from a runtime helper routine, we've found a pending exception.
 * Can we handle it here - or need to bail out to caller?
 *
 */
MterpException:
    movq    rSELF, OUT_ARG0
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG1
    call    SYMBOL(MterpHandleException)
    testb   %al, %al
    jz      MterpExceptionReturn
    movq    OFF_FP_DEX_INSTRUCTIONS(rFP), %rax
    mov     OFF_FP_DEX_PC(rFP), %ecx
    leaq    (%rax, %rcx, 2), rPC
    movq    rPC, OFF_FP_DEX_PC_PTR(rFP)
    /* Do we need to switch interpreters? */
    movq    rSELF, %rax
    cmpb    LITERAL(0), THREAD_USE_MTERP_OFFSET(%rax)
    jz      MterpFallback
    /* resume execution at catch block */
    REFRESH_IBASE
    FETCH_INST
    GOTO_NEXT
    /* NOTE: no fallthrough */

/*
 * Common handling for branches with support for Jit profiling.
 * On entry:
 *    rINST          <= signed offset
 *    rPROFILE       <= signed hotness countdown (expanded to 32 bits)
 *    condition bits <= set to establish sign of offset (use "NoFlags" entry if not)
 *
 * We have quite a few different cases for branch profiling, OSR detection and
 * suspend check support here.
 *
 * Taken backward branches:
 *    If profiling active, do hotness countdown and report if we hit zero.
 *    If in osr check mode, see if our target is a compiled loop header entry and do OSR if so.
 *    Is there a pending suspend request?  If so, suspend.
 *
 * Taken forward branches and not-taken backward branches:
 *    If in osr check mode, see if our target is a compiled loop header entry and do OSR if so.
 *
 * Our most common case is expected to be a taken backward branch with active jit profiling,
 * but no full OSR check and no pending suspend request.
 * Next most common case is not-taken branch with no full OSR check.
 *
 */
MterpCommonTakenBranch:
    jg      .L_forward_branch               # don't add forward branches to hotness
/*
 * We need to subtract 1 from positive values and we should not see 0 here,
 * so we may use the result of the comparison with -1.
 */
#if JIT_CHECK_OSR != -1
#  error "JIT_CHECK_OSR must be -1."
#endif
    cmpl    $JIT_CHECK_OSR, rPROFILE
    je      .L_osr_check
    decl    rPROFILE
    je      .L_add_batch                    # counted down to zero - report
.L_resume_backward_branch:
    movq    rSELF, %rax
    testl   $(THREAD_SUSPEND_OR_CHECKPOINT_REQUEST), THREAD_FLAGS_OFFSET(%rax)
    REFRESH_IBASE_REG %rax
    leaq    (rPC, rINSTq, 2), rPC
    FETCH_INST
    jnz     .L_suspend_request_pending
    GOTO_NEXT

.L_suspend_request_pending:
    EXPORT_PC
    movq    rSELF, OUT_ARG0
    call    SYMBOL(MterpSuspendCheck)       # (self)
    testb   %al, %al
    jnz     MterpFallback
    REFRESH_IBASE                           # might have changed during suspend
    GOTO_NEXT

.L_no_count_backwards:
    cmpl    $JIT_CHECK_OSR, rPROFILE         # possible OSR re-entry?
    jne     .L_resume_backward_branch
.L_osr_check:
    EXPORT_PC
    movq    rSELF, OUT_ARG0
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG1
    movq    rINSTq, OUT_ARG2
    call    SYMBOL(MterpMaybeDoOnStackReplacement) # (self, shadow_frame, offset)
    testb   %al, %al
    jz      .L_resume_backward_branch
    jmp     MterpOnStackReplacement

.L_forward_branch:
    cmpl    $JIT_CHECK_OSR, rPROFILE         # possible OSR re-entry?
    je      .L_check_osr_forward
.L_resume_forward_branch:
    leaq    (rPC, rINSTq, 2), rPC
    FETCH_INST
    GOTO_NEXT

.L_check_osr_forward:
    EXPORT_PC
    movq    rSELF, OUT_ARG0
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG1
    movq    rINSTq, OUT_ARG2
    call    SYMBOL(MterpMaybeDoOnStackReplacement) # (self, shadow_frame, offset)
    testb   %al, %al
    jz      .L_resume_forward_branch
    jmp     MterpOnStackReplacement

.L_add_batch:
    movl    rPROFILE, %eax
    movq    OFF_FP_METHOD(rFP), OUT_ARG0
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG1
    movw    %ax, OFF_FP_COUNTDOWN_OFFSET(rFP)
    movq    rSELF, OUT_ARG2
    call    SYMBOL(MterpAddHotnessBatch)    # (method, shadow_frame, self)
    movswl  %ax, rPROFILE
    jmp     .L_no_count_backwards

/*
 * Entered from the conditional branch handlers when OSR check request active on
 * not-taken path.  All Dalvik not-taken conditional branch offsets are 2.
 */
.L_check_not_taken_osr:
    EXPORT_PC
    movq    rSELF, OUT_ARG0
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG1
    movl    $2, OUT_32_ARG2
    call    SYMBOL(MterpMaybeDoOnStackReplacement) # (self, shadow_frame, offset)
    testb   %al, %al
    jnz     MterpOnStackReplacement
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

/*
 * On-stack replacement has happened, and now we've returned from the compiled method.
 */
MterpOnStackReplacement:
#if MTERP_LOGGING
    movq    rSELF, OUT_ARG0
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG1
    movl    rINST, OUT_32_ARG2
    call    SYMBOL(MterpLogOSR)
#endif
    movl    $1, %eax
    jmp     MterpDone

/*
 * Bail out to reference interpreter.
 */
MterpFallback:
    EXPORT_PC
#if MTERP_LOGGING
    movq    rSELF, OUT_ARG0
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG1
    call    SYMBOL(MterpLogFallback)
#endif
MterpCommonFallback:
    xorl    %eax, %eax
    jmp     MterpDone

/*
 * On entry:
 *  uint32_t* rFP  (should still be live, pointer to base of vregs)
 */
MterpExceptionReturn:
    movl    $1, %eax
    jmp     MterpDone
MterpReturn:
    movq    OFF_FP_RESULT_REGISTER(rFP), %rdx
    movq    %rax, (%rdx)
    movl    $1, %eax
MterpDone:
/*
 * At this point, we expect rPROFILE to be non-zero.  If negative, hotness is disabled or we're
 * checking for OSR.  If greater than zero, we might have unreported hotness to register
 * (the difference between the ending rPROFILE and the cached hotness counter).  rPROFILE
 * should only reach zero immediately after a hotness decrement, and is then reset to either
 * a negative special state or the new non-zero countdown value.
 */
    testl   rPROFILE, rPROFILE
    jle     MRestoreFrame                   # if > 0, we may have some counts to report.

    movl    %eax, rINST                     # stash return value
    /* Report cached hotness counts */
    movl    rPROFILE, %eax
    movq    OFF_FP_METHOD(rFP), OUT_ARG0
    leaq    OFF_FP_SHADOWFRAME(rFP), OUT_ARG1
    movw    %ax, OFF_FP_COUNTDOWN_OFFSET(rFP)
    movq    rSELF, OUT_ARG2
    call    SYMBOL(MterpAddHotnessBatch)    # (method, shadow_frame, self)
    movl    rINST, %eax                     # restore return value

    /* pop up frame */
MRestoreFrame:
    addq    $FRAME_SIZE, %rsp
    .cfi_adjust_cfa_offset -FRAME_SIZE

    /* Restore callee save register */
    POP %r15
    POP %r14
    POP %r13
    POP %r12
    POP %rbp
    POP %rbx
    ret
    .cfi_endproc
    END MterpHelpers

