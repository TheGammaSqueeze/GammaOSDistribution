/* DO NOT EDIT: This file was generated by gen-mterp.py. */
/*
 * Copyright (C) 2020 The Android Open Source Project
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/*
 * This is a #include, not a %include, because we want the C pre-processor
 * to expand the macros into assembler assignment statements.
 */
#include "asm_support.h"
#include "arch/arm/asm_support_arm.S"

/**
 * ARM EABI general notes:
 *
 * r0-r3 hold first 4 args to a method; they are not preserved across method calls
 * r4-r8 are available for general use
 * r9 is given special treatment in some situations, but not for us
 * r10 (sl) seems to be generally available
 * r11 (fp) is used by gcc (unless -fomit-frame-pointer is set)
 * r12 (ip) is scratch -- not preserved across method calls
 * r13 (sp) should be managed carefully in case a signal arrives
 * r14 (lr) must be preserved
 * r15 (pc) can be tinkered with directly
 *
 * r0 holds returns of <= 4 bytes
 * r0-r1 hold returns of 8 bytes, low word in r0
 *
 * Callee must save/restore r4+ (except r12) if it modifies them.  If VFP
 * is present, registers s16-s31 (a/k/a d8-d15, a/k/a q4-q7) must be preserved,
 * s0-s15 (d0-d7, q0-a3) do not need to be.
 *
 * Stack is "full descending".  Only the arguments that don't fit in the first 4
 * registers are placed on the stack.  "sp" points at the first stacked argument
 * (i.e. the 5th arg).
 *
 * Native ABI uses soft-float, single-precision results are in r0,
 * double-precision results in r0-r1.
 *
 * In the EABI, "sp" must be 64-bit aligned on entry to a function, and any
 * 64-bit quantities (long long, double) must be 64-bit aligned.
 *
 * Nterp notes:
 *
 * The following registers have fixed assignments:
 *
 *   reg nick      purpose
 *   r5  rFP       interpreted frame pointer, used for accessing locals and args
 *   r6  rREFS     base of object references of dex registers
 *   r7  rINST     first 16-bit code unit of current instruction
 *   r8  rMR       marking register
 *   r9  rSELF     self (Thread) pointer
 *   r10 rIBASE    interpreted instruction base pointer, used for computed goto
 *   r11 rPC       interpreted program counter, used for fetching instructions
 *
 *   r4, ip, and lr can be used as temporary
 *
 * Note that r4 is a callee-save register in ARM EABI, but not in managed code.
 *
 */

/* single-purpose registers, given names for clarity */
#define CFI_DEX  11 // DWARF register number of the register holding dex-pc (rPC).
#define CFI_TMP  0  // DWARF register number of the first argument register (r0).
#define CFI_REFS 6
#define rFP      r5
#define rREFS    r6
#define rINST    r7
#define rSELF    r9
#define rIBASE   r10
#define rPC      r11

// To avoid putting ifdefs arond the use of rMR, make sure it's defined.
// IsNterpSupported returns false for configurations that don't have rMR (typically CMS).
#ifndef rMR
#define rMR r8
#endif

// Temporary registers while setting up a frame.
#define rNEW_FP   r8
#define rNEW_REFS r10
#define CFI_NEW_REFS 10

#define CALLEE_SAVES_SIZE (9 * 4 + 16 * 4)

// +4 for the ArtMethod of the caller.
#define OFFSET_TO_FIRST_ARGUMENT_IN_STACK (CALLEE_SAVES_SIZE + 4)

/*
 * Fetch the next instruction from rPC into rINST.  Does not advance rPC.
 */
.macro FETCH_INST
    ldrh    rINST, [rPC]
.endm

/*
 * Fetch the next instruction from the specified offset.  Advances rPC
 * to point to the next instruction.  "count" is in 16-bit code units.
 *
 * Because of the limited size of immediate constants on ARM, this is only
 * suitable for small forward movements (i.e. don't try to implement "goto"
 * with this).
 *
 * This must come AFTER anything that can throw an exception, or the
 * exception catch may miss.  (This also implies that it must come after
 * EXPORT_PC.)
 */
.macro FETCH_ADVANCE_INST count
    ldrh    rINST, [rPC, #((\count)*2)]!
.endm

/*
 * Similar to FETCH_ADVANCE_INST, but does not update xPC.  Used to load
 * rINST ahead of possible exception point.  Be sure to manually advance xPC
 * later.
 */
.macro PREFETCH_INST count
    ldrh    rINST, [rPC, #((\count)*2)]
.endm

/* Advance xPC by some number of code units. */
.macro ADVANCE count
  add  rPC, #((\count)*2)
.endm

/*
 * Fetch the next instruction from an offset specified by "reg" and advance xPC.
 * xPC to point to the next instruction.  "reg" must specify the distance
 * in bytes, *not* 16-bit code units, and may be a signed value.
 */
.macro FETCH_ADVANCE_INST_RB reg
    ldrh    rINST, [rPC, \reg]!
.endm

/*
 * Fetch a half-word code unit from an offset past the current PC.  The
 * "count" value is in 16-bit code units.  Does not advance xPC.
 *
 * The "_S" variant works the same but treats the value as signed.
 */
.macro FETCH reg, count
    ldrh    \reg, [rPC, #((\count)*2)]
.endm

.macro FETCH_S reg, count
    ldrsh   \reg, [rPC, #((\count)*2)]
.endm

/*
 * Fetch one byte from an offset past the current PC.  Pass in the same
 * "count" as you would for FETCH, and an additional 0/1 indicating which
 * byte of the halfword you want (lo/hi).
 */
.macro FETCH_B reg, count, byte
    ldrb     \reg, [rPC, #((\count)*2+(\byte))]
.endm

/*
 * Put the instruction's opcode field into the specified register.
 */
.macro GET_INST_OPCODE reg
    and     \reg, rINST, #255
.endm

/*
 * Begin executing the opcode in _reg.  Clobbers reg
 */

.macro GOTO_OPCODE reg
    add     pc, rIBASE, \reg, lsl #MTERP_HANDLER_SIZE_LOG2
.endm

/*
 * Get/set value from a Dalvik register.
 */
.macro GET_VREG reg, vreg
    ldr     \reg, [rFP, \vreg, lsl #2]
.endm
.macro GET_VREG_OBJECT reg, vreg
    ldr     \reg, [rREFS, \vreg, lsl #2]
.endm
.macro SET_VREG reg, vreg
    str     \reg, [rFP, \vreg, lsl #2]
    mov     \reg, #0
    str     \reg, [rREFS, \vreg, lsl #2]
.endm
.macro SET_VREG_OBJECT reg, vreg
    str     \reg, [rFP, \vreg, lsl #2]
    str     \reg, [rREFS, \vreg, lsl #2]
.endm
.macro SET_VREG_FLOAT reg, vreg, tmpreg
    add     \tmpreg, rFP, \vreg, lsl #2
    vstr    \reg, [\tmpreg]
    mov     \tmpreg, #0
    str     \tmpreg, [rREFS, \vreg, lsl #2]
.endm
.macro GET_VREG_WIDE_BY_ADDR reg0, reg1, addr
    ldmia \addr, {\reg0, \reg1}
.endm
.macro SET_VREG_WIDE_BY_ADDR reg0, reg1, addr
    stmia \addr, {\reg0, \reg1}
.endm
.macro GET_VREG_FLOAT sreg, vreg
    ldr  \vreg, [rFP, \vreg, lsl #2]
    vmov \sreg, \vreg
.endm
.macro GET_VREG_FLOAT_BY_ADDR reg, addr
    vldr \reg, [\addr]
.endm
.macro SET_VREG_FLOAT_BY_ADDR reg, addr
    vstr \reg, [\addr]
.endm
.macro GET_VREG_DOUBLE_BY_ADDR reg, addr
    vldr \reg, [\addr]
.endm
.macro SET_VREG_DOUBLE_BY_ADDR reg, addr
    vstr \reg, [\addr]
.endm
.macro SET_VREG_SHADOW reg, vreg
    str     \reg, [rREFS, \vreg, lsl #2]
.endm
.macro CLEAR_SHADOW_PAIR vreg, tmp1, tmp2
    mov     \tmp1, #0
    add     \tmp2, \vreg, #1
    SET_VREG_SHADOW \tmp1, \vreg
    SET_VREG_SHADOW \tmp1, \tmp2
.endm
.macro VREG_INDEX_TO_ADDR reg, vreg
    add     \reg, rFP, \vreg, lsl #2
.endm

// An assembly entry that has a OatQuickMethodHeader prefix.
.macro OAT_ENTRY name, end
    .arm
    .type \name, #function
    .hidden \name
    .global \name
    .balign 16
    // Padding of 3 * 8 bytes to get 16 bytes alignment of code entry.
    .long 0
    .long 0
    .long 0
    // OatQuickMethodHeader. Note that the top two bits must be clear.
    .long (\end - \name)
\name:
.endm

.macro SIZE name
    .size \name, .-\name
.endm

.macro NAME_START name
    .arm
    .type \name, #function
    .hidden \name  // Hide this as a global symbol, so we do not incur plt calls.
    .global \name
    /* Cache alignment for function entry */
    .balign 16
\name:
.endm

.macro NAME_END name
  SIZE \name
.endm

// Macro for defining entrypoints into runtime. We don't need to save registers
// (we're not holding references there), but there is no
// kDontSave runtime method. So just use the kSaveRefsOnly runtime method.
.macro NTERP_TRAMPOLINE name, helper
ENTRY \name
  SETUP_SAVE_REFS_ONLY_FRAME ip
  bl \helper
  RESTORE_SAVE_REFS_ONLY_FRAME
  REFRESH_MARKING_REGISTER
  RETURN_OR_DELIVER_PENDING_EXCEPTION
END \name
.endm

.macro CLEAR_STATIC_VOLATILE_MARKER reg
  and \reg, \reg, #-2
.endm

.macro CLEAR_INSTANCE_VOLATILE_MARKER reg
  rsb \reg, \reg, #0
.endm

.macro EXPORT_PC
    str    rPC, [rREFS, #-8]
.endm

.macro BRANCH
    // Update method counter and do a suspend check if the branch is negative.
    cmp rINST, #0
    blt 2f
1:
    add r2, rINST, rINST                // r2<- byte offset
    FETCH_ADVANCE_INST_RB r2            // update xPC, load rINST
    GET_INST_OPCODE ip                  // extract opcode from rINST
    GOTO_OPCODE ip                      // jump to next instruction
2:
    ldr r0, [sp]
    ldrh r2, [r0, #ART_METHOD_HOTNESS_COUNT_OFFSET]
    add r2, r2, #1
    ubfx r2, r2, #0, #NTERP_HOTNESS_BITS
    strh r2, [r0, #ART_METHOD_HOTNESS_COUNT_OFFSET]
    // If the counter overflows, handle this in the runtime.
    cmp r2, #0
    beq NterpHandleHotnessOverflow
    // Otherwise, do a suspend check.
    ldr r0, [rSELF, #THREAD_FLAGS_OFFSET]
    ands r0, r0, #THREAD_SUSPEND_OR_CHECKPOINT_REQUEST
    beq 1b
    EXPORT_PC
    bl    art_quick_test_suspend
    b 1b
.endm

// Expects:
// - ip and lr to be available.
// Outputs:
// - \registers contains the dex registers size
// - \outs contains the outs size
// - if load_ins is 1, \ins contains the ins
// - \code_item is replaced with a pointer to the instructions
.macro FETCH_CODE_ITEM_INFO code_item, registers, outs, ins, load_ins
    tst \code_item, #1
    beq 5f
    bic \code_item, \code_item, #1 // Remove the extra bit that marks it's a compact dex file
    ldrh lr, [\code_item, #COMPACT_CODE_ITEM_FIELDS_OFFSET]
    ubfx \registers, lr, #COMPACT_CODE_ITEM_REGISTERS_SIZE_SHIFT, #4
    ubfx \outs, lr, #COMPACT_CODE_ITEM_OUTS_SIZE_SHIFT, #4
    .if \load_ins
    ubfx \ins, lr, #COMPACT_CODE_ITEM_INS_SIZE_SHIFT, #4
    .else
    ubfx ip, lr, #COMPACT_CODE_ITEM_INS_SIZE_SHIFT, #4
    add \registers, \registers, ip
    .endif

    ldrh lr, [\code_item, #COMPACT_CODE_ITEM_FLAGS_OFFSET]
    tst lr, #COMPACT_CODE_ITEM_REGISTERS_INS_OUTS_FLAGS
    beq 4f
    mov ip, \code_item
    tst lr, #COMPACT_CODE_ITEM_INSNS_FLAG
    beq 1f
    sub ip, ip, #4
1:
    tst lr, #COMPACT_CODE_ITEM_REGISTERS_FLAG
    beq 2f
    ldrh lr, [ip, #-2]!
    add \registers, \registers, lr
    ldrh lr, [\code_item, #COMPACT_CODE_ITEM_FLAGS_OFFSET]
2:
    tst lr, #COMPACT_CODE_ITEM_INS_FLAG
    beq 3f
    ldrh lr, [ip, #-2]!
    .if \load_ins
    add \ins, \ins, lr
    .else
    add \registers, \registers, lr
    .endif
    ldrh lr, [\code_item, #COMPACT_CODE_ITEM_FLAGS_OFFSET]
3:
    tst lr, #COMPACT_CODE_ITEM_OUTS_FLAG
    beq 4f
    ldrh lr, [ip, #-2]!
    add \outs, \outs, lr
4:
    .if \load_ins
    add \registers, \registers, \ins
    .endif
    add \code_item, \code_item, #COMPACT_CODE_ITEM_INSNS_OFFSET
    b 6f
5:
    // Fetch dex register size.
    ldrh \registers, [\code_item, #CODE_ITEM_REGISTERS_SIZE_OFFSET]
    // Fetch outs size.
    ldrh \outs, [\code_item, #CODE_ITEM_OUTS_SIZE_OFFSET]
    .if \load_ins
    ldrh \ins, [\code_item, #CODE_ITEM_INS_SIZE_OFFSET]
    .endif
    add \code_item, \code_item, #CODE_ITEM_INSNS_OFFSET
6:
.endm

// Setup the stack to start executing the method. Expects:
// - r0 to contain the ArtMethod
// - \code_item to already contain the code item
// - rINST, ip, lr to be available
//
// Outputs
// - rINST contains the dex registers size
// - ip contains the old stack pointer.
// - \code_item is replaced with a pointer to the instructions
// - if load_ins is 1, r4 contains the ins
//
.macro SETUP_STACK_FRAME code_item, refs, fp, cfi_refs, load_ins
    FETCH_CODE_ITEM_INFO \code_item, rINST, \refs, r4, \load_ins

    // Compute required frame size: ((2 * rINST) + \refs) * 4 + 12
    // 12 is for saving the previous frame, pc, and method being executed.
    add ip, \refs, rINST, lsl #1

    // Compute new stack pointer in lr
    sub lr, sp, #12
    sub lr, lr, ip, lsl #2
    // Alignment
    and lr, lr, #-16

    // Set reference and dex registers.
    add \refs, lr, \refs, lsl #2
    add \refs, \refs, #12
    add \fp, \refs, rINST, lsl #2

    // Now setup the stack pointer.
    mov ip, sp
    .cfi_def_cfa_register ip
    mov sp, lr
    str ip, [\refs, #-4]
    CFI_DEF_CFA_BREG_PLUS_UCONST \cfi_refs, -4, CALLEE_SAVES_SIZE

    // Save the ArtMethod, and use r0 as a temporary.
    str r0, [sp]

    // Put nulls in reference frame.
    cmp rINST, #0
    beq 2f
    mov lr, \refs
    mov r0, #0
1:
    str r0, [lr], #4
    str r0, [lr], #4  // May clear vreg[0].
    cmp lr, \fp
    blo 1b
2:
    ldr r0, [sp]  // Reload the ArtMethod, expected by the callers.
.endm

// Increase method hotness and do suspend check before starting executing the method.
.macro START_EXECUTING_INSTRUCTIONS
    ldr r0, [sp]
    ldrh r2, [r0, #ART_METHOD_HOTNESS_COUNT_OFFSET]
    add r2, r2, #1
    ubfx r2, r2, #0, #NTERP_HOTNESS_BITS
    strh r2, [r0, #ART_METHOD_HOTNESS_COUNT_OFFSET]
    // If the counter overflows, handle this in the runtime.
    cmp r2, #0
    beq 2f
    ldr r0, [rSELF, #THREAD_FLAGS_OFFSET]
    tst r0, #THREAD_SUSPEND_OR_CHECKPOINT_REQUEST
    bne 3f
1:
    FETCH_INST
    GET_INST_OPCODE ip
    GOTO_OPCODE ip
2:
    mov r1, #0
    mov r2, rFP
    bl nterp_hot_method
    b 1b
3:
    EXPORT_PC
    bl art_quick_test_suspend
    b 1b
.endm

.macro SPILL_ALL_CALLEE_SAVES
    SPILL_ALL_CALLEE_SAVE_GPRS                    @ 9 words (36 bytes) of callee saves.
    vpush {s16-s31}                               @ 16 words (64 bytes) of floats.
    .cfi_adjust_cfa_offset 64
.endm

.macro RESTORE_ALL_CALLEE_SAVES lr_to_pc=0
    vpop {s16-s31}
    .cfi_adjust_cfa_offset -64
    pop {r4-r7}
    .cfi_adjust_cfa_offset -16
    .cfi_restore r4
    .cfi_restore r5
    .cfi_restore r6
    .cfi_restore r7
    // Don't restore r8, the marking register gets updated when coming back from runtime.
    add sp, sp, #4
    .cfi_adjust_cfa_offset -4
    .if \lr_to_pc
    pop {r9-r11, pc}  @ 9 words of callee saves and args.
    .else
    pop {r9-r11, lr}  @ 9 words of callee saves and args.
    .cfi_adjust_cfa_offset -16
    .cfi_restore r9
    .cfi_restore r10
    .cfi_restore r11
    .cfi_restore lr
    .endif
.endm

// Helper to setup the stack after doing a nterp to nterp call. This will setup:
// - rNEW_FP: the new pointer to dex registers
// - rNEW_REFS: the new pointer to references
// - rPC: the new PC pointer to execute
// - r2: value in instruction to decode the number of arguments.
// - r3: first dex register for range invokes, up to 4 arguments for non-range invokes.
// - r4: top of dex register array
//
// The method expects:
// - r0 to contain the ArtMethod
// - r4 to contain the code item
.macro SETUP_STACK_FOR_INVOKE
   // We do the same stack overflow check as the compiler. See CanMethodUseNterp
   // in how we limit the maximum nterp frame size.
   sub ip, sp, #STACK_OVERFLOW_RESERVED_BYTES
   ldr ip, [ip]

   // Spill all callee saves to have a consistent stack frame whether we
   // are called by compiled code or nterp.
   SPILL_ALL_CALLEE_SAVES

   // Setup the frame.
   SETUP_STACK_FRAME r4, rNEW_REFS, rNEW_FP, CFI_NEW_REFS, load_ins=0

   // Fetch instruction information before replacing rPC.
   FETCH_B r2, 0, 1
   FETCH r3, 2

   // Set the dex pc pointer.
   mov rPC, r4

   // Make r4 point to the top of the dex register array.
   add r4, rNEW_FP, rINST, lsl #2

   CFI_DEFINE_DEX_PC_WITH_OFFSET(CFI_TMP, CFI_DEX, 0)
.endm

// Setup arguments based on a non-range nterp to nterp call, and start executing
// the method. We expect:
// - rNEW_FP: the new pointer to dex registers
// - rPC: the new PC pointer to execute
// - r2: number of arguments (bits 4-7), 5th argument if any (bits 0-3)
// - r3: up to four dex register arguments
// - r4: top of dex register array
// - r1: receiver if non-static.
//
// Uses r0 and rINST as temporaries.
.macro SETUP_NON_RANGE_ARGUMENTS_AND_EXECUTE is_static=0, is_string_init=0
   // /* op vA, vB, {vC...vG} */
   .if \is_static
   asrs   r0, r2, #4
   beq    6f
   .else
   asr    r0, r2, #4
   .endif
   mov rINST, #-4
   cmp r0, #2
   blt 1f
   beq 2f
   cmp r0, #4
   blt 3f
   beq 4f

  // We use a decrementing rINST to store references relative
  // to rNEW_FP and dex registers relative to r4
  //
  // TODO: We could set up rINST as the number of registers (this can be an additional output from
  // SETUP_STACK_FOR_INVOKE) and then just decrement it by one before copying each arg.
  // Maybe even introduce macros NEW_VREG_ADDRESS/NEW_VREG_REF_ADDRESS.
5:
   and         r2, r2, #15
   GET_VREG_OBJECT r0, r2
   str         r0, [rNEW_FP, rINST]
   GET_VREG    r0, r2
   str         r0, [r4, rINST]
   sub         rINST, rINST, #4
4:
   asr         r2, r3, #12
   GET_VREG_OBJECT r0, r2
   str         r0, [rNEW_FP, rINST]
   GET_VREG    r0, r2
   str         r0, [r4, rINST]
   sub         rINST, rINST, #4
3:
   ubfx        r2, r3, #8, #4
   GET_VREG_OBJECT r0, r2
   str         r0, [rNEW_FP, rINST]
   GET_VREG    r0, r2
   str         r0, [r4, rINST]
   sub         rINST, rINST, #4
2:
   ubfx        r2, r3, #4, #4
   GET_VREG_OBJECT r0, r2
   str         r0, [rNEW_FP, rINST]
   GET_VREG    r0, r2
   str         r0, [r4, rINST]
   .if !\is_string_init
   sub         rINST, rINST, #4
   .endif
1:
   .if \is_string_init
   // Ignore the first argument
   .elseif \is_static
   and         r2, r3, #0xf
   GET_VREG_OBJECT r0, r2
   str         r0, [rNEW_FP, rINST]
   GET_VREG    r0, r2
   str         r0, [r4, rINST]
   .else
   str         r1, [rNEW_FP, rINST]
   str         r1, [r4, rINST]
   .endif

6:
   // Start executing the method.
   mov rFP, rNEW_FP
   mov rREFS, rNEW_REFS
   CFI_DEF_CFA_BREG_PLUS_UCONST CFI_REFS, -4, CALLEE_SAVES_SIZE
   // r8 was used for setting up the frame, restore it now.
   REFRESH_MARKING_REGISTER
   // Branch to the main handler, which will reload rIBASE,
   // that was used for setting up the frame.
   b .Lexecute_instructions
.endm

// Setup arguments based on a range nterp to nterp call, and start executing
// the method.
// - rNEW_FP: the new pointer to dex registers
// - rNEW_REFS: the new pointer to references
// - rPC: the new PC pointer to execute
// - r2: number of arguments
// - r3: first dex register
// - r4: top of dex register array
// - r1: receiver if non-static.
//
// Expects r0 to be available.
.macro SETUP_RANGE_ARGUMENTS_AND_EXECUTE is_static=0, is_string_init=0
   mov r0, #-4
   .if \is_string_init
   // Ignore the first argument
   sub r2, r2, #1
   add r3, r3, #1
   .elseif !\is_static
   sub r2, r2, #1
   add r3, r3, #1
   .endif

   cmp r2, #0
   beq 2f
   add rREFS, rREFS, r3, lsl #2  // pointer to first argument in reference array
   add rREFS, rREFS, r2, lsl #2    // pointer to last argument in reference array
   add rFP, rFP, r3, lsl #2     // pointer to first argument in register array
   add rFP, rFP, r2, lsl #2      // pointer to last argument in register array
1:
   ldr  r3, [rREFS, #-4]!
   str  r3, [rNEW_FP, r0]
   subs r2, r2, 1
   ldr  r3, [rFP, #-4]!
   str  r3, [r4, r0]
   sub r0, r0, 4
   bne 1b
2:
   .if \is_string_init
   // Ignore first argument
   .elseif !\is_static
   str r1, [rNEW_FP, r0]
   str r1, [r4, r0]
   .endif
   mov rFP, rNEW_FP
   mov rREFS, rNEW_REFS
   CFI_DEF_CFA_BREG_PLUS_UCONST CFI_REFS, -4, CALLEE_SAVES_SIZE
   // r8 was used for setting up the frame, restore it now.
   REFRESH_MARKING_REGISTER
   // Branch to the main handler, which will reload rIBASE,
   // that was used for setting up the frame.
   b .Lexecute_instructions
.endm

.macro GET_SHORTY dest, is_interface, is_polymorphic, is_custom
   push {r0-r3}
   .if \is_polymorphic
   ldr r0, [sp, #16]
   mov r1, rPC
   bl NterpGetShortyFromInvokePolymorphic
   .elseif \is_custom
   ldr r0, [sp, #16]
   mov r1, rPC
   bl NterpGetShortyFromInvokeCustom
   .elseif \is_interface
   ldr r0, [sp, #16]
   FETCH r1, 1
   bl NterpGetShortyFromMethodId
   .else
   bl NterpGetShorty
   .endif
   mov \dest, r0
   pop {r0-r3}
.endm

// Input:  r0 contains the ArtMethod
// Output: r4 contains the code item
.macro GET_CODE_ITEM
   ldr r4, [r0, #ART_METHOD_DATA_OFFSET_32]
.endm

.macro DO_ENTRY_POINT_CHECK call_compiled_code, name
   // On entry, the method is r0, the instance is r1
   ldr r2, .Lfetch_nterp_\name
.Lfetch_location_\name:
   // Note that this won't work for thumb.
   sub r2, pc, r2
   ldr r3, [r0, #ART_METHOD_QUICK_CODE_OFFSET_32]
   cmp r2, r3
   bne  \call_compiled_code
.endm

// Expects ip and lr to be available.
.macro UPDATE_REGISTERS_FOR_STRING_INIT old_value, new_value
   mov ip, #0
1:
   GET_VREG_OBJECT lr, ip
   cmp lr, \old_value
   bne 2f
   SET_VREG_OBJECT \new_value, ip
2:
   add ip, ip, #1
   add lr, rREFS, ip, lsl #2
   cmp lr, rFP
   bne 1b
.endm

// Puts the next floating point argument into the expected register,
// fetching values based on a non-range invoke.
// Uses ip and lr.
.macro LOOP_OVER_SHORTY_LOADING_FPS dreg, sreg, inst, shorty, arg_index, finished, if_double
1: // LOOP
    ldrb ip, [\shorty], #1          // Load next character in shorty, and increment.
    cmp ip, #0
    beq \finished                   // if (ip == '\0') goto finished
    cmp ip, #68                    // if (ip == 'D') goto FOUND_DOUBLE
    beq 2f
    cmp ip, #70                    // if (ip == 'F') goto FOUND_FLOAT
    beq 3f
    lsr \inst, \inst, #4
    add \arg_index, \arg_index, #1
    //  Handle extra argument in arg array taken by a long.
    cmp ip, #74                   // if (ip != 'J') goto LOOP
    bne 1b
    lsr \inst, \inst, #4
    add \arg_index, \arg_index, #1
    b 1b                        // goto LOOP
2:  // FOUND_DOUBLE
    and ip, \inst, #0xf
    GET_VREG ip, ip
    lsr \inst, \inst, #4
    add \arg_index, \arg_index, #1
    cmp \arg_index, #4
    beq 5f
    and lr, \inst, #0xf
    lsr \inst, \inst, #4
    add \arg_index, \arg_index, #1
    b 6f
5:
    FETCH_B lr, 0, 1
    and lr, lr, #0xf
6:
    GET_VREG lr, lr
    vmov \dreg, ip, lr
    b \if_double
3:  // FOUND_FLOAT
    cmp \arg_index, #4
    beq 7f
    and ip, \inst, #0xf
    lsr \inst, \inst, #4
    add \arg_index, \arg_index, #1
    b 8f
7:
    FETCH_B ip, 0, 1
    and ip, ip, #0xf
8:
    GET_VREG_FLOAT \sreg, ip
.endm

// Puts the next int/long/object argument in the expected register,
// fetching values based on a non-range invoke.
// Uses ip.
.macro LOOP_OVER_SHORTY_LOADING_GPRS gpr_reg, inst, shorty, arg_index, finished, if_long, is_r3
1: // LOOP
    ldrb ip, [\shorty], #1         // Load next character in shorty, and increment.
    cmp ip, #0
    beq \finished                   // if (ip == '\0') goto finished
    cmp ip, #74                    // if (ip == 'J') goto FOUND_LONG
    beq 2f
    cmp ip, #70                    // if (ip == 'F') goto SKIP_FLOAT
    beq 3f
    cmp ip, #68                    // if (ip == 'D') goto SKIP_DOUBLE
    beq 4f
    cmp \arg_index, #4
    beq 7f
    and ip, \inst, #0xf
    lsr \inst, \inst, #4
    add \arg_index, \arg_index, #1
    b 8f
7:
    FETCH_B ip, 0, 1
    and ip, ip, #0xf
8:
    GET_VREG \gpr_reg, ip
    b 5f
2:  // FOUND_LONG
    .if \is_r3
    // Put back shorty and exit
    sub \shorty, \shorty, #1
    b 5f
    .endif
    and ip, \inst, #0xf
    GET_VREG ip, ip
    // The only one possible for non-range long is r2-r3
    mov r2, ip
    lsr \inst, \inst, #4
    add \arg_index, \arg_index, #1
    cmp \arg_index, #4
    beq 9f
    and ip, \inst, #0xf
    lsr \inst, \inst, #4
    b 10f
9:
    FETCH_B ip, 0, 1
    and ip, ip, #0xf
10:
    GET_VREG ip, ip
    // The only one possible for non-range long is r2-r3
    mov r3, ip
    add \arg_index, \arg_index, #1
    b \if_long
3:  // SKIP_FLOAT
    lsr \inst, \inst, #4
    add \arg_index, \arg_index, #1
    b 1b
4:  // SKIP_DOUBLE
    lsr \inst, \inst, #8
    add \arg_index, \arg_index, #2
    b 1b
5:
.endm

// Puts the next int/long/object argument in the expected stack slot,
// fetching values based on a non-range invoke.
// Uses ip as temporary.
.macro LOOP_OVER_SHORTY_LOADING_INTs shorty, inst, arg_index, finished, is_string_init
1: // LOOP
    ldrb ip, [\shorty], #1         // Load next character in shorty, and increment.
    cmp ip, #0
    beq \finished                  // if (ip == '\0') goto finished
    cmp ip, #74                    // if (ip == 'J') goto FOUND_LONG
    beq 2f
    cmp ip, #70                    // if (ip == 'F') goto SKIP_FLOAT
    beq 3f
    cmp ip, #68                    // if (ip == 'D') goto SKIP_DOUBLE
    beq 4f
    .if \is_string_init
    cmp \arg_index, #4
    .else
    cmp \arg_index, #(4+1)         // +1 for ArtMethod
    .endif
    beq 7f
    and ip, \inst, #0xf
    lsr \inst, \inst, #4
    b 8f
7:
    FETCH_B ip, 0, 1
    and ip, ip, #0xf
8:
    GET_VREG ip, ip
    str ip, [sp, \arg_index, lsl #2]
    add \arg_index, \arg_index, #1
    b 1b
2:  // FOUND_LONG
    and ip, \inst, #0xf
    GET_VREG ip, ip
    str ip, [sp, \arg_index, lsl #2]
    lsr \inst, \inst, #4
    add \arg_index, \arg_index, #1
    .if \is_string_init
    cmp \arg_index, #4
    .else
    cmp \arg_index, #(4+1)         // +1 for ArtMethod
    .endif
    beq 9f
    and ip, \inst, #0xf
    lsr \inst, \inst, #4
    b 10f
9:
    FETCH_B ip, 0, 1
    and ip, ip, #0xf
10:
    GET_VREG ip, ip
    str ip, [sp, \arg_index, lsl #2]
    add \arg_index, \arg_index, #1
    b 1b
3:  // SKIP_FLOAT
    lsr \inst, \inst, #4
    add \arg_index, \arg_index, #1
    b 1b
4:  // SKIP_DOUBLE
    lsr \inst, \inst, #8
    add \arg_index, \arg_index, #2
    b 1b
.endm

.macro SETUP_RETURN_VALUE shorty
   ldrb ip, [\shorty]
   cmp ip, #68       // Test if result type char == 'D'.
   beq 1f
   cmp ip, #70       // Test if result type char == 'F'.
   bne 2f
   vmov r0, s0
   b 2f
1:
   vmov r0, r1, d0
2:
.endm

.macro GET_SHORTY_SLOW_PATH dest, is_interface
   // Save all registers that can hold arguments in the fast path.
   vpush {s0}
   push {r0-r2}
   .if \is_interface
   ldr r0, [sp, #16]
   FETCH r1, 1
   bl NterpGetShortyFromMethodId
   .else
   bl NterpGetShorty
   .endif
   mov \dest, r0
   pop {r0-r2}
   vpop {s0}
.endm

.macro COMMON_INVOKE_NON_RANGE is_static=0, is_interface=0, suffix="", is_string_init=0, is_polymorphic=0, is_custom=0
   .if \is_polymorphic
   // We always go to compiled code for polymorphic calls.
   .elseif \is_custom
   // We always go to compiled code for custom calls.
   .else
     DO_ENTRY_POINT_CHECK .Lcall_compiled_code_\suffix, \suffix
     GET_CODE_ITEM
     .if \is_string_init
     bl nterp_to_nterp_string_init_non_range
     .elseif \is_static
     bl nterp_to_nterp_static_non_range
     .else
     bl nterp_to_nterp_instance_non_range
     .endif
     b .Ldone_return_\suffix
.Lfetch_nterp_\suffix:
    .word   (.Lfetch_location_\suffix+8) - ExecuteNterpImpl
   .endif

.Lcall_compiled_code_\suffix:
   .if \is_polymorphic
   // No fast path for polymorphic calls.
   .elseif \is_custom
   // No fast path for custom calls.
   .elseif \is_string_init
   // No fast path for string.init.
   .else
     ldr ip, [r0, #ART_METHOD_ACCESS_FLAGS_OFFSET]
     tst ip, #ART_METHOD_NTERP_INVOKE_FAST_PATH_FLAG
     beq .Lfast_path_with_few_args_\suffix
     FETCH_B rINST, 0, 1
     .if \is_static
     asrs lr, rINST, #4
     beq .Linvoke_fast_path_\suffix
     .else
     asr lr, rINST, #4
     cmp lr, #1
     beq .Linvoke_fast_path_\suffix
     .endif
     FETCH ip, 2
     cmp lr, #2
     .if \is_static
     blt .Lone_arg_fast_path_\suffix
     .endif
     beq .Ltwo_args_fast_path_\suffix
     cmp lr, #4
     blt .Lthree_args_fast_path_\suffix
     beq .Lfour_args_fast_path_\suffix
     and         rINST, rINST, #15
     GET_VREG    rINST, rINST
     str         rINST, [sp, #(4 + 4 * 4)]
.Lfour_args_fast_path_\suffix:
     asr         rINST, ip, #12
     GET_VREG    rINST, rINST
     str         rINST, [sp, #(4 + 3 * 4)]
.Lthree_args_fast_path_\suffix:
     ubfx        rINST, ip, #8, #4
     GET_VREG    r3, rINST
.Ltwo_args_fast_path_\suffix:
     ubfx        rINST, ip, #4, #4
     GET_VREG    r2, rINST
.Lone_arg_fast_path_\suffix:
     .if \is_static
     and         rINST, ip, #0xf
     GET_VREG    r1, rINST
     .else
     // First argument already in r1.
     .endif
.Linvoke_fast_path_\suffix:
     .if \is_interface
     // Setup hidden argument.
     mov ip, r4
     .endif
     ldr lr, [r0, #ART_METHOD_QUICK_CODE_OFFSET_32]
     blx lr
     FETCH_ADVANCE_INST 3
     GET_INST_OPCODE ip
     GOTO_OPCODE ip

.Lfast_path_with_few_args_\suffix:
     // Fast path when we have zero or one argument (modulo 'this'). If there
     // is one argument, we can put it in both floating point and core register.
     FETCH_B r2, 0, 1
     asr r2, r2, #4  // number of arguments
     .if \is_static
     cmp r2, #1
     blt .Linvoke_with_few_args_\suffix
     bne .Lget_shorty_\suffix
     FETCH r2, 2
     and r2, r2, #0xf  // dex register of first argument
     GET_VREG r1, r2
     vmov s0, r1
     .else
     cmp r2, #2
     blt .Linvoke_with_few_args_\suffix
     bne .Lget_shorty_\suffix
     FETCH r2, 2
     ubfx r2, r2, #4, #4  // dex register of second argument
     GET_VREG r2, r2
     vmov s0, r2
     .endif
.Linvoke_with_few_args_\suffix:
     // Check if the next instruction is move-result or move-result-wide.
     // If it is, we fetch the shorty and jump to the regular invocation.
     FETCH r3, 3
     and r3, r3, #0xfe
     cmp r3, #0x0a
     beq .Lget_shorty_and_invoke_\suffix
     .if \is_interface
     // Setup hidden argument.
     mov ip, r4
     .endif
     ldr lr, [r0, #ART_METHOD_QUICK_CODE_OFFSET_32]
     blx lr
     FETCH_ADVANCE_INST 3
     GET_INST_OPCODE ip
     GOTO_OPCODE ip
.Lget_shorty_and_invoke_\suffix:
     .if \is_interface
     // Save hidden argument.
     vmov s16, r4
     .endif
     GET_SHORTY_SLOW_PATH rINST, \is_interface
     b .Lgpr_setup_finished_\suffix
   .endif

.Lget_shorty_\suffix:
   .if \is_interface
   // Save hidden argument.
   vmov s16, r4
   .endif
   GET_SHORTY rINST, \is_interface, \is_polymorphic, \is_custom
   // From this point:
   // - rINST contains shorty (in callee-save to switch over return value after call).
   // - r0 contains method
   // - r1 contains 'this' pointer for instance method.
   // We need three registers.
   add r3, rINST, #1  // shorty + 1  ; ie skip return arg character
   FETCH r2, 2 // arguments
   .if \is_string_init
   lsr r2, r2, #4
   mov r4, #1       // ignore first argument
   .elseif \is_static
   mov r4, #0      // arg_index
   .else
   lsr r2, r2, #4
   mov r4, #1       // ignore first argument
   .endif
   LOOP_OVER_SHORTY_LOADING_FPS d0, s0, r2, r3, r4, .Lxmm_setup_finished_\suffix, .Ld1_s2_\suffix
.Ld1_s1_\suffix:
   LOOP_OVER_SHORTY_LOADING_FPS d1, s1, r2, r3, r4, .Lxmm_setup_finished_\suffix, .Ld2_s1_\suffix
.Ld1_s2_\suffix:
   LOOP_OVER_SHORTY_LOADING_FPS d1, s2, r2, r3, r4, .Lxmm_setup_finished_\suffix, .Ls4_\suffix
.Ld2_s3_\suffix:
   LOOP_OVER_SHORTY_LOADING_FPS d2, s3, r2, r3, r4, .Lxmm_setup_finished_\suffix, .Lxmm_setup_finished_\suffix
   b .Ls4_\suffix
.Ld2_s1_\suffix:
   LOOP_OVER_SHORTY_LOADING_FPS d2, s1, r2, r3, r4, .Lxmm_setup_finished_\suffix, .Lxmm_setup_finished_\suffix
.Ls4_\suffix:
   // If we arrive here, we can only have a float.
   LOOP_OVER_SHORTY_LOADING_FPS d2, s4, r2, r3, r4, .Lxmm_setup_finished_\suffix, .Lxmm_setup_finished_\suffix
.Lxmm_setup_finished_\suffix:
   add r4, rINST, #1  // shorty + 1  ; ie skip return arg character
   FETCH r8, 2 // arguments
   .if \is_string_init
   lsr r8, r8, #4
   mov lr, #1       // ignore first argument
   LOOP_OVER_SHORTY_LOADING_GPRS r1, r8, r4, lr, .Lgpr_setup_finished_\suffix, .Lif_long_\suffix, is_r3=0
   .elseif \is_static
   mov lr, #0      // arg_index
   LOOP_OVER_SHORTY_LOADING_GPRS r1, r8, r4, lr, .Lgpr_setup_finished_\suffix, .Lif_long_\suffix, is_r3=0
   .else
   lsr r8, r8, #4
   mov lr, #1       // ignore first argument
   .endif
   LOOP_OVER_SHORTY_LOADING_GPRS r2, r8, r4, lr, .Lgpr_setup_finished_\suffix, .Lif_long_\suffix, is_r3=0
   LOOP_OVER_SHORTY_LOADING_GPRS r3, r8, r4, lr, .Lgpr_setup_finished_\suffix, .Lif_long_\suffix, is_r3=1
.Lif_long_\suffix:
   // Store in the outs array (stored above the ArtMethod in the stack). We only do this for non-string-init
   // calls as the index is already adjusted above.
   .if !\is_string_init
   add lr, lr, #1
   .endif
   LOOP_OVER_SHORTY_LOADING_INTs r4, r8, lr, .Lgpr_setup_finished_\suffix, \is_string_init
.Lgpr_setup_finished_\suffix:
   REFRESH_MARKING_REGISTER // r8 was used when setting parameters, restore it.
   .if \is_polymorphic
   bl art_quick_invoke_polymorphic
   .elseif \is_custom
   bl art_quick_invoke_custom
   .else
      .if \is_interface
      // Setup hidden argument.
      vmov ip, s16
      .endif
      ldr lr, [r0, #ART_METHOD_QUICK_CODE_OFFSET_32]
      blx lr
   .endif
   SETUP_RETURN_VALUE rINST
.Ldone_return_\suffix:
   /* resume execution of caller */
   .if \is_string_init
   FETCH ip, 2 // arguments
   and ip, ip, #0xf
   GET_VREG r1, ip
   UPDATE_REGISTERS_FOR_STRING_INIT r1, r0
   .endif

   .if \is_polymorphic
   FETCH_ADVANCE_INST 4
   .else
   FETCH_ADVANCE_INST 3
   .endif
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.endm

// Puts the next int/long/object argument in the expected register,
// fetching values based on a range invoke.
// Uses ip as temporary.
.macro LOOP_RANGE_OVER_SHORTY_LOADING_GPRS reg32, shorty, arg_index, stack_index, finished, if_long, is_r3
1: // LOOP
    ldrb ip, [\shorty], #1         // Load next character in shorty, and increment.
    cmp ip, #0
    beq \finished                  // if (ip == '\0') goto finished
    cmp ip, #74                    // if (ip == 'J') goto FOUND_LONG
    beq 2f
    cmp ip, #70                    // if (ip == 'F') goto SKIP_FLOAT
    beq 3f
    cmp ip, #68                    // if (ip == 'D') goto SKIP_DOUBLE
    beq 4f
    GET_VREG \reg32, \arg_index
    add \arg_index, \arg_index, #1
    add \stack_index, \stack_index, #1
    b 5f
2:  // FOUND_LONG
    .if \is_r3
    // Put back shorty and jump to \if_long
    sub \shorty, \shorty, #1
    .else
    GET_VREG r2, \arg_index
    add \arg_index, \arg_index, #1
    add \stack_index, \stack_index, #1
    GET_VREG r3, \arg_index
    add \arg_index, \arg_index, #1
    add \stack_index, \stack_index, #1
    .endif
    b \if_long
3:  // SKIP_FLOAT
    add \arg_index, \arg_index, #1
    add \stack_index, \stack_index, #1
    b 1b
4:  // SKIP_DOUBLE
    add \arg_index, \arg_index, #2
    add \stack_index, \stack_index, #2
    b 1b
5:
.endm

// Puts the next int/long/object argument in the expected stack slot,
// fetching values based on a range invoke.
// Uses ip as temporary.
.macro LOOP_RANGE_OVER_INTs shorty, arg_index, stack_index, finished
1: // LOOP
    ldrb ip, [\shorty], #1         // Load next character in shorty, and increment.
    cmp ip, #0
    beq \finished                     // if (ip == '\0') goto finished
    cmp ip, #74                    // if (ip == 'J') goto FOUND_LONG
    beq 2f
    cmp ip, #70                    // if (ip == 'F') goto SKIP_FLOAT
    beq 3f
    cmp ip, #68                    // if (ip == 'D') goto SKIP_DOUBLE
    beq 4f
    GET_VREG ip, \arg_index
    str ip, [sp, \stack_index, lsl #2]
    add \arg_index, \arg_index, #1
    add \stack_index, \stack_index, #1
    b 1b
2:  // FOUND_LONG
    GET_VREG ip, \arg_index
    str ip, [sp, \stack_index, lsl #2]
    add \arg_index, \arg_index, #1
    add \stack_index, \stack_index, #1
    GET_VREG ip, \arg_index
    str ip, [sp, \stack_index, lsl #2]
    add \arg_index, \arg_index, #1
    add \stack_index, \stack_index, #1
    b 1b
3:  // SKIP_FLOAT
    add \arg_index, \arg_index, #1
    add \stack_index, \stack_index, #1
    b 1b
4:  // SKIP_DOUBLE
    add \arg_index, \arg_index, #2
    add \stack_index, \stack_index, #2
    b 1b
.endm

.macro COMMON_INVOKE_RANGE is_static=0, is_interface=0, suffix="", is_string_init=0, is_polymorphic=0, is_custom=0
   .if \is_polymorphic
   // We always go to compiled code for polymorphic calls.
   .elseif \is_custom
   // We always go to compiled code for custom calls.
   .else
     DO_ENTRY_POINT_CHECK .Lcall_compiled_code_range_\suffix, range_\suffix
     GET_CODE_ITEM
     .if \is_string_init
     bl nterp_to_nterp_string_init_range
     .elseif \is_static
     bl nterp_to_nterp_static_range
     .else
     bl nterp_to_nterp_instance_range
     .endif
     b .Ldone_return_range_\suffix
.Lfetch_nterp_range_\suffix:
    .word   (.Lfetch_location_range_\suffix+8) - ExecuteNterpImpl
   .endif

.Lcall_compiled_code_range_\suffix:
   .if \is_polymorphic
   // No fast path for polymorphic calls.
   .elseif \is_custom
   // No fast path for custom calls.
   .elseif \is_string_init
   // No fast path for string.init.
   .else
     ldr ip, [r0, #ART_METHOD_ACCESS_FLAGS_OFFSET]
     tst ip, #ART_METHOD_NTERP_INVOKE_FAST_PATH_FLAG
     beq .Lfast_path_with_few_args_range_\suffix
     FETCH_B ip, 0, 1  // Number of arguments
     .if \is_static
     cmp ip, #0
     .else
     cmp ip, #1
     .endif
     beq .Linvoke_fast_path_range_\suffix
     FETCH lr, 2  // dex register of first argument
     add lr, rFP, lr, lsl #2  // location of first dex register value.
     .if \is_static
     cmp ip, #2
     blt .Lone_arg_fast_path_range_\suffix
     beq .Ltwo_args_fast_path_range_\suffix
     cmp ip, #3
     .else
     cmp ip, #3
     blt .Ltwo_args_fast_path_range_\suffix
     .endif
     beq .Lthree_args_fast_path_range_\suffix
     add rINST, sp, #4  // Add space for the ArtMethod

.Lloop_over_fast_path_range_\suffix:
     sub ip, ip, #1
     ldr r3, [lr, ip, lsl #2]
     str r3, [rINST, ip, lsl #2]
     cmp ip, #3
     bne .Lloop_over_fast_path_range_\suffix

.Lthree_args_fast_path_range_\suffix:
     ldr r3, [lr, #8]
.Ltwo_args_fast_path_range_\suffix:
     ldr r2, [lr, #4]
.Lone_arg_fast_path_range_\suffix:
     .if \is_static
     ldr r1, [lr, #0]
     .else
     // First argument already in r1.
     .endif
.Linvoke_fast_path_range_\suffix:
     .if \is_interface
     // Setup hidden argument.
     mov ip, r4
     .endif
     ldr lr, [r0, #ART_METHOD_QUICK_CODE_OFFSET_32]
     blx lr
     FETCH_ADVANCE_INST 3
     GET_INST_OPCODE ip
     GOTO_OPCODE ip

.Lfast_path_with_few_args_range_\suffix:
     // Fast path when we have zero or one argument (modulo 'this'). If there
     // is one argument, we can put it in both floating point and core register.
     FETCH_B r2, 0, 1 // number of arguments
     .if \is_static
     cmp r2, #1
     blt .Linvoke_with_few_args_range_\suffix
     bne .Lget_shorty_range_\suffix
     FETCH r3, 2  // dex register of first argument
     GET_VREG r1, r3
     vmov s0, r1
     .else
     cmp r2, #2
     blt .Linvoke_with_few_args_range_\suffix
     bne .Lget_shorty_range_\suffix
     FETCH r3, 2  // dex register of first argument
     add r3, r3, #1  // Add 1 for next argument
     GET_VREG r2, r3
     vmov s0, r2
     .endif
.Linvoke_with_few_args_range_\suffix:
     // Check if the next instruction is move-result or move-result-wide.
     // If it is, we fetch the shorty and jump to the regular invocation.
     FETCH r3, 3
     and r3, r3, #0xfe
     cmp r3, #0x0a
     beq .Lget_shorty_and_invoke_range_\suffix
     .if \is_interface
     // Setup hidden argument.
     mov ip, r4
     .endif
     ldr lr, [r0, #ART_METHOD_QUICK_CODE_OFFSET_32]
     blx lr
     FETCH_ADVANCE_INST 3
     GET_INST_OPCODE ip
     GOTO_OPCODE ip
.Lget_shorty_and_invoke_range_\suffix:
     .if \is_interface
     // Save hidden argument.
     vmov s16, r4
     .endif
     GET_SHORTY_SLOW_PATH rINST, \is_interface
     b .Lgpr_setup_finished_range_\suffix
   .endif

.Lget_shorty_range_\suffix:
   .if \is_interface
   // Save hidden argument.
   vmov s16, r4
   .endif
   GET_SHORTY rINST, \is_interface, \is_polymorphic, \is_custom
   // From this point:
   // - rINST contains shorty (in callee-save to switch over return value after call).
   // - r0 contains method
   // - r1 contains 'this' pointer for instance method.
   //
   // Save r0 and r1 before calling NterpSetupArm32Fprs.
   push {r0, r1}
   add r0, rINST, #1  // shorty + 1  ; ie skip return arg character
   FETCH r1, 2 // arguments
   .if \is_string_init
   add r1, r1, #1  // arg start index
   mov r2, #1       // index in stack
   .elseif \is_static
   mov r2, #0       // index in stack
   .else
   add r1, r1, #1  // arg start index
   mov r2, #1       // index in stack
   .endif
   vpush {s0-s15}
   mov r3, sp
   // Pass the stack address for arguments, +16 for fprs, +2 for saved registers,
   // +1 for ArtMethod.
   add lr, sp, #((16 + 2 + 1) * 4)
   push {rFP, lr}
   bl NterpSetupArm32Fprs
   add sp, sp, #8
   vpop {s0-s15}
   pop {r0, r1}
.Lxmm_setup_finished_range_\suffix:
   add r8, rINST, #1  // shorty + 1  ; ie skip return arg character
   FETCH lr, 2 // arguments
   .if \is_string_init
   add lr, lr, #1  // arg start index
   mov r4, #0      // index in stack
   LOOP_RANGE_OVER_SHORTY_LOADING_GPRS r1, r8, lr, r4, .Lgpr_setup_finished_range_\suffix, .Lif_long_range_\suffix, is_r3=0
   .elseif \is_static
   mov r4, #0      // index in stack
   LOOP_RANGE_OVER_SHORTY_LOADING_GPRS r1, r8, lr, r4, .Lgpr_setup_finished_range_\suffix, .Lif_long_range_\suffix, is_r3=0
   .else
   add lr, lr, #1  // arg start index
   mov r4, #1       // index in stack
   .endif
   LOOP_RANGE_OVER_SHORTY_LOADING_GPRS r2, r8, lr, r4, .Lgpr_setup_finished_range_\suffix, .Lif_long_range_\suffix, is_r3=0
   LOOP_RANGE_OVER_SHORTY_LOADING_GPRS r3, r8, lr, r4, .Lgpr_setup_finished_range_\suffix, .Lif_long_range_\suffix, is_r3=1
.Lif_long_range_\suffix:
   // Add 1 word for the ArtMethod stored before the outs.
   add r4, r4, #1
   LOOP_RANGE_OVER_INTs r8, lr, r4, .Lgpr_setup_finished_range_\suffix
.Lgpr_setup_finished_range_\suffix:
   REFRESH_MARKING_REGISTER // r8 was used when setting parameters, restore it.
   .if \is_polymorphic
   bl art_quick_invoke_polymorphic
   .elseif \is_custom
   bl art_quick_invoke_custom
   .else
      .if \is_interface
      // Setup hidden argument.
      vmov ip, s16
      .endif
      ldr lr, [r0, #ART_METHOD_QUICK_CODE_OFFSET_32]
      blx lr
   .endif
   SETUP_RETURN_VALUE rINST
.Ldone_return_range_\suffix:
   /* resume execution of caller */
   .if \is_string_init
   FETCH ip, 2 // arguments
   GET_VREG r1, ip
   UPDATE_REGISTERS_FOR_STRING_INIT r1, r0
   .endif

   .if \is_polymorphic
    FETCH_ADVANCE_INST 4
   .else
   FETCH_ADVANCE_INST 3
   .endif
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.endm

.macro WRITE_BARRIER_IF_OBJECT is_object, value, holder, label, tmp
   .if \is_object
   // In T32, we would use `SMART_CBZ \value, \label`
   cmp     \value, #0
   beq     \label
   ldr     ip, [rSELF, #THREAD_CARD_TABLE_OFFSET]
   lsr     \tmp, \holder, #CARD_TABLE_CARD_SHIFT
   strb    ip, [ip, \tmp]
\label:
   .endif
.endm

.macro LDREXD_STREXD_LOOP addr, load1, load2, store1, store2, tmp, label
\label:
   ldrexd  \load1, \load2, [\addr]
   strexd  \tmp, \store1, \store2, [\addr]
   cmp     \tmp, #0
   bne     \label
.endm

.macro ATOMIC_LOAD64 addr, load1, load2, tmp, label
   LDREXD_STREXD_LOOP \addr, \load1, \load2, \load1, \load2, \tmp, \label
.endm

.macro ATOMIC_STORE64 addr, store1, store2, tmp1, tmp2, label
   LDREXD_STREXD_LOOP \addr, \tmp1, \tmp2, \store1, \store2, \tmp1, \label
.endm

// Fetch some information from the thread cache.
// Uses ip and lr as temporaries.
.macro FETCH_FROM_THREAD_CACHE dest_reg, slow_path
   add      ip, rSELF, #THREAD_INTERPRETER_CACHE_OFFSET       // cache address
   ubfx     lr, rPC, #2, #THREAD_INTERPRETER_CACHE_SIZE_LOG2  // entry index
   add      ip, ip, lr, lsl #3             // entry address within the cache
   // In T32, we would use `ldrd ip, \dest_reg, [ip]`
   ldr      \dest_reg, [ip, #4]            // value (offset)
   ldr      ip, [ip]                       // entry key (pc)
   cmp      ip, rPC
   bne \slow_path
.endm

// Puts the next int/long/object parameter passed in physical register
// in the expected dex register array entry, and in case of object in the
// expected reference array entry.
// Uses ip as temporary.
.macro LOOP_OVER_SHORTY_STORING_GPRS gpr_32, shorty, arg_offset, regs, refs, finished, if_long, is_r3
1: // LOOP
    ldrb ip, [\shorty], #1       // Load next character in shorty, and increment.
    cmp ip, #0
    beq \finished            // if (ip == '\0') goto finished
    cmp ip, #74                  // if (ip == 'J') goto FOUND_LONG
    beq 2f
    cmp ip, #70                  // if (ip == 'F') goto SKIP_FLOAT
    beq 3f
    cmp ip, #68                  // if (ip == 'D') goto SKIP_DOUBLE
    beq 4f
    str \gpr_32, [\regs, \arg_offset]
    cmp ip, #76                  // if (ip != 'L') goto NOT_REFERENCE
    bne 6f
    str \gpr_32, [\refs, \arg_offset]
6:  // NOT_REFERENCE
    add \arg_offset, \arg_offset, #4
    b 5f
2:  // FOUND_LONG
    .if \is_r3
    // Put back shorty and jump to \if_long
    sub \shorty, \shorty, #1
    .else
    // A long can only be in r2, r3
    str r2, [\regs, \arg_offset]
    add \arg_offset, \arg_offset, #4
    str r3, [\regs, \arg_offset]
    add \arg_offset, \arg_offset, #4
    .endif
    b \if_long
3:  // SKIP_FLOAT
    add \arg_offset, \arg_offset, #4
    b 1b
4:  // SKIP_DOUBLE
    add \arg_offset, \arg_offset, #8
    b 1b
5:
.endm

// Puts the next int/long/object parameter passed in stack
// in the expected dex register array entry, and in case of object in the
// expected reference array entry.
.macro LOOP_OVER_INTs shorty, arg_offset, regs, refs, stack_ptr, tmp1, tmp2, finished
1: // LOOP
    ldrb \tmp1, [\shorty], #1       // Load next character in shorty, and increment.
    cmp \tmp1, #0
    beq \finished                   // if (\tmp1 == '\0') goto finished
    cmp \tmp1, #74                  // if (\tmp1 == 'J') goto FOUND_LONG
    beq 2f
    cmp \tmp1, #70                  // if (\tmp1 == 'F') goto SKIP_FLOAT
    beq 3f
    cmp \tmp1, #68                  // if (\tmp1 == 'D') goto SKIP_DOUBLE
    beq 4f
    add \tmp2, \stack_ptr, \arg_offset
    ldr \tmp2, [\tmp2,  #OFFSET_TO_FIRST_ARGUMENT_IN_STACK]
    str \tmp2, [\regs, \arg_offset]
    cmp \tmp1, #76                  // if (\tmp1 != 'L') goto loop
    bne 3f
    str \tmp2, [\refs, \arg_offset]
    add \arg_offset, \arg_offset, #4
    b 1b
2:  // FOUND_LONG
    add \tmp1, \stack_ptr, \arg_offset
    ldr \tmp1, [\tmp1,  #OFFSET_TO_FIRST_ARGUMENT_IN_STACK]
    str \tmp1, [\regs, \arg_offset]
    add \arg_offset, \arg_offset, #4
    add \tmp1, \stack_ptr, \arg_offset
    ldr \tmp1, [\tmp1,  #OFFSET_TO_FIRST_ARGUMENT_IN_STACK]
    str \tmp1, [\regs, \arg_offset]
    add \arg_offset, \arg_offset, #4
    b 1b
3:  // SKIP_FLOAT
    add \arg_offset, \arg_offset, #4
    b 1b
4:  // SKIP_DOUBLE
    add \arg_offset, \arg_offset, #8
    b 1b
.endm

.macro SETUP_REFERENCE_PARAMETER_IN_GPR gpr32, regs, refs, ins, arg_offset, finished
    str \gpr32, [\regs, \arg_offset]
    subs \ins, \ins, #1
    str \gpr32, [\refs, \arg_offset]
    add \arg_offset, \arg_offset, #4
    beq \finished
.endm

.macro SETUP_REFERENCE_PARAMETERS_IN_STACK regs, refs, ins, stack_ptr, arg_offset
1:
    ldr ip, [\stack_ptr, \arg_offset]
    subs \ins, \ins, #1
    str ip, [\regs, \arg_offset]
    str ip, [\refs, \arg_offset]
    add \arg_offset, \arg_offset, #4
    bne 1b
.endm

/*
 * ArtMethod entry point.
 *
 * On entry:
 *  r0   ArtMethod* callee
 *  rest  method parameters
 */

OAT_ENTRY ExecuteNterpImpl, EndExecuteNterpImpl
    .cfi_startproc
    sub ip, sp, #STACK_OVERFLOW_RESERVED_BYTES
    ldr ip, [ip]
    /* Spill callee save regs */
    SPILL_ALL_CALLEE_SAVES

    ldr rPC, [r0, #ART_METHOD_DATA_OFFSET_32]

    // Setup the stack for executing the method.
    SETUP_STACK_FRAME rPC, rREFS, rFP, CFI_REFS, load_ins=1

    // Setup the parameters
    cmp r4, #0
    beq .Lxmm_setup_finished

    sub rINST, rINST, r4
    ldr r8, [r0, #ART_METHOD_ACCESS_FLAGS_OFFSET]
    lsl rINST, rINST, #2 // rINST is now the offset for inputs into the registers array.
    mov rIBASE, ip // rIBASE contains the old stack pointer

    tst r8, #ART_METHOD_NTERP_ENTRY_POINT_FAST_PATH_FLAG
    beq .Lsetup_slow_path
    // Setup pointer to inputs in FP and pointer to inputs in REFS
    add lr, rFP, rINST
    add r8, rREFS, rINST
    mov r0, #0
    SETUP_REFERENCE_PARAMETER_IN_GPR r1, lr, r8, r4, r0, .Lxmm_setup_finished
    SETUP_REFERENCE_PARAMETER_IN_GPR r2, lr, r8, r4, r0, .Lxmm_setup_finished
    SETUP_REFERENCE_PARAMETER_IN_GPR r3, lr, r8, r4, r0, .Lxmm_setup_finished
    add rIBASE, rIBASE, #OFFSET_TO_FIRST_ARGUMENT_IN_STACK
    SETUP_REFERENCE_PARAMETERS_IN_STACK lr, r8, r4, rIBASE, r0
    b .Lxmm_setup_finished

.Lsetup_slow_path:
    // If the method is not static and there is one argument ('this'), we don't need to fetch the
    // shorty.
    tst r8, #ART_METHOD_IS_STATIC_FLAG
    bne .Lsetup_with_shorty
    str r1, [rFP, rINST]
    str r1, [rREFS, rINST]
    cmp r4, #1
    beq .Lxmm_setup_finished

.Lsetup_with_shorty:
    // Save arguments that were passed before calling into the runtime.
    // No need to save r0 (ArtMethod) as we're not using it later in this code.
    // Save r4 for stack aligment.
    // TODO: Get shorty in a better way and remove below
    push {r1-r4}
    vpush {s0-s15}
    bl NterpGetShorty
    vpop {s0-s15}
    pop {r1-r4}

    mov ip, r8
    add r8, rREFS, rINST
    add r7, rFP, rINST
    mov r4, #0
    // Setup shorty, pointer to inputs in FP and pointer to inputs in REFS
    add lr, r0, #1  // shorty + 1  ; ie skip return arg character
    tst ip, #ART_METHOD_IS_STATIC_FLAG
    bne .Lhandle_static_method
    add r7, r7, #4
    add r8, r8, #4
    add rIBASE, rIBASE, #4
    b .Lcontinue_setup_gprs
.Lhandle_static_method:
    LOOP_OVER_SHORTY_STORING_GPRS r1, lr, r4, r7, r8, .Lgpr_setup_finished, .Lif_long, is_r3=0
.Lcontinue_setup_gprs:
    LOOP_OVER_SHORTY_STORING_GPRS r2, lr, r4, r7, r8, .Lgpr_setup_finished, .Lif_long, is_r3=0
    LOOP_OVER_SHORTY_STORING_GPRS r3, lr, r4, r7, r8, .Lgpr_setup_finished, .Lif_long, is_r3=1
.Lif_long:
    LOOP_OVER_INTs lr, r4, r7, r8, rIBASE, ip, r1, .Lgpr_setup_finished
.Lgpr_setup_finished:
    add r0, r0, #1  // shorty + 1  ; ie skip return arg character
    mov r1, r7
    add r2, rIBASE, #OFFSET_TO_FIRST_ARGUMENT_IN_STACK
    vpush {s0-s15}
    mov r3, sp
    bl NterpStoreArm32Fprs
    add sp, sp, #(16 * 4)
.Lxmm_setup_finished:
    CFI_DEFINE_DEX_PC_WITH_OFFSET(CFI_TMP, CFI_DEX, 0)
    // r8 was used for setting up the frame, restore it now.
    REFRESH_MARKING_REGISTER
.Lexecute_instructions:
    // Set rIBASE
    adr rIBASE, artNterpAsmInstructionStart
    /* start executing the instruction at rPC */
    START_EXECUTING_INSTRUCTIONS
    /* NOTE: no fallthrough */
    // cfi info continues, and covers the whole nterp implementation.
    SIZE ExecuteNterpImpl

    .type artNterpAsmInstructionStart, #object
    .hidden artNterpAsmInstructionStart
    .global artNterpAsmInstructionStart
artNterpAsmInstructionStart = .L_op_nop
    .text

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_nop: /* 0x00 */
    NAME_START nterp_op_nop

    FETCH_ADVANCE_INST 1                @ advance to next instr, load rINST
    GET_INST_OPCODE ip                  @ ip<- opcode from rINST
    GOTO_OPCODE ip                      @ execute it

    NAME_END nterp_op_nop

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_move: /* 0x01 */
    NAME_START nterp_op_move

    /* for move, move-object, long-to-int */
    /* op vA, vB */
    mov     r1, rINST, lsr #12          @ r1<- B from 15:12
    ubfx    r0, rINST, #8, #4           @ r0<- A from 11:8
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST
    GET_VREG r2, r1                     @ r2<- fp[B]
    GET_INST_OPCODE ip                  @ ip<- opcode from rINST
    .if 0
    SET_VREG_OBJECT r2, r0              @ fp[A]<- r2
    .else
    SET_VREG r2, r0                     @ fp[A]<- r2
    .endif
    GOTO_OPCODE ip                      @ execute next instruction

    NAME_END nterp_op_move

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_move_from16: /* 0x02 */
    NAME_START nterp_op_move_from16

    /* for: move/from16, move-object/from16 */
    /* op vAA, vBBBB */
    FETCH r1, 1                         @ r1<- BBBB
    mov     r0, rINST, lsr #8           @ r0<- AA
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    GET_VREG r2, r1                     @ r2<- fp[BBBB]
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    .if 0
    SET_VREG_OBJECT r2, r0              @ fp[AA]<- r2
    .else
    SET_VREG r2, r0                     @ fp[AA]<- r2
    .endif
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_move_from16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_move_16: /* 0x03 */
    NAME_START nterp_op_move_16

    /* for: move/16, move-object/16 */
    /* op vAAAA, vBBBB */
    FETCH r1, 2                         @ r1<- BBBB
    FETCH r0, 1                         @ r0<- AAAA
    FETCH_ADVANCE_INST 3                @ advance rPC, load rINST
    GET_VREG r2, r1                     @ r2<- fp[BBBB]
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    .if 0
    SET_VREG_OBJECT r2, r0              @ fp[AAAA]<- r2
    .else
    SET_VREG r2, r0                     @ fp[AAAA]<- r2
    .endif
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_move_16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_move_wide: /* 0x04 */
    NAME_START nterp_op_move_wide

    /* move-wide vA, vB */
    /* NOTE: regs can overlap, e.g. "move v6,v7" or "move v7,v6" */
    mov     r3, rINST, lsr #12          @ r3<- B
    ubfx    rINST, rINST, #8, #4        @ rINST<- A
    VREG_INDEX_TO_ADDR r3, r3           @ r3<- &fp[B]
    VREG_INDEX_TO_ADDR r2, rINST        @ r2<- &fp[A]
    GET_VREG_WIDE_BY_ADDR r0, r1, r3    @ r0/r1<- fp[B]
    CLEAR_SHADOW_PAIR rINST, ip, lr     @ Zero out the shadow regs
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_WIDE_BY_ADDR r0, r1, r2    @ fp[A]<- r0/r1
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_move_wide

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_move_wide_from16: /* 0x05 */
    NAME_START nterp_op_move_wide_from16

    /* move-wide/from16 vAA, vBBBB */
    /* NOTE: regs can overlap, e.g. "move v6,v7" or "move v7,v6" */
    FETCH r3, 1                         @ r3<- BBBB
    mov     rINST, rINST, lsr #8        @ rINST<- AA
    VREG_INDEX_TO_ADDR r3, r3           @ r3<- &fp[BBBB]
    VREG_INDEX_TO_ADDR r2, rINST        @ r2<- &fp[AA]
    GET_VREG_WIDE_BY_ADDR r0, r1, r3    @ r0/r1<- fp[BBBB]
    CLEAR_SHADOW_PAIR rINST, ip, lr     @ Zero out the shadow regs
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_WIDE_BY_ADDR r0, r1, r2    @ fp[AA]<- r0/r1
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_move_wide_from16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_move_wide_16: /* 0x06 */
    NAME_START nterp_op_move_wide_16

    /* move-wide/16 vAAAA, vBBBB */
    /* NOTE: regs can overlap, e.g. "move v6,v7" or "move v7,v6" */
    FETCH r3, 2                         @ r3<- BBBB
    FETCH r2, 1                         @ r2<- AAAA
    VREG_INDEX_TO_ADDR r3, r3           @ r3<- &fp[BBBB]
    VREG_INDEX_TO_ADDR lr, r2           @ r2<- &fp[AAAA]
    GET_VREG_WIDE_BY_ADDR r0, r1, r3    @ r0/r1<- fp[BBBB]
    FETCH_ADVANCE_INST 3                @ advance rPC, load rINST
    CLEAR_SHADOW_PAIR r2, r3, ip        @ Zero out the shadow regs
    SET_VREG_WIDE_BY_ADDR r0, r1, lr    @ fp[AAAA]<- r0/r1
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_move_wide_16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_move_object: /* 0x07 */
    NAME_START nterp_op_move_object

    /* for move, move-object, long-to-int */
    /* op vA, vB */
    mov     r1, rINST, lsr #12          @ r1<- B from 15:12
    ubfx    r0, rINST, #8, #4           @ r0<- A from 11:8
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST
    GET_VREG r2, r1                     @ r2<- fp[B]
    GET_INST_OPCODE ip                  @ ip<- opcode from rINST
    .if 1
    SET_VREG_OBJECT r2, r0              @ fp[A]<- r2
    .else
    SET_VREG r2, r0                     @ fp[A]<- r2
    .endif
    GOTO_OPCODE ip                      @ execute next instruction

    NAME_END nterp_op_move_object

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_move_object_from16: /* 0x08 */
    NAME_START nterp_op_move_object_from16

    /* for: move/from16, move-object/from16 */
    /* op vAA, vBBBB */
    FETCH r1, 1                         @ r1<- BBBB
    mov     r0, rINST, lsr #8           @ r0<- AA
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    GET_VREG r2, r1                     @ r2<- fp[BBBB]
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    .if 1
    SET_VREG_OBJECT r2, r0              @ fp[AA]<- r2
    .else
    SET_VREG r2, r0                     @ fp[AA]<- r2
    .endif
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_move_object_from16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_move_object_16: /* 0x09 */
    NAME_START nterp_op_move_object_16

    /* for: move/16, move-object/16 */
    /* op vAAAA, vBBBB */
    FETCH r1, 2                         @ r1<- BBBB
    FETCH r0, 1                         @ r0<- AAAA
    FETCH_ADVANCE_INST 3                @ advance rPC, load rINST
    GET_VREG r2, r1                     @ r2<- fp[BBBB]
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    .if 1
    SET_VREG_OBJECT r2, r0              @ fp[AAAA]<- r2
    .else
    SET_VREG r2, r0                     @ fp[AAAA]<- r2
    .endif
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_move_object_16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_move_result: /* 0x0a */
    NAME_START nterp_op_move_result

    /* for: move-result, move-result-object */
    /* op vAA */
    mov     r2, rINST, lsr #8           @ r2<- AA
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    .if 0
    SET_VREG_OBJECT r0, r2              @ fp[AA]<- r0
    .else
    SET_VREG r0, r2                     @ fp[AA]<- r0
    .endif
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_move_result

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_move_result_wide: /* 0x0b */
    NAME_START nterp_op_move_result_wide

    /* move-result-wide vAA */
    mov     rINST, rINST, lsr #8        @ rINST<- AA
    VREG_INDEX_TO_ADDR r2, rINST        @ r2<- &fp[AA]
    CLEAR_SHADOW_PAIR rINST, ip, lr     @ Zero out the shadow regs
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST
    SET_VREG_WIDE_BY_ADDR r0, r1, r2    @ fp[AA]<- r0/r1
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_move_result_wide

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_move_result_object: /* 0x0c */
    NAME_START nterp_op_move_result_object

    /* for: move-result, move-result-object */
    /* op vAA */
    mov     r2, rINST, lsr #8           @ r2<- AA
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    .if 1
    SET_VREG_OBJECT r0, r2              @ fp[AA]<- r0
    .else
    SET_VREG r0, r2                     @ fp[AA]<- r0
    .endif
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_move_result_object

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_move_exception: /* 0x0d */
    NAME_START nterp_op_move_exception

    /* move-exception vAA */
    mov     r2, rINST, lsr #8           @ r2<- AA
    ldr     r3, [rSELF, #THREAD_EXCEPTION_OFFSET]
    mov     r1, #0                      @ r1<- 0
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST
    SET_VREG_OBJECT r3, r2              @ fp[AA]<- exception obj
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    str     r1, [rSELF, #THREAD_EXCEPTION_OFFSET]  @ clear exception
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_move_exception

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_return_void: /* 0x0e */
    NAME_START nterp_op_return_void

    .if 1
      // Thread fence for constructor
      dmb ishst
    .else
      mov     r2, rINST, lsr #8           @ r2<- AA
      .if 0
        VREG_INDEX_TO_ADDR r2, r2
        GET_VREG_WIDE_BY_ADDR r0, r1, r2 // r0,r1 <- vAA
        // In case we're going back to compiled code, put the
        // result also in d0.
        vmov d0, r0, r1
      .else
        GET_VREG r0, r2                     // r0<- vAA
        .if !0
        // In case we're going back to compiled code, put the
        // result also in s0.
        vmov s0, r0
        .endif
      .endif
    .endif
    .cfi_remember_state
    ldr ip, [rREFS, #-4]
    mov sp, ip
    .cfi_def_cfa sp, CALLEE_SAVES_SIZE
    RESTORE_ALL_CALLEE_SAVES lr_to_pc=1
    .cfi_restore_state

    NAME_END nterp_op_return_void

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_return: /* 0x0f */
    NAME_START nterp_op_return

    .if 0
      // Thread fence for constructor
      dmb ishst
    .else
      mov     r2, rINST, lsr #8           @ r2<- AA
      .if 0
        VREG_INDEX_TO_ADDR r2, r2
        GET_VREG_WIDE_BY_ADDR r0, r1, r2 // r0,r1 <- vAA
        // In case we're going back to compiled code, put the
        // result also in d0.
        vmov d0, r0, r1
      .else
        GET_VREG r0, r2                     // r0<- vAA
        .if !0
        // In case we're going back to compiled code, put the
        // result also in s0.
        vmov s0, r0
        .endif
      .endif
    .endif
    .cfi_remember_state
    ldr ip, [rREFS, #-4]
    mov sp, ip
    .cfi_def_cfa sp, CALLEE_SAVES_SIZE
    RESTORE_ALL_CALLEE_SAVES lr_to_pc=1
    .cfi_restore_state

    NAME_END nterp_op_return

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_return_wide: /* 0x10 */
    NAME_START nterp_op_return_wide

    .if 0
      // Thread fence for constructor
      dmb ishst
    .else
      mov     r2, rINST, lsr #8           @ r2<- AA
      .if 1
        VREG_INDEX_TO_ADDR r2, r2
        GET_VREG_WIDE_BY_ADDR r0, r1, r2 // r0,r1 <- vAA
        // In case we're going back to compiled code, put the
        // result also in d0.
        vmov d0, r0, r1
      .else
        GET_VREG r0, r2                     // r0<- vAA
        .if !0
        // In case we're going back to compiled code, put the
        // result also in s0.
        vmov s0, r0
        .endif
      .endif
    .endif
    .cfi_remember_state
    ldr ip, [rREFS, #-4]
    mov sp, ip
    .cfi_def_cfa sp, CALLEE_SAVES_SIZE
    RESTORE_ALL_CALLEE_SAVES lr_to_pc=1
    .cfi_restore_state

    NAME_END nterp_op_return_wide

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_return_object: /* 0x11 */
    NAME_START nterp_op_return_object

    .if 0
      // Thread fence for constructor
      dmb ishst
    .else
      mov     r2, rINST, lsr #8           @ r2<- AA
      .if 0
        VREG_INDEX_TO_ADDR r2, r2
        GET_VREG_WIDE_BY_ADDR r0, r1, r2 // r0,r1 <- vAA
        // In case we're going back to compiled code, put the
        // result also in d0.
        vmov d0, r0, r1
      .else
        GET_VREG r0, r2                     // r0<- vAA
        .if !1
        // In case we're going back to compiled code, put the
        // result also in s0.
        vmov s0, r0
        .endif
      .endif
    .endif
    .cfi_remember_state
    ldr ip, [rREFS, #-4]
    mov sp, ip
    .cfi_def_cfa sp, CALLEE_SAVES_SIZE
    RESTORE_ALL_CALLEE_SAVES lr_to_pc=1
    .cfi_restore_state

    NAME_END nterp_op_return_object

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_const_4: /* 0x12 */
    NAME_START nterp_op_const_4

    /* const/4 vA, #+B */
    sbfx    r1, rINST, #12, #4          @ r1<- sssssssB (sign-extended)
    ubfx    r0, rINST, #8, #4           @ r0<- A
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST
    GET_INST_OPCODE ip                  @ ip<- opcode from rINST
    SET_VREG r1, r0                     @ fp[A]<- r1
    GOTO_OPCODE ip                      @ execute next instruction

    NAME_END nterp_op_const_4

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_const_16: /* 0x13 */
    NAME_START nterp_op_const_16

    /* const/16 vAA, #+BBBB */
    FETCH_S r0, 1                       @ r0<- ssssBBBB (sign-extended)
    mov     r3, rINST, lsr #8           @ r3<- AA
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    SET_VREG r0, r3                     @ vAA<- r0
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_const_16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_const: /* 0x14 */
    NAME_START nterp_op_const

    /* const vAA, #+BBBBbbbb */
    mov     r3, rINST, lsr #8           @ r3<- AA
    FETCH r0, 1                         @ r0<- bbbb (low)
    FETCH r1, 2                         @ r1<- BBBB (high)
    FETCH_ADVANCE_INST 3                @ advance rPC, load rINST
    orr     r0, r0, r1, lsl #16         @ r0<- BBBBbbbb
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG r0, r3                     @ vAA<- r0
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_const

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_const_high16: /* 0x15 */
    NAME_START nterp_op_const_high16

    /* const/high16 vAA, #+BBBB0000 */
    FETCH r0, 1                         @ r0<- 0000BBBB (zero-extended)
    mov     r3, rINST, lsr #8           @ r3<- AA
    mov     r0, r0, lsl #16             @ r0<- BBBB0000
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    SET_VREG r0, r3                     @ vAA<- r0
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_const_high16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_const_wide_16: /* 0x16 */
    NAME_START nterp_op_const_wide_16

    /* const-wide/16 vAA, #+BBBB */
    FETCH_S r0, 1                       @ r0<- ssssBBBB (sign-extended)
    mov     r3, rINST, lsr #8           @ r3<- AA
    mov     r1, r0, asr #31             @ r1<- ssssssss
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    CLEAR_SHADOW_PAIR r3, r2, lr        @ Zero out the shadow regs
    VREG_INDEX_TO_ADDR r3, r3           @ r3<- &fp[AA]
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_WIDE_BY_ADDR r0, r1, r3    @ vAA<- r0/r1
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_const_wide_16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_const_wide_32: /* 0x17 */
    NAME_START nterp_op_const_wide_32

    /* const-wide/32 vAA, #+BBBBbbbb */
    FETCH r0, 1                         @ r0<- 0000bbbb (low)
    mov     r3, rINST, lsr #8           @ r3<- AA
    FETCH_S r2, 2                       @ r2<- ssssBBBB (high)
    FETCH_ADVANCE_INST 3                @ advance rPC, load rINST
    orr     r0, r0, r2, lsl #16         @ r0<- BBBBbbbb
    CLEAR_SHADOW_PAIR r3, r2, lr        @ Zero out the shadow regs
    VREG_INDEX_TO_ADDR r3, r3           @ r3<- &fp[AA]
    mov     r1, r0, asr #31             @ r1<- ssssssss
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_WIDE_BY_ADDR r0, r1, r3    @ vAA<- r0/r1
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_const_wide_32

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_const_wide: /* 0x18 */
    NAME_START nterp_op_const_wide

    /* const-wide vAA, #+HHHHhhhhBBBBbbbb */
    FETCH r0, 1                         @ r0<- bbbb (low)
    FETCH r1, 2                         @ r1<- BBBB (low middle)
    FETCH r2, 3                         @ r2<- hhhh (high middle)
    orr     r0, r0, r1, lsl #16         @ r0<- BBBBbbbb (low word)
    FETCH r3, 4                         @ r3<- HHHH (high)
    mov     r4, rINST, lsr #8           @ r4<- AA
    orr     r1, r2, r3, lsl #16         @ r1<- HHHHhhhh (high word)
    CLEAR_SHADOW_PAIR r4, r2, r3        @ Zero out the shadow regs
    FETCH_ADVANCE_INST 5                @ advance rPC, load rINST
    VREG_INDEX_TO_ADDR r4, r4           @ r4<- &fp[AA]
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_WIDE_BY_ADDR r0, r1, r4    @ vAA<- r0/r1
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_const_wide

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_const_wide_high16: /* 0x19 */
    NAME_START nterp_op_const_wide_high16

    /* const-wide/high16 vAA, #+BBBB000000000000 */
    FETCH r1, 1                         @ r1<- 0000BBBB (zero-extended)
    mov     r3, rINST, lsr #8           @ r3<- AA
    mov     r0, #0                      @ r0<- 00000000
    mov     r1, r1, lsl #16             @ r1<- BBBB0000
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    CLEAR_SHADOW_PAIR r3, r0, r2        @ Zero shadow regs
    VREG_INDEX_TO_ADDR r3, r3           @ r3<- &fp[AA]
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_WIDE_BY_ADDR r0, r1, r3    @ vAA<- r0/r1
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_const_wide_high16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_const_string: /* 0x1a */
    NAME_START nterp_op_const_string

   /* const/string vAA, String@BBBB */
   // Fast-path which gets the object from thread-local cache.
   FETCH_FROM_THREAD_CACHE r0, 2f
   cmp rMR, #0
   bne 3f
1:
   mov     r1, rINST, lsr #8           @ r1<- AA
   .if 0
   FETCH_ADVANCE_INST 3                // advance rPC, load rINST
   .else
   FETCH_ADVANCE_INST 2                // advance rPC, load rINST
   .endif
   GET_INST_OPCODE ip                  // extract opcode from rINST
   SET_VREG_OBJECT r0, r1              // vAA <- value
   GOTO_OPCODE ip                      // jump to next instruction
2:
   EXPORT_PC
   mov r0, rSELF
   ldr r1, [sp]
   mov r2, rPC
   bl nterp_load_object
   b 1b
3:
   bl art_quick_read_barrier_mark_reg00
   b 1b

    NAME_END nterp_op_const_string

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_const_string_jumbo: /* 0x1b */
    NAME_START nterp_op_const_string_jumbo

   /* const/string vAA, String@BBBBBBBB */
   // Fast-path which gets the object from thread-local cache.
   FETCH_FROM_THREAD_CACHE r0, 2f
   cmp rMR, #0
   bne 3f
1:
   mov     r1, rINST, lsr #8           @ r1<- AA
   .if 1
   FETCH_ADVANCE_INST 3                // advance rPC, load rINST
   .else
   FETCH_ADVANCE_INST 2                // advance rPC, load rINST
   .endif
   GET_INST_OPCODE ip                  // extract opcode from rINST
   SET_VREG_OBJECT r0, r1              // vAA <- value
   GOTO_OPCODE ip                      // jump to next instruction
2:
   EXPORT_PC
   mov r0, rSELF
   ldr r1, [sp]
   mov r2, rPC
   bl nterp_load_object
   b 1b
3:
   bl art_quick_read_barrier_mark_reg00
   b 1b

    NAME_END nterp_op_const_string_jumbo

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_const_class: /* 0x1c */
    NAME_START nterp_op_const_class

   // Fast-path which gets the object from thread-local cache.
   FETCH_FROM_THREAD_CACHE r0, 2f
   cmp rMR, #0
   bne 3f
1:
   mov     r1, rINST, lsr #8           @ r1<- AA
   .if 0
   FETCH_ADVANCE_INST 3                // advance rPC, load rINST
   .else
   FETCH_ADVANCE_INST 2                // advance rPC, load rINST
   .endif
   GET_INST_OPCODE ip                  // extract opcode from rINST
   SET_VREG_OBJECT r0, r1              // vAA <- value
   GOTO_OPCODE ip                      // jump to next instruction
2:
   EXPORT_PC
   mov r0, rSELF
   ldr r1, [sp]
   mov r2, rPC
   bl nterp_get_class_or_allocate_object
   b 1b
3:
   bl art_quick_read_barrier_mark_reg00
   b 1b

    NAME_END nterp_op_const_class

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_monitor_enter: /* 0x1d */
    NAME_START nterp_op_monitor_enter

    /*
     * Synchronize on an object.
     */
    /* monitor-enter vAA */
    EXPORT_PC
    mov      r2, rINST, lsr #8           @ r2<- AA
    GET_VREG r0, r2                      @ r0<- vAA (object)
    bl       art_quick_lock_object
    FETCH_ADVANCE_INST 1
    GET_INST_OPCODE ip                   @ extract opcode from rINST
    GOTO_OPCODE ip                       @ jump to next instruction

    NAME_END nterp_op_monitor_enter

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_monitor_exit: /* 0x1e */
    NAME_START nterp_op_monitor_exit

    /*
     * Unlock an object.
     *
     * Exceptions that occur when unlocking a monitor need to appear as
     * if they happened at the following instruction.  See the Dalvik
     * instruction spec.
     */
    /* monitor-exit vAA */
    EXPORT_PC
    mov      r2, rINST, lsr #8          @ r2<- AA
    GET_VREG r0, r2                     @ r0<- vAA (object)
    bl       art_quick_unlock_object
    FETCH_ADVANCE_INST 1                @ before throw: advance rPC, load rINST
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_monitor_exit

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_check_cast: /* 0x1f */
    NAME_START nterp_op_check_cast

   // Fast-path which gets the class from thread-local cache.
   EXPORT_PC
   FETCH_FROM_THREAD_CACHE r1, 3f
   cmp     rMR, #0
   bne     4f
1:
   lsr     r2, rINST, #8               // r2<- A
   GET_VREG r0, r2                     // r0<- vA (object)
   cmp     r0, #0
   beq     2f
   bl      art_quick_check_instance_of
2:
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
3:
   mov     r0, rSELF
   ldr     r1, [sp]
   mov     r2, rPC
   bl      nterp_get_class_or_allocate_object
   mov     r1, r0
   b       1b
4:
   bl      art_quick_read_barrier_mark_reg01
   b       1b

    NAME_END nterp_op_check_cast

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_instance_of: /* 0x20 */
    NAME_START nterp_op_instance_of

   /* instance-of vA, vB, class@CCCC */
   // Fast-path which gets the class from thread-local cache.
   EXPORT_PC
   FETCH_FROM_THREAD_CACHE r1, 3f
   cmp     rMR, #0
   bne     4f
1:
   lsr     r2, rINST, #12              // r2<- B
   GET_VREG r0, r2                     // r0<- vB (object)
   cmp     r0, #0
   beq     2f
   bl      artInstanceOfFromCode
2:
   ubfx    r1, rINST, #8, #4           // r1<- A
   SET_VREG r0, r1
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
3:
   mov     r0, rSELF
   ldr     r1, [sp]
   mov     r2, rPC
   bl      nterp_get_class_or_allocate_object
   mov     r1, r0
   b       1b
4:
   bl      art_quick_read_barrier_mark_reg01
   b       1b

    NAME_END nterp_op_instance_of

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_array_length: /* 0x21 */
    NAME_START nterp_op_array_length

    /*
     * Return the length of an array.
     */
    mov     r1, rINST, lsr #12          @ r1<- B
    ubfx    r2, rINST, #8, #4           @ r2<- A
    GET_VREG r0, r1                     @ r0<- vB (object ref)
    cmp     r0, #0                      @ is object null?
    beq     common_errNullObject        @ yup, fail
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST
    ldr     r3, [r0, #MIRROR_ARRAY_LENGTH_OFFSET]    @ r3<- array length
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG r3, r2                     @ vB<- length
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_array_length

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_new_instance: /* 0x22 */
    NAME_START nterp_op_new_instance

   // The routine is too big to fit in a handler, so jump to it.
   EXPORT_PC
   // Fast-path which gets the class from thread-local cache.
   FETCH_FROM_THREAD_CACHE r0, 2f
   cmp     rMR, #0
   bne     3f
4:
   ldr     lr, [rSELF, #THREAD_ALLOC_OBJECT_ENTRYPOINT_OFFSET]
   blx     lr
1:
   lsr     r1, rINST, #8                    // r1 <- A
   SET_VREG_OBJECT r0, r1               // fp[A] <- value
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
2:
   mov     r0, rSELF
   ldr     r1, [sp]
   mov     r2, rPC
   bl      nterp_get_class_or_allocate_object
   b       1b
3:
   bl      art_quick_read_barrier_mark_reg00
   b       4b
    NAME_END nterp_op_new_instance

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_new_array: /* 0x23 */
    NAME_START nterp_op_new_array

  b NterpNewArray
    NAME_END nterp_op_new_array

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_filled_new_array: /* 0x24 */
    NAME_START nterp_op_filled_new_array

/*
 * Create a new array with elements filled from registers.
 *
 * for: filled-new-array, filled-new-array/range
 */
    /* op vB, {vD, vE, vF, vG, vA}, class@CCCC */
    /* op {vCCCC..v(CCCC+AA-1)}, type@BBBB */
    EXPORT_PC
    mov     r0, rSELF
    ldr     r1, [sp]
    mov     r2, rFP
    mov     r3, rPC
    bl      nterp_filled_new_array
    FETCH_ADVANCE_INST 3                // advance rPC, load rINST
    GET_INST_OPCODE ip                  // extract opcode from rINST
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_filled_new_array

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_filled_new_array_range: /* 0x25 */
    NAME_START nterp_op_filled_new_array_range

/*
 * Create a new array with elements filled from registers.
 *
 * for: filled-new-array, filled-new-array/range
 */
    /* op vB, {vD, vE, vF, vG, vA}, class@CCCC */
    /* op {vCCCC..v(CCCC+AA-1)}, type@BBBB */
    EXPORT_PC
    mov     r0, rSELF
    ldr     r1, [sp]
    mov     r2, rFP
    mov     r3, rPC
    bl      nterp_filled_new_array_range
    FETCH_ADVANCE_INST 3                // advance rPC, load rINST
    GET_INST_OPCODE ip                  // extract opcode from rINST
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_filled_new_array_range

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_fill_array_data: /* 0x26 */
    NAME_START nterp_op_fill_array_data

    /* fill-array-data vAA, +BBBBBBBB */
    EXPORT_PC
    FETCH r0, 1                         @ r0<- bbbb (lo)
    FETCH r1, 2                         @ r1<- BBBB (hi)
    mov     r3, rINST, lsr #8           @ r3<- AA
    orr     r0, r0, r1, lsl #16         @ r0<- BBBBbbbb
    GET_VREG r1, r3                     @ r1<- vAA (array object)
    add     r0, rPC, r0, lsl #1         @ r1<- PC + BBBBbbbb*2 (array data off.)
    bl      art_quick_handle_fill_data  @ (payload, object)
    FETCH_ADVANCE_INST 3                @ advance rPC, load rINST
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_fill_array_data

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_throw: /* 0x27 */
    NAME_START nterp_op_throw

  EXPORT_PC
  mov      r2, rINST, lsr #8           @ r2<- AA
  GET_VREG r0, r2                      @ r0<- vAA (exception object)
  mov r1, rSELF
  bl art_quick_deliver_exception
  bkpt 0
    NAME_END nterp_op_throw

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_goto: /* 0x28 */
    NAME_START nterp_op_goto

/*
 * Unconditional branch, 8-bit offset.
 *
 * The branch distance is a signed code-unit offset, which we need to
 * double to get a byte offset.
 */
    /* goto +AA */
    sbfx    rINST, rINST, #8, #8           // rINST<- ssssssAA (sign-extended)
    BRANCH

    NAME_END nterp_op_goto

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_goto_16: /* 0x29 */
    NAME_START nterp_op_goto_16

/*
 * Unconditional branch, 16-bit offset.
 *
 * The branch distance is a signed code-unit offset, which we need to
 * double to get a byte offset.
 */
    /* goto/16 +AAAA */
    FETCH_S rINST, 1                    // wINST<- ssssAAAA (sign-extended)
    BRANCH

    NAME_END nterp_op_goto_16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_goto_32: /* 0x2a */
    NAME_START nterp_op_goto_32

/*
 * Unconditional branch, 32-bit offset.
 *
 * The branch distance is a signed code-unit offset, which we need to
 * double to get a byte offset.
 *
 * Because we need the SF bit set, we'll use an adds
 * to convert from Dalvik offset to byte offset.
 */
    /* goto/32 +AAAAAAAA */
    FETCH r0, 1                         // r0<- aaaa (lo)
    FETCH r1, 2                         // r1<- AAAA (hi)
    orrs     rINST, r0, r1, lsl #16      // wINST<- AAAAaaaa
    BRANCH

    NAME_END nterp_op_goto_32

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_packed_switch: /* 0x2b */
    NAME_START nterp_op_packed_switch

/*
 * Handle a packed-switch or sparse-switch instruction.  In both cases
 * we decode it and hand it off to a helper function.
 *
 * We don't really expect backward branches in a switch statement, but
 * they're perfectly legal, so we check for them here.
 *
 * for: packed-switch, sparse-switch
 */
    /* op vAA, +BBBB */
    FETCH r0, 1                         @ r0<- bbbb (lo)
    FETCH r1, 2                         @ r1<- BBBB (hi)
    mov     r3, rINST, lsr #8           @ r3<- AA
    orr     r0, r0, r1, lsl #16         @ r0<- BBBBbbbb
    GET_VREG r1, r3                     @ r1<- vAA
    add     r0, rPC, r0, lsl #1         @ r0<- PC + BBBBbbbb*2
    bl      NterpDoPackedSwitch                       @ r0<- code-unit branch offset
    mov     rINST, r0
    BRANCH

    NAME_END nterp_op_packed_switch

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sparse_switch: /* 0x2c */
    NAME_START nterp_op_sparse_switch

/*
 * Handle a packed-switch or sparse-switch instruction.  In both cases
 * we decode it and hand it off to a helper function.
 *
 * We don't really expect backward branches in a switch statement, but
 * they're perfectly legal, so we check for them here.
 *
 * for: packed-switch, sparse-switch
 */
    /* op vAA, +BBBB */
    FETCH r0, 1                         @ r0<- bbbb (lo)
    FETCH r1, 2                         @ r1<- BBBB (hi)
    mov     r3, rINST, lsr #8           @ r3<- AA
    orr     r0, r0, r1, lsl #16         @ r0<- BBBBbbbb
    GET_VREG r1, r3                     @ r1<- vAA
    add     r0, rPC, r0, lsl #1         @ r0<- PC + BBBBbbbb*2
    bl      NterpDoSparseSwitch                       @ r0<- code-unit branch offset
    mov     rINST, r0
    BRANCH

/*
 * Return a 32-bit value.
 */
    NAME_END nterp_op_sparse_switch

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_cmpl_float: /* 0x2d */
    NAME_START nterp_op_cmpl_float

    /*
     * Compare two floating-point values.  Puts 0, 1, or -1 into the
     * destination register based on the results of the comparison.
     *
     * int compare(x, y) {
     *     if (x == y) {
     *         return 0;
     *     } else if (x > y) {
     *         return 1;
     *     } else if (x < y) {
     *         return -1;
     *     } else {
     *         return -1;
     *     }
     * }
     */
    /* op vAA, vBB, vCC */
    FETCH r0, 1                         @ r0<- CCBB
    mov     r4, rINST, lsr #8           @ r4<- AA
    and     r2, r0, #255                @ r2<- BB
    mov     r3, r0, lsr #8              @ r3<- CC
    VREG_INDEX_TO_ADDR r2, r2           @ r2<- &vBB
    VREG_INDEX_TO_ADDR r3, r3           @ r3<- &vCC
    GET_VREG_FLOAT_BY_ADDR s0, r2       @ s0<- vBB
    GET_VREG_FLOAT_BY_ADDR s1, r3       @ s1<- vCC
    vcmpe.f32  s0, s1                   @ compare (vBB, vCC)
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    mvn     r0, #0                      @ r0<- -1 (default)
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    fmstat                              @ export status flags
    it gt
    movgt   r0, #1                      @ (greater than) r1<- 1
    it eq
    moveq   r0, #0                      @ (equal) r1<- 0
    SET_VREG r0, r4                     @ vAA<- r0
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_cmpl_float

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_cmpg_float: /* 0x2e */
    NAME_START nterp_op_cmpg_float

    /*
     * Compare two floating-point values.  Puts 0, 1, or -1 into the
     * destination register based on the results of the comparison.
     *
     * int compare(x, y) {
     *     if (x == y) {
     *         return 0;
     *     } else if (x < y) {
     *         return -1;
     *     } else if (x > y) {
     *         return 1;
     *     } else {
     *         return 1;
     *     }
     * }
     */
    /* op vAA, vBB, vCC */
    FETCH r0, 1                         @ r0<- CCBB
    mov     r4, rINST, lsr #8           @ r4<- AA
    and     r2, r0, #255                @ r2<- BB
    mov     r3, r0, lsr #8              @ r3<- CC
    VREG_INDEX_TO_ADDR r2, r2           @ r2<- &vBB
    VREG_INDEX_TO_ADDR r3, r3           @ r3<- &vCC
    GET_VREG_FLOAT_BY_ADDR s0, r2       @ s0<- vBB
    GET_VREG_FLOAT_BY_ADDR s1, r3       @ s1<- vCC
    vcmpe.f32 s0, s1                    @ compare (vBB, vCC)
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    mvn     r0, #0                      @ r0<- -1 (default)
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    fmstat
    it      hi
    movhi   r0, #1                      @ (greater than, or unordered) r0<- 1
    moveq   r0, #0                      @ (equal) r0<- 0
    SET_VREG r0, r4                     @ vAA<- r0
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_cmpg_float

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_cmpl_double: /* 0x2f */
    NAME_START nterp_op_cmpl_double

    /*
     * Compare two floating-point values.  Puts 0, 1, or -1 into the
     * destination register based on the results of the comparison.
     *
     * int compare(x, y) {
     *     if (x == y) {
     *         return 0;
     *     } else if (x > y) {
     *         return 1;
     *     } else if (x < y) {
     *         return -1;
     *     } else {
     *         return -1;
     *     }
     * }
     */
    /* op vAA, vBB, vCC */
    FETCH r0, 1                         @ r0<- CCBB
    mov     r4, rINST, lsr #8           @ r4<- AA
    and     r2, r0, #255                @ r2<- BB
    mov     r3, r0, lsr #8              @ r3<- CC
    VREG_INDEX_TO_ADDR r2, r2           @ r2<- &vBB
    VREG_INDEX_TO_ADDR r3, r3           @ r3<- &vCC
    GET_VREG_DOUBLE_BY_ADDR d0, r2      @ d0<- vBB
    GET_VREG_DOUBLE_BY_ADDR d1, r3      @ d1<- vCC
    vcmpe.f64 d0, d1                    @ compare (vBB, vCC)
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    mvn     r0, #0                      @ r0<- -1 (default)
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    fmstat                              @ export status flags
    it gt
    movgt   r0, #1                      @ (greater than) r1<- 1
    it eq
    moveq   r0, #0                      @ (equal) r1<- 0
    SET_VREG r0, r4                     @ vAA<- r0
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_cmpl_double

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_cmpg_double: /* 0x30 */
    NAME_START nterp_op_cmpg_double

    /*
     * Compare two floating-point values.  Puts 0, 1, or -1 into the
     * destination register based on the results of the comparison.
     *
     * int compare(x, y) {
     *     if (x == y) {
     *         return 0;
     *     } else if (x < y) {
     *         return -1;
     *     } else if (x > y) {
     *         return 1;
     *     } else {
     *         return 1;
     *     }
     * }
     */
    /* op vAA, vBB, vCC */
    FETCH r0, 1                         @ r0<- CCBB
    mov     r4, rINST, lsr #8           @ r4<- AA
    and     r2, r0, #255                @ r2<- BB
    mov     r3, r0, lsr #8              @ r3<- CC
    VREG_INDEX_TO_ADDR r2, r2           @ r2<- &vBB
    VREG_INDEX_TO_ADDR r3, r3           @ r3<- &vCC
    GET_VREG_DOUBLE_BY_ADDR d0, r2      @ d0<- vBB
    GET_VREG_DOUBLE_BY_ADDR d1, r3      @ d1<- vCC
    vcmpe.f64 d0, d1                    @ compare (vBB, vCC)
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    mvn     r0, #0                      @ r0<- -1 (default)
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    fmstat
    it      hi
    movhi   r0, #1                      @ (greater than, or unordered) r0<- 1
    moveq   r0, #0                      @ (equal) r0<- 0
    SET_VREG r0, r4                     @ vAA<- r0
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_cmpg_double

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_cmp_long: /* 0x31 */
    NAME_START nterp_op_cmp_long

    /*
     * Compare two 64-bit values.  Puts 0, 1, or -1 into the destination
     * register based on the results of the comparison.
     */
    /* cmp-long vAA, vBB, vCC */
    FETCH r0, 1                         @ r0<- CCBB
    mov     r4, rINST, lsr #8           @ r4<- AA
    and     r2, r0, #255                @ r2<- BB
    mov     r3, r0, lsr #8              @ r3<- CC
    VREG_INDEX_TO_ADDR r2, r2           @ r2<- &fp[BB]
    VREG_INDEX_TO_ADDR r3, r3           @ r3<- &fp[CC]
    GET_VREG_WIDE_BY_ADDR r0, r1, r2    @ r0/r1<- vBB/vBB+1
    GET_VREG_WIDE_BY_ADDR r2, r3, r3    @ r2/r3<- vCC/vCC+1
    cmp     r0, r2
    sbcs    ip, r1, r3                  @ Sets correct CCs for checking LT (but not EQ/NE)
    mov     r3, #-1
    it      ge
    movge   r3, #1
    it      eq
    cmpeq   r0, r2
    it      eq
    moveq   r3, #0
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    SET_VREG r3, r4                     @ vAA<- ip
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_cmp_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_if_eq: /* 0x32 */
    NAME_START nterp_op_if_eq

    /*
     * Generic two-operand compare-and-branch operation.  Provide a "condition"
     * fragment that specifies the comparison to perform.
     *
     * For: if-eq, if-ne, if-lt, if-ge, if-gt, if-le
     */
    /* if-cmp vA, vB, +CCCC */
    mov     r1, rINST, lsr #12          @ r1<- B
    ubfx    r0, rINST, #8, #4           @ r0<- A
    GET_VREG r3, r1                     @ r3<- vB
    GET_VREG r0, r0                     @ r0<- vA
    FETCH_S rINST, 1                    @ rINST<- branch offset, in code units
    cmp     r0, r3                      @ compare (vA, vB)
    beq 1f
    FETCH_ADVANCE_INST 2
    GET_INST_OPCODE ip                  // extract opcode from rINST
    GOTO_OPCODE ip                      // jump to next instruction
1:
    FETCH_S rINST, 1                    // rINST<- branch offset, in code units
    BRANCH

    NAME_END nterp_op_if_eq

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_if_ne: /* 0x33 */
    NAME_START nterp_op_if_ne

    /*
     * Generic two-operand compare-and-branch operation.  Provide a "condition"
     * fragment that specifies the comparison to perform.
     *
     * For: if-eq, if-ne, if-lt, if-ge, if-gt, if-le
     */
    /* if-cmp vA, vB, +CCCC */
    mov     r1, rINST, lsr #12          @ r1<- B
    ubfx    r0, rINST, #8, #4           @ r0<- A
    GET_VREG r3, r1                     @ r3<- vB
    GET_VREG r0, r0                     @ r0<- vA
    FETCH_S rINST, 1                    @ rINST<- branch offset, in code units
    cmp     r0, r3                      @ compare (vA, vB)
    bne 1f
    FETCH_ADVANCE_INST 2
    GET_INST_OPCODE ip                  // extract opcode from rINST
    GOTO_OPCODE ip                      // jump to next instruction
1:
    FETCH_S rINST, 1                    // rINST<- branch offset, in code units
    BRANCH

    NAME_END nterp_op_if_ne

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_if_lt: /* 0x34 */
    NAME_START nterp_op_if_lt

    /*
     * Generic two-operand compare-and-branch operation.  Provide a "condition"
     * fragment that specifies the comparison to perform.
     *
     * For: if-eq, if-ne, if-lt, if-ge, if-gt, if-le
     */
    /* if-cmp vA, vB, +CCCC */
    mov     r1, rINST, lsr #12          @ r1<- B
    ubfx    r0, rINST, #8, #4           @ r0<- A
    GET_VREG r3, r1                     @ r3<- vB
    GET_VREG r0, r0                     @ r0<- vA
    FETCH_S rINST, 1                    @ rINST<- branch offset, in code units
    cmp     r0, r3                      @ compare (vA, vB)
    blt 1f
    FETCH_ADVANCE_INST 2
    GET_INST_OPCODE ip                  // extract opcode from rINST
    GOTO_OPCODE ip                      // jump to next instruction
1:
    FETCH_S rINST, 1                    // rINST<- branch offset, in code units
    BRANCH

    NAME_END nterp_op_if_lt

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_if_ge: /* 0x35 */
    NAME_START nterp_op_if_ge

    /*
     * Generic two-operand compare-and-branch operation.  Provide a "condition"
     * fragment that specifies the comparison to perform.
     *
     * For: if-eq, if-ne, if-lt, if-ge, if-gt, if-le
     */
    /* if-cmp vA, vB, +CCCC */
    mov     r1, rINST, lsr #12          @ r1<- B
    ubfx    r0, rINST, #8, #4           @ r0<- A
    GET_VREG r3, r1                     @ r3<- vB
    GET_VREG r0, r0                     @ r0<- vA
    FETCH_S rINST, 1                    @ rINST<- branch offset, in code units
    cmp     r0, r3                      @ compare (vA, vB)
    bge 1f
    FETCH_ADVANCE_INST 2
    GET_INST_OPCODE ip                  // extract opcode from rINST
    GOTO_OPCODE ip                      // jump to next instruction
1:
    FETCH_S rINST, 1                    // rINST<- branch offset, in code units
    BRANCH

    NAME_END nterp_op_if_ge

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_if_gt: /* 0x36 */
    NAME_START nterp_op_if_gt

    /*
     * Generic two-operand compare-and-branch operation.  Provide a "condition"
     * fragment that specifies the comparison to perform.
     *
     * For: if-eq, if-ne, if-lt, if-ge, if-gt, if-le
     */
    /* if-cmp vA, vB, +CCCC */
    mov     r1, rINST, lsr #12          @ r1<- B
    ubfx    r0, rINST, #8, #4           @ r0<- A
    GET_VREG r3, r1                     @ r3<- vB
    GET_VREG r0, r0                     @ r0<- vA
    FETCH_S rINST, 1                    @ rINST<- branch offset, in code units
    cmp     r0, r3                      @ compare (vA, vB)
    bgt 1f
    FETCH_ADVANCE_INST 2
    GET_INST_OPCODE ip                  // extract opcode from rINST
    GOTO_OPCODE ip                      // jump to next instruction
1:
    FETCH_S rINST, 1                    // rINST<- branch offset, in code units
    BRANCH

    NAME_END nterp_op_if_gt

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_if_le: /* 0x37 */
    NAME_START nterp_op_if_le

    /*
     * Generic two-operand compare-and-branch operation.  Provide a "condition"
     * fragment that specifies the comparison to perform.
     *
     * For: if-eq, if-ne, if-lt, if-ge, if-gt, if-le
     */
    /* if-cmp vA, vB, +CCCC */
    mov     r1, rINST, lsr #12          @ r1<- B
    ubfx    r0, rINST, #8, #4           @ r0<- A
    GET_VREG r3, r1                     @ r3<- vB
    GET_VREG r0, r0                     @ r0<- vA
    FETCH_S rINST, 1                    @ rINST<- branch offset, in code units
    cmp     r0, r3                      @ compare (vA, vB)
    ble 1f
    FETCH_ADVANCE_INST 2
    GET_INST_OPCODE ip                  // extract opcode from rINST
    GOTO_OPCODE ip                      // jump to next instruction
1:
    FETCH_S rINST, 1                    // rINST<- branch offset, in code units
    BRANCH

    NAME_END nterp_op_if_le

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_if_eqz: /* 0x38 */
    NAME_START nterp_op_if_eqz

    /*
     * Generic one-operand compare-and-branch operation.  Provide a "condition"
     * fragment that specifies the comparison to perform.
     *
     * for: if-eqz, if-nez, if-ltz, if-gez, if-gtz, if-lez
     */
    /* if-cmp vAA, +BBBB */
    mov     r0, rINST, lsr #8           @ r0<- AA
    GET_VREG r0, r0                     @ r0<- vAA
    FETCH_S rINST, 1                    @ rINST<- branch offset, in code units
    cmp     r0, #0                      // compare (vA, 0)
    beq 1f
    FETCH_ADVANCE_INST 2
    GET_INST_OPCODE ip                  // extract opcode from rINST
    GOTO_OPCODE ip                      // jump to next instruction
1:
    FETCH_S rINST, 1                    // rINST<- branch offset, in code units
    BRANCH

    NAME_END nterp_op_if_eqz

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_if_nez: /* 0x39 */
    NAME_START nterp_op_if_nez

    /*
     * Generic one-operand compare-and-branch operation.  Provide a "condition"
     * fragment that specifies the comparison to perform.
     *
     * for: if-eqz, if-nez, if-ltz, if-gez, if-gtz, if-lez
     */
    /* if-cmp vAA, +BBBB */
    mov     r0, rINST, lsr #8           @ r0<- AA
    GET_VREG r0, r0                     @ r0<- vAA
    FETCH_S rINST, 1                    @ rINST<- branch offset, in code units
    cmp     r0, #0                      // compare (vA, 0)
    bne 1f
    FETCH_ADVANCE_INST 2
    GET_INST_OPCODE ip                  // extract opcode from rINST
    GOTO_OPCODE ip                      // jump to next instruction
1:
    FETCH_S rINST, 1                    // rINST<- branch offset, in code units
    BRANCH

    NAME_END nterp_op_if_nez

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_if_ltz: /* 0x3a */
    NAME_START nterp_op_if_ltz

    /*
     * Generic one-operand compare-and-branch operation.  Provide a "condition"
     * fragment that specifies the comparison to perform.
     *
     * for: if-eqz, if-nez, if-ltz, if-gez, if-gtz, if-lez
     */
    /* if-cmp vAA, +BBBB */
    mov     r0, rINST, lsr #8           @ r0<- AA
    GET_VREG r0, r0                     @ r0<- vAA
    FETCH_S rINST, 1                    @ rINST<- branch offset, in code units
    cmp     r0, #0                      // compare (vA, 0)
    blt 1f
    FETCH_ADVANCE_INST 2
    GET_INST_OPCODE ip                  // extract opcode from rINST
    GOTO_OPCODE ip                      // jump to next instruction
1:
    FETCH_S rINST, 1                    // rINST<- branch offset, in code units
    BRANCH

    NAME_END nterp_op_if_ltz

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_if_gez: /* 0x3b */
    NAME_START nterp_op_if_gez

    /*
     * Generic one-operand compare-and-branch operation.  Provide a "condition"
     * fragment that specifies the comparison to perform.
     *
     * for: if-eqz, if-nez, if-ltz, if-gez, if-gtz, if-lez
     */
    /* if-cmp vAA, +BBBB */
    mov     r0, rINST, lsr #8           @ r0<- AA
    GET_VREG r0, r0                     @ r0<- vAA
    FETCH_S rINST, 1                    @ rINST<- branch offset, in code units
    cmp     r0, #0                      // compare (vA, 0)
    bge 1f
    FETCH_ADVANCE_INST 2
    GET_INST_OPCODE ip                  // extract opcode from rINST
    GOTO_OPCODE ip                      // jump to next instruction
1:
    FETCH_S rINST, 1                    // rINST<- branch offset, in code units
    BRANCH

    NAME_END nterp_op_if_gez

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_if_gtz: /* 0x3c */
    NAME_START nterp_op_if_gtz

    /*
     * Generic one-operand compare-and-branch operation.  Provide a "condition"
     * fragment that specifies the comparison to perform.
     *
     * for: if-eqz, if-nez, if-ltz, if-gez, if-gtz, if-lez
     */
    /* if-cmp vAA, +BBBB */
    mov     r0, rINST, lsr #8           @ r0<- AA
    GET_VREG r0, r0                     @ r0<- vAA
    FETCH_S rINST, 1                    @ rINST<- branch offset, in code units
    cmp     r0, #0                      // compare (vA, 0)
    bgt 1f
    FETCH_ADVANCE_INST 2
    GET_INST_OPCODE ip                  // extract opcode from rINST
    GOTO_OPCODE ip                      // jump to next instruction
1:
    FETCH_S rINST, 1                    // rINST<- branch offset, in code units
    BRANCH

    NAME_END nterp_op_if_gtz

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_if_lez: /* 0x3d */
    NAME_START nterp_op_if_lez

    /*
     * Generic one-operand compare-and-branch operation.  Provide a "condition"
     * fragment that specifies the comparison to perform.
     *
     * for: if-eqz, if-nez, if-ltz, if-gez, if-gtz, if-lez
     */
    /* if-cmp vAA, +BBBB */
    mov     r0, rINST, lsr #8           @ r0<- AA
    GET_VREG r0, r0                     @ r0<- vAA
    FETCH_S rINST, 1                    @ rINST<- branch offset, in code units
    cmp     r0, #0                      // compare (vA, 0)
    ble 1f
    FETCH_ADVANCE_INST 2
    GET_INST_OPCODE ip                  // extract opcode from rINST
    GOTO_OPCODE ip                      // jump to next instruction
1:
    FETCH_S rINST, 1                    // rINST<- branch offset, in code units
    BRANCH

    NAME_END nterp_op_if_lez

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_3e: /* 0x3e */
    NAME_START nterp_op_unused_3e

    bkpt

    NAME_END nterp_op_unused_3e

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_3f: /* 0x3f */
    NAME_START nterp_op_unused_3f

    bkpt

    NAME_END nterp_op_unused_3f

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_40: /* 0x40 */
    NAME_START nterp_op_unused_40

    bkpt

    NAME_END nterp_op_unused_40

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_41: /* 0x41 */
    NAME_START nterp_op_unused_41

    bkpt

    NAME_END nterp_op_unused_41

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_42: /* 0x42 */
    NAME_START nterp_op_unused_42

    bkpt

    NAME_END nterp_op_unused_42

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_43: /* 0x43 */
    NAME_START nterp_op_unused_43

    bkpt

    NAME_END nterp_op_unused_43

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aget: /* 0x44 */
    NAME_START nterp_op_aget

/*
 * Array get.  vAA <- vBB[vCC].
 *
 * for: aget, aget-boolean, aget-byte, aget-char, aget-short, aget-wide, aget-object
 *
 */
    FETCH_B r2, 1, 0                    @ r2<- BB
    mov     r4, rINST, lsr #8           @ r4<- AA
    FETCH_B r3, 1, 1                    @ r3<- CC
    GET_VREG r0, r2                     @ r0<- vBB (array object)
    GET_VREG r1, r3                     @ r1<- vCC (requested index)
    cmp     r0, #0                      @ null array object?
    beq     common_errNullObject        @ yes, bail
    ldr     r3, [r0, #MIRROR_ARRAY_LENGTH_OFFSET]    @ r3<- arrayObj->length
    add     r0, r0, r1, lsl #2     @ r0<- arrayObj + index*width
    cmp     r1, r3                      @ compare unsigned index, length
    bcs     common_errArrayIndex        @ index >= length, bail
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    .if 0
    CLEAR_SHADOW_PAIR r4, lr, ip        @ Zero out the shadow regs
    ldrd    r2, [r0, #MIRROR_INT_ARRAY_DATA_OFFSET]     @ r2/r3<- vBB[vCC]
    VREG_INDEX_TO_ADDR r4, r4           @ r4<- &fp[AA]
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_WIDE_BY_ADDR r2, r3, r4    @ vAA/vAA+1<- r2/r3
    GOTO_OPCODE ip                      @ jump to next instruction
    .elseif 0
    ldr   r2, [r0, #MIRROR_INT_ARRAY_DATA_OFFSET]     @ w2<- vBB[vCC]
    cmp rMR, #0
    bne 2f
1:
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_OBJECT r2, r4              @ vAA<- w2
    GOTO_OPCODE ip                      @ jump to next instruction
2:
    bl art_quick_read_barrier_mark_reg02
    b 1b
    .else
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    ldr   r2, [r0, #MIRROR_INT_ARRAY_DATA_OFFSET]     @ r2<- vBB[vCC]
    SET_VREG r2, r4                     @ vAA<- r2
    GOTO_OPCODE ip                      @ jump to next instruction
    .endif

    NAME_END nterp_op_aget

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aget_wide: /* 0x45 */
    NAME_START nterp_op_aget_wide

/*
 * Array get.  vAA <- vBB[vCC].
 *
 * for: aget, aget-boolean, aget-byte, aget-char, aget-short, aget-wide, aget-object
 *
 */
    FETCH_B r2, 1, 0                    @ r2<- BB
    mov     r4, rINST, lsr #8           @ r4<- AA
    FETCH_B r3, 1, 1                    @ r3<- CC
    GET_VREG r0, r2                     @ r0<- vBB (array object)
    GET_VREG r1, r3                     @ r1<- vCC (requested index)
    cmp     r0, #0                      @ null array object?
    beq     common_errNullObject        @ yes, bail
    ldr     r3, [r0, #MIRROR_ARRAY_LENGTH_OFFSET]    @ r3<- arrayObj->length
    add     r0, r0, r1, lsl #3     @ r0<- arrayObj + index*width
    cmp     r1, r3                      @ compare unsigned index, length
    bcs     common_errArrayIndex        @ index >= length, bail
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    .if 1
    CLEAR_SHADOW_PAIR r4, lr, ip        @ Zero out the shadow regs
    ldrd    r2, [r0, #MIRROR_WIDE_ARRAY_DATA_OFFSET]     @ r2/r3<- vBB[vCC]
    VREG_INDEX_TO_ADDR r4, r4           @ r4<- &fp[AA]
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_WIDE_BY_ADDR r2, r3, r4    @ vAA/vAA+1<- r2/r3
    GOTO_OPCODE ip                      @ jump to next instruction
    .elseif 0
    ldrd   r2, [r0, #MIRROR_WIDE_ARRAY_DATA_OFFSET]     @ w2<- vBB[vCC]
    cmp rMR, #0
    bne 2f
1:
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_OBJECT r2, r4              @ vAA<- w2
    GOTO_OPCODE ip                      @ jump to next instruction
2:
    bl art_quick_read_barrier_mark_reg02
    b 1b
    .else
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    ldrd   r2, [r0, #MIRROR_WIDE_ARRAY_DATA_OFFSET]     @ r2<- vBB[vCC]
    SET_VREG r2, r4                     @ vAA<- r2
    GOTO_OPCODE ip                      @ jump to next instruction
    .endif

    NAME_END nterp_op_aget_wide

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aget_object: /* 0x46 */
    NAME_START nterp_op_aget_object

/*
 * Array get.  vAA <- vBB[vCC].
 *
 * for: aget, aget-boolean, aget-byte, aget-char, aget-short, aget-wide, aget-object
 *
 */
    FETCH_B r2, 1, 0                    @ r2<- BB
    mov     r4, rINST, lsr #8           @ r4<- AA
    FETCH_B r3, 1, 1                    @ r3<- CC
    GET_VREG r0, r2                     @ r0<- vBB (array object)
    GET_VREG r1, r3                     @ r1<- vCC (requested index)
    cmp     r0, #0                      @ null array object?
    beq     common_errNullObject        @ yes, bail
    ldr     r3, [r0, #MIRROR_ARRAY_LENGTH_OFFSET]    @ r3<- arrayObj->length
    add     r0, r0, r1, lsl #2     @ r0<- arrayObj + index*width
    cmp     r1, r3                      @ compare unsigned index, length
    bcs     common_errArrayIndex        @ index >= length, bail
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    .if 0
    CLEAR_SHADOW_PAIR r4, lr, ip        @ Zero out the shadow regs
    ldrd    r2, [r0, #MIRROR_OBJECT_ARRAY_DATA_OFFSET]     @ r2/r3<- vBB[vCC]
    VREG_INDEX_TO_ADDR r4, r4           @ r4<- &fp[AA]
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_WIDE_BY_ADDR r2, r3, r4    @ vAA/vAA+1<- r2/r3
    GOTO_OPCODE ip                      @ jump to next instruction
    .elseif 1
    ldr   r2, [r0, #MIRROR_OBJECT_ARRAY_DATA_OFFSET]     @ w2<- vBB[vCC]
    cmp rMR, #0
    bne 2f
1:
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_OBJECT r2, r4              @ vAA<- w2
    GOTO_OPCODE ip                      @ jump to next instruction
2:
    bl art_quick_read_barrier_mark_reg02
    b 1b
    .else
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    ldr   r2, [r0, #MIRROR_OBJECT_ARRAY_DATA_OFFSET]     @ r2<- vBB[vCC]
    SET_VREG r2, r4                     @ vAA<- r2
    GOTO_OPCODE ip                      @ jump to next instruction
    .endif

    NAME_END nterp_op_aget_object

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aget_boolean: /* 0x47 */
    NAME_START nterp_op_aget_boolean

/*
 * Array get.  vAA <- vBB[vCC].
 *
 * for: aget, aget-boolean, aget-byte, aget-char, aget-short, aget-wide, aget-object
 *
 */
    FETCH_B r2, 1, 0                    @ r2<- BB
    mov     r4, rINST, lsr #8           @ r4<- AA
    FETCH_B r3, 1, 1                    @ r3<- CC
    GET_VREG r0, r2                     @ r0<- vBB (array object)
    GET_VREG r1, r3                     @ r1<- vCC (requested index)
    cmp     r0, #0                      @ null array object?
    beq     common_errNullObject        @ yes, bail
    ldr     r3, [r0, #MIRROR_ARRAY_LENGTH_OFFSET]    @ r3<- arrayObj->length
    add     r0, r0, r1, lsl #0     @ r0<- arrayObj + index*width
    cmp     r1, r3                      @ compare unsigned index, length
    bcs     common_errArrayIndex        @ index >= length, bail
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    .if 0
    CLEAR_SHADOW_PAIR r4, lr, ip        @ Zero out the shadow regs
    ldrd    r2, [r0, #MIRROR_BOOLEAN_ARRAY_DATA_OFFSET]     @ r2/r3<- vBB[vCC]
    VREG_INDEX_TO_ADDR r4, r4           @ r4<- &fp[AA]
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_WIDE_BY_ADDR r2, r3, r4    @ vAA/vAA+1<- r2/r3
    GOTO_OPCODE ip                      @ jump to next instruction
    .elseif 0
    ldrb   r2, [r0, #MIRROR_BOOLEAN_ARRAY_DATA_OFFSET]     @ w2<- vBB[vCC]
    cmp rMR, #0
    bne 2f
1:
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_OBJECT r2, r4              @ vAA<- w2
    GOTO_OPCODE ip                      @ jump to next instruction
2:
    bl art_quick_read_barrier_mark_reg02
    b 1b
    .else
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    ldrb   r2, [r0, #MIRROR_BOOLEAN_ARRAY_DATA_OFFSET]     @ r2<- vBB[vCC]
    SET_VREG r2, r4                     @ vAA<- r2
    GOTO_OPCODE ip                      @ jump to next instruction
    .endif

    NAME_END nterp_op_aget_boolean

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aget_byte: /* 0x48 */
    NAME_START nterp_op_aget_byte

/*
 * Array get.  vAA <- vBB[vCC].
 *
 * for: aget, aget-boolean, aget-byte, aget-char, aget-short, aget-wide, aget-object
 *
 */
    FETCH_B r2, 1, 0                    @ r2<- BB
    mov     r4, rINST, lsr #8           @ r4<- AA
    FETCH_B r3, 1, 1                    @ r3<- CC
    GET_VREG r0, r2                     @ r0<- vBB (array object)
    GET_VREG r1, r3                     @ r1<- vCC (requested index)
    cmp     r0, #0                      @ null array object?
    beq     common_errNullObject        @ yes, bail
    ldr     r3, [r0, #MIRROR_ARRAY_LENGTH_OFFSET]    @ r3<- arrayObj->length
    add     r0, r0, r1, lsl #0     @ r0<- arrayObj + index*width
    cmp     r1, r3                      @ compare unsigned index, length
    bcs     common_errArrayIndex        @ index >= length, bail
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    .if 0
    CLEAR_SHADOW_PAIR r4, lr, ip        @ Zero out the shadow regs
    ldrd    r2, [r0, #MIRROR_BYTE_ARRAY_DATA_OFFSET]     @ r2/r3<- vBB[vCC]
    VREG_INDEX_TO_ADDR r4, r4           @ r4<- &fp[AA]
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_WIDE_BY_ADDR r2, r3, r4    @ vAA/vAA+1<- r2/r3
    GOTO_OPCODE ip                      @ jump to next instruction
    .elseif 0
    ldrsb   r2, [r0, #MIRROR_BYTE_ARRAY_DATA_OFFSET]     @ w2<- vBB[vCC]
    cmp rMR, #0
    bne 2f
1:
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_OBJECT r2, r4              @ vAA<- w2
    GOTO_OPCODE ip                      @ jump to next instruction
2:
    bl art_quick_read_barrier_mark_reg02
    b 1b
    .else
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    ldrsb   r2, [r0, #MIRROR_BYTE_ARRAY_DATA_OFFSET]     @ r2<- vBB[vCC]
    SET_VREG r2, r4                     @ vAA<- r2
    GOTO_OPCODE ip                      @ jump to next instruction
    .endif

    NAME_END nterp_op_aget_byte

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aget_char: /* 0x49 */
    NAME_START nterp_op_aget_char

/*
 * Array get.  vAA <- vBB[vCC].
 *
 * for: aget, aget-boolean, aget-byte, aget-char, aget-short, aget-wide, aget-object
 *
 */
    FETCH_B r2, 1, 0                    @ r2<- BB
    mov     r4, rINST, lsr #8           @ r4<- AA
    FETCH_B r3, 1, 1                    @ r3<- CC
    GET_VREG r0, r2                     @ r0<- vBB (array object)
    GET_VREG r1, r3                     @ r1<- vCC (requested index)
    cmp     r0, #0                      @ null array object?
    beq     common_errNullObject        @ yes, bail
    ldr     r3, [r0, #MIRROR_ARRAY_LENGTH_OFFSET]    @ r3<- arrayObj->length
    add     r0, r0, r1, lsl #1     @ r0<- arrayObj + index*width
    cmp     r1, r3                      @ compare unsigned index, length
    bcs     common_errArrayIndex        @ index >= length, bail
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    .if 0
    CLEAR_SHADOW_PAIR r4, lr, ip        @ Zero out the shadow regs
    ldrd    r2, [r0, #MIRROR_CHAR_ARRAY_DATA_OFFSET]     @ r2/r3<- vBB[vCC]
    VREG_INDEX_TO_ADDR r4, r4           @ r4<- &fp[AA]
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_WIDE_BY_ADDR r2, r3, r4    @ vAA/vAA+1<- r2/r3
    GOTO_OPCODE ip                      @ jump to next instruction
    .elseif 0
    ldrh   r2, [r0, #MIRROR_CHAR_ARRAY_DATA_OFFSET]     @ w2<- vBB[vCC]
    cmp rMR, #0
    bne 2f
1:
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_OBJECT r2, r4              @ vAA<- w2
    GOTO_OPCODE ip                      @ jump to next instruction
2:
    bl art_quick_read_barrier_mark_reg02
    b 1b
    .else
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    ldrh   r2, [r0, #MIRROR_CHAR_ARRAY_DATA_OFFSET]     @ r2<- vBB[vCC]
    SET_VREG r2, r4                     @ vAA<- r2
    GOTO_OPCODE ip                      @ jump to next instruction
    .endif

    NAME_END nterp_op_aget_char

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aget_short: /* 0x4a */
    NAME_START nterp_op_aget_short

/*
 * Array get.  vAA <- vBB[vCC].
 *
 * for: aget, aget-boolean, aget-byte, aget-char, aget-short, aget-wide, aget-object
 *
 */
    FETCH_B r2, 1, 0                    @ r2<- BB
    mov     r4, rINST, lsr #8           @ r4<- AA
    FETCH_B r3, 1, 1                    @ r3<- CC
    GET_VREG r0, r2                     @ r0<- vBB (array object)
    GET_VREG r1, r3                     @ r1<- vCC (requested index)
    cmp     r0, #0                      @ null array object?
    beq     common_errNullObject        @ yes, bail
    ldr     r3, [r0, #MIRROR_ARRAY_LENGTH_OFFSET]    @ r3<- arrayObj->length
    add     r0, r0, r1, lsl #1     @ r0<- arrayObj + index*width
    cmp     r1, r3                      @ compare unsigned index, length
    bcs     common_errArrayIndex        @ index >= length, bail
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    .if 0
    CLEAR_SHADOW_PAIR r4, lr, ip        @ Zero out the shadow regs
    ldrd    r2, [r0, #MIRROR_SHORT_ARRAY_DATA_OFFSET]     @ r2/r3<- vBB[vCC]
    VREG_INDEX_TO_ADDR r4, r4           @ r4<- &fp[AA]
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_WIDE_BY_ADDR r2, r3, r4    @ vAA/vAA+1<- r2/r3
    GOTO_OPCODE ip                      @ jump to next instruction
    .elseif 0
    ldrsh   r2, [r0, #MIRROR_SHORT_ARRAY_DATA_OFFSET]     @ w2<- vBB[vCC]
    cmp rMR, #0
    bne 2f
1:
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_OBJECT r2, r4              @ vAA<- w2
    GOTO_OPCODE ip                      @ jump to next instruction
2:
    bl art_quick_read_barrier_mark_reg02
    b 1b
    .else
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    ldrsh   r2, [r0, #MIRROR_SHORT_ARRAY_DATA_OFFSET]     @ r2<- vBB[vCC]
    SET_VREG r2, r4                     @ vAA<- r2
    GOTO_OPCODE ip                      @ jump to next instruction
    .endif

    NAME_END nterp_op_aget_short

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aput: /* 0x4b */
    NAME_START nterp_op_aput

/*
 * Array put.  vBB[vCC] <- vAA.
 *
 * for: aput, aput-boolean, aput-byte, aput-char, aput-short, aput-wide, aput-object
 *
 */
    FETCH_B r2, 1, 0                    @ r2<- BB
    mov     r4, rINST, lsr #8           @ r4<- AA
    FETCH_B r3, 1, 1                    @ r3<- CC
    GET_VREG r0, r2                     @ r0<- vBB (array object)
    GET_VREG r1, r3                     @ r1<- vCC (requested index)
    cmp     r0, #0                      @ null array object?
    beq     common_errNullObject        @ yes, bail
    ldr     r3, [r0, #MIRROR_ARRAY_LENGTH_OFFSET]     @ r3<- arrayObj->length
    cmp     r1, r3                      @ compare unsigned index, length
    bcs     common_errArrayIndex        @ index >= length, bail
    .if 0
    EXPORT_PC                           // Export PC before overwriting it.
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    GET_VREG r2, r4                     @ r2<- vAA
    bl art_quick_aput_obj
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    .elseif 0
    add     r0, r0, r1, lsl #2     @ r0<- arrayObj + index*width
    VREG_INDEX_TO_ADDR r4, r4           @ r4<- &fp[AA]
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    GET_VREG_WIDE_BY_ADDR r2, r3, r4    @ r2/r3<- vAA/vAA+1
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    strd    r2, [r0, #MIRROR_INT_ARRAY_DATA_OFFSET]     @ r2/r3<- vBB[vCC]
    .else
    add     r0, r0, r1, lsl #2     @ r0<- arrayObj + index*width
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    GET_VREG r2, r4                     @ r2<- vAA
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    str  r2, [r0, #MIRROR_INT_ARRAY_DATA_OFFSET]     @ vBB[vCC]<- r2
    .endif
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_aput

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aput_wide: /* 0x4c */
    NAME_START nterp_op_aput_wide

/*
 * Array put.  vBB[vCC] <- vAA.
 *
 * for: aput, aput-boolean, aput-byte, aput-char, aput-short, aput-wide, aput-object
 *
 */
    FETCH_B r2, 1, 0                    @ r2<- BB
    mov     r4, rINST, lsr #8           @ r4<- AA
    FETCH_B r3, 1, 1                    @ r3<- CC
    GET_VREG r0, r2                     @ r0<- vBB (array object)
    GET_VREG r1, r3                     @ r1<- vCC (requested index)
    cmp     r0, #0                      @ null array object?
    beq     common_errNullObject        @ yes, bail
    ldr     r3, [r0, #MIRROR_ARRAY_LENGTH_OFFSET]     @ r3<- arrayObj->length
    cmp     r1, r3                      @ compare unsigned index, length
    bcs     common_errArrayIndex        @ index >= length, bail
    .if 0
    EXPORT_PC                           // Export PC before overwriting it.
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    GET_VREG r2, r4                     @ r2<- vAA
    bl art_quick_aput_obj
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    .elseif 1
    add     r0, r0, r1, lsl #3     @ r0<- arrayObj + index*width
    VREG_INDEX_TO_ADDR r4, r4           @ r4<- &fp[AA]
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    GET_VREG_WIDE_BY_ADDR r2, r3, r4    @ r2/r3<- vAA/vAA+1
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    strd    r2, [r0, #MIRROR_WIDE_ARRAY_DATA_OFFSET]     @ r2/r3<- vBB[vCC]
    .else
    add     r0, r0, r1, lsl #3     @ r0<- arrayObj + index*width
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    GET_VREG r2, r4                     @ r2<- vAA
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    str  r2, [r0, #MIRROR_WIDE_ARRAY_DATA_OFFSET]     @ vBB[vCC]<- r2
    .endif
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_aput_wide

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aput_object: /* 0x4d */
    NAME_START nterp_op_aput_object

/*
 * Array put.  vBB[vCC] <- vAA.
 *
 * for: aput, aput-boolean, aput-byte, aput-char, aput-short, aput-wide, aput-object
 *
 */
    FETCH_B r2, 1, 0                    @ r2<- BB
    mov     r4, rINST, lsr #8           @ r4<- AA
    FETCH_B r3, 1, 1                    @ r3<- CC
    GET_VREG r0, r2                     @ r0<- vBB (array object)
    GET_VREG r1, r3                     @ r1<- vCC (requested index)
    cmp     r0, #0                      @ null array object?
    beq     common_errNullObject        @ yes, bail
    ldr     r3, [r0, #MIRROR_ARRAY_LENGTH_OFFSET]     @ r3<- arrayObj->length
    cmp     r1, r3                      @ compare unsigned index, length
    bcs     common_errArrayIndex        @ index >= length, bail
    .if 1
    EXPORT_PC                           // Export PC before overwriting it.
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    GET_VREG r2, r4                     @ r2<- vAA
    bl art_quick_aput_obj
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    .elseif 0
    add     r0, r0, r1, lsl #2     @ r0<- arrayObj + index*width
    VREG_INDEX_TO_ADDR r4, r4           @ r4<- &fp[AA]
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    GET_VREG_WIDE_BY_ADDR r2, r3, r4    @ r2/r3<- vAA/vAA+1
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    strd    r2, [r0, #MIRROR_OBJECT_ARRAY_DATA_OFFSET]     @ r2/r3<- vBB[vCC]
    .else
    add     r0, r0, r1, lsl #2     @ r0<- arrayObj + index*width
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    GET_VREG r2, r4                     @ r2<- vAA
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    str  r2, [r0, #MIRROR_OBJECT_ARRAY_DATA_OFFSET]     @ vBB[vCC]<- r2
    .endif
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_aput_object

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aput_boolean: /* 0x4e */
    NAME_START nterp_op_aput_boolean

/*
 * Array put.  vBB[vCC] <- vAA.
 *
 * for: aput, aput-boolean, aput-byte, aput-char, aput-short, aput-wide, aput-object
 *
 */
    FETCH_B r2, 1, 0                    @ r2<- BB
    mov     r4, rINST, lsr #8           @ r4<- AA
    FETCH_B r3, 1, 1                    @ r3<- CC
    GET_VREG r0, r2                     @ r0<- vBB (array object)
    GET_VREG r1, r3                     @ r1<- vCC (requested index)
    cmp     r0, #0                      @ null array object?
    beq     common_errNullObject        @ yes, bail
    ldr     r3, [r0, #MIRROR_ARRAY_LENGTH_OFFSET]     @ r3<- arrayObj->length
    cmp     r1, r3                      @ compare unsigned index, length
    bcs     common_errArrayIndex        @ index >= length, bail
    .if 0
    EXPORT_PC                           // Export PC before overwriting it.
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    GET_VREG r2, r4                     @ r2<- vAA
    bl art_quick_aput_obj
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    .elseif 0
    add     r0, r0, r1, lsl #0     @ r0<- arrayObj + index*width
    VREG_INDEX_TO_ADDR r4, r4           @ r4<- &fp[AA]
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    GET_VREG_WIDE_BY_ADDR r2, r3, r4    @ r2/r3<- vAA/vAA+1
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    strd    r2, [r0, #MIRROR_BOOLEAN_ARRAY_DATA_OFFSET]     @ r2/r3<- vBB[vCC]
    .else
    add     r0, r0, r1, lsl #0     @ r0<- arrayObj + index*width
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    GET_VREG r2, r4                     @ r2<- vAA
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    strb  r2, [r0, #MIRROR_BOOLEAN_ARRAY_DATA_OFFSET]     @ vBB[vCC]<- r2
    .endif
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_aput_boolean

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aput_byte: /* 0x4f */
    NAME_START nterp_op_aput_byte

/*
 * Array put.  vBB[vCC] <- vAA.
 *
 * for: aput, aput-boolean, aput-byte, aput-char, aput-short, aput-wide, aput-object
 *
 */
    FETCH_B r2, 1, 0                    @ r2<- BB
    mov     r4, rINST, lsr #8           @ r4<- AA
    FETCH_B r3, 1, 1                    @ r3<- CC
    GET_VREG r0, r2                     @ r0<- vBB (array object)
    GET_VREG r1, r3                     @ r1<- vCC (requested index)
    cmp     r0, #0                      @ null array object?
    beq     common_errNullObject        @ yes, bail
    ldr     r3, [r0, #MIRROR_ARRAY_LENGTH_OFFSET]     @ r3<- arrayObj->length
    cmp     r1, r3                      @ compare unsigned index, length
    bcs     common_errArrayIndex        @ index >= length, bail
    .if 0
    EXPORT_PC                           // Export PC before overwriting it.
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    GET_VREG r2, r4                     @ r2<- vAA
    bl art_quick_aput_obj
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    .elseif 0
    add     r0, r0, r1, lsl #0     @ r0<- arrayObj + index*width
    VREG_INDEX_TO_ADDR r4, r4           @ r4<- &fp[AA]
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    GET_VREG_WIDE_BY_ADDR r2, r3, r4    @ r2/r3<- vAA/vAA+1
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    strd    r2, [r0, #MIRROR_BYTE_ARRAY_DATA_OFFSET]     @ r2/r3<- vBB[vCC]
    .else
    add     r0, r0, r1, lsl #0     @ r0<- arrayObj + index*width
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    GET_VREG r2, r4                     @ r2<- vAA
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    strb  r2, [r0, #MIRROR_BYTE_ARRAY_DATA_OFFSET]     @ vBB[vCC]<- r2
    .endif
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_aput_byte

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aput_char: /* 0x50 */
    NAME_START nterp_op_aput_char

/*
 * Array put.  vBB[vCC] <- vAA.
 *
 * for: aput, aput-boolean, aput-byte, aput-char, aput-short, aput-wide, aput-object
 *
 */
    FETCH_B r2, 1, 0                    @ r2<- BB
    mov     r4, rINST, lsr #8           @ r4<- AA
    FETCH_B r3, 1, 1                    @ r3<- CC
    GET_VREG r0, r2                     @ r0<- vBB (array object)
    GET_VREG r1, r3                     @ r1<- vCC (requested index)
    cmp     r0, #0                      @ null array object?
    beq     common_errNullObject        @ yes, bail
    ldr     r3, [r0, #MIRROR_ARRAY_LENGTH_OFFSET]     @ r3<- arrayObj->length
    cmp     r1, r3                      @ compare unsigned index, length
    bcs     common_errArrayIndex        @ index >= length, bail
    .if 0
    EXPORT_PC                           // Export PC before overwriting it.
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    GET_VREG r2, r4                     @ r2<- vAA
    bl art_quick_aput_obj
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    .elseif 0
    add     r0, r0, r1, lsl #1     @ r0<- arrayObj + index*width
    VREG_INDEX_TO_ADDR r4, r4           @ r4<- &fp[AA]
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    GET_VREG_WIDE_BY_ADDR r2, r3, r4    @ r2/r3<- vAA/vAA+1
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    strd    r2, [r0, #MIRROR_CHAR_ARRAY_DATA_OFFSET]     @ r2/r3<- vBB[vCC]
    .else
    add     r0, r0, r1, lsl #1     @ r0<- arrayObj + index*width
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    GET_VREG r2, r4                     @ r2<- vAA
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    strh  r2, [r0, #MIRROR_CHAR_ARRAY_DATA_OFFSET]     @ vBB[vCC]<- r2
    .endif
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_aput_char

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aput_short: /* 0x51 */
    NAME_START nterp_op_aput_short

/*
 * Array put.  vBB[vCC] <- vAA.
 *
 * for: aput, aput-boolean, aput-byte, aput-char, aput-short, aput-wide, aput-object
 *
 */
    FETCH_B r2, 1, 0                    @ r2<- BB
    mov     r4, rINST, lsr #8           @ r4<- AA
    FETCH_B r3, 1, 1                    @ r3<- CC
    GET_VREG r0, r2                     @ r0<- vBB (array object)
    GET_VREG r1, r3                     @ r1<- vCC (requested index)
    cmp     r0, #0                      @ null array object?
    beq     common_errNullObject        @ yes, bail
    ldr     r3, [r0, #MIRROR_ARRAY_LENGTH_OFFSET]     @ r3<- arrayObj->length
    cmp     r1, r3                      @ compare unsigned index, length
    bcs     common_errArrayIndex        @ index >= length, bail
    .if 0
    EXPORT_PC                           // Export PC before overwriting it.
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    GET_VREG r2, r4                     @ r2<- vAA
    bl art_quick_aput_obj
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    .elseif 0
    add     r0, r0, r1, lsl #1     @ r0<- arrayObj + index*width
    VREG_INDEX_TO_ADDR r4, r4           @ r4<- &fp[AA]
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    GET_VREG_WIDE_BY_ADDR r2, r3, r4    @ r2/r3<- vAA/vAA+1
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    strd    r2, [r0, #MIRROR_SHORT_ARRAY_DATA_OFFSET]     @ r2/r3<- vBB[vCC]
    .else
    add     r0, r0, r1, lsl #1     @ r0<- arrayObj + index*width
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    GET_VREG r2, r4                     @ r2<- vAA
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    strh  r2, [r0, #MIRROR_SHORT_ARRAY_DATA_OFFSET]     @ vBB[vCC]<- r2
    .endif
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_aput_short

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iget: /* 0x52 */
    NAME_START nterp_op_iget

   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE r0, nterp_op_iget_helper
.Lop_iget_resume:
   lsr     r2, rINST, #12              // r2<- B
   GET_VREG r3, r2                     // r3<- object we're operating on
   ubfx    r2, rINST, #8, #4           // r2<- A
   cmp     r3, #0
   beq     common_errNullObject        // object was null
   .if 0
   add     r3, r3, r0
   ldrd    r0, r1, [r3]
   CLEAR_SHADOW_PAIR r2, ip, lr
   VREG_INDEX_TO_ADDR r2, r2
   SET_VREG_WIDE_BY_ADDR r0, r1, r2    // fp[A] <- value
   .elseif 0
   ldr   r0, [r3, r0]
   cmp     rMR, #0
   bne     .Lop_iget_read_barrier
.Lop_iget_resume_after_read_barrier:
   SET_VREG_OBJECT r0, r2              // fp[A] <- value
   .else
   ldr   r0, [r3, r0]
   SET_VREG r0, r2                     // fp[A] <- value
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
   .if 0
.Lop_iget_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_iget_resume_after_read_barrier
   .endif

    NAME_END nterp_op_iget

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iget_wide: /* 0x53 */
    NAME_START nterp_op_iget_wide

   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE r0, nterp_op_iget_wide_helper
.Lop_iget_wide_resume:
   lsr     r2, rINST, #12              // r2<- B
   GET_VREG r3, r2                     // r3<- object we're operating on
   ubfx    r2, rINST, #8, #4           // r2<- A
   cmp     r3, #0
   beq     common_errNullObject        // object was null
   .if 1
   add     r3, r3, r0
   ldrd    r0, r1, [r3]
   CLEAR_SHADOW_PAIR r2, ip, lr
   VREG_INDEX_TO_ADDR r2, r2
   SET_VREG_WIDE_BY_ADDR r0, r1, r2    // fp[A] <- value
   .elseif 0
   ldr   r0, [r3, r0]
   cmp     rMR, #0
   bne     .Lop_iget_wide_read_barrier
.Lop_iget_wide_resume_after_read_barrier:
   SET_VREG_OBJECT r0, r2              // fp[A] <- value
   .else
   ldr   r0, [r3, r0]
   SET_VREG r0, r2                     // fp[A] <- value
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
   .if 0
.Lop_iget_wide_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_iget_wide_resume_after_read_barrier
   .endif

    NAME_END nterp_op_iget_wide

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iget_object: /* 0x54 */
    NAME_START nterp_op_iget_object

   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE r0, nterp_op_iget_object_helper
.Lop_iget_object_resume:
   lsr     r2, rINST, #12              // r2<- B
   GET_VREG r3, r2                     // r3<- object we're operating on
   ubfx    r2, rINST, #8, #4           // r2<- A
   cmp     r3, #0
   beq     common_errNullObject        // object was null
   .if 0
   add     r3, r3, r0
   ldrd    r0, r1, [r3]
   CLEAR_SHADOW_PAIR r2, ip, lr
   VREG_INDEX_TO_ADDR r2, r2
   SET_VREG_WIDE_BY_ADDR r0, r1, r2    // fp[A] <- value
   .elseif 1
   ldr   r0, [r3, r0]
   cmp     rMR, #0
   bne     .Lop_iget_object_read_barrier
.Lop_iget_object_resume_after_read_barrier:
   SET_VREG_OBJECT r0, r2              // fp[A] <- value
   .else
   ldr   r0, [r3, r0]
   SET_VREG r0, r2                     // fp[A] <- value
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
   .if 1
.Lop_iget_object_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_iget_object_resume_after_read_barrier
   .endif

    NAME_END nterp_op_iget_object

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iget_boolean: /* 0x55 */
    NAME_START nterp_op_iget_boolean

   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE r0, nterp_op_iget_boolean_helper
.Lop_iget_boolean_resume:
   lsr     r2, rINST, #12              // r2<- B
   GET_VREG r3, r2                     // r3<- object we're operating on
   ubfx    r2, rINST, #8, #4           // r2<- A
   cmp     r3, #0
   beq     common_errNullObject        // object was null
   .if 0
   add     r3, r3, r0
   ldrd    r0, r1, [r3]
   CLEAR_SHADOW_PAIR r2, ip, lr
   VREG_INDEX_TO_ADDR r2, r2
   SET_VREG_WIDE_BY_ADDR r0, r1, r2    // fp[A] <- value
   .elseif 0
   ldrb   r0, [r3, r0]
   cmp     rMR, #0
   bne     .Lop_iget_boolean_read_barrier
.Lop_iget_boolean_resume_after_read_barrier:
   SET_VREG_OBJECT r0, r2              // fp[A] <- value
   .else
   ldrb   r0, [r3, r0]
   SET_VREG r0, r2                     // fp[A] <- value
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
   .if 0
.Lop_iget_boolean_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_iget_boolean_resume_after_read_barrier
   .endif

    NAME_END nterp_op_iget_boolean

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iget_byte: /* 0x56 */
    NAME_START nterp_op_iget_byte

   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE r0, nterp_op_iget_byte_helper
.Lop_iget_byte_resume:
   lsr     r2, rINST, #12              // r2<- B
   GET_VREG r3, r2                     // r3<- object we're operating on
   ubfx    r2, rINST, #8, #4           // r2<- A
   cmp     r3, #0
   beq     common_errNullObject        // object was null
   .if 0
   add     r3, r3, r0
   ldrd    r0, r1, [r3]
   CLEAR_SHADOW_PAIR r2, ip, lr
   VREG_INDEX_TO_ADDR r2, r2
   SET_VREG_WIDE_BY_ADDR r0, r1, r2    // fp[A] <- value
   .elseif 0
   ldrsb   r0, [r3, r0]
   cmp     rMR, #0
   bne     .Lop_iget_byte_read_barrier
.Lop_iget_byte_resume_after_read_barrier:
   SET_VREG_OBJECT r0, r2              // fp[A] <- value
   .else
   ldrsb   r0, [r3, r0]
   SET_VREG r0, r2                     // fp[A] <- value
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
   .if 0
.Lop_iget_byte_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_iget_byte_resume_after_read_barrier
   .endif

    NAME_END nterp_op_iget_byte

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iget_char: /* 0x57 */
    NAME_START nterp_op_iget_char

   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE r0, nterp_op_iget_char_helper
.Lop_iget_char_resume:
   lsr     r2, rINST, #12              // r2<- B
   GET_VREG r3, r2                     // r3<- object we're operating on
   ubfx    r2, rINST, #8, #4           // r2<- A
   cmp     r3, #0
   beq     common_errNullObject        // object was null
   .if 0
   add     r3, r3, r0
   ldrd    r0, r1, [r3]
   CLEAR_SHADOW_PAIR r2, ip, lr
   VREG_INDEX_TO_ADDR r2, r2
   SET_VREG_WIDE_BY_ADDR r0, r1, r2    // fp[A] <- value
   .elseif 0
   ldrh   r0, [r3, r0]
   cmp     rMR, #0
   bne     .Lop_iget_char_read_barrier
.Lop_iget_char_resume_after_read_barrier:
   SET_VREG_OBJECT r0, r2              // fp[A] <- value
   .else
   ldrh   r0, [r3, r0]
   SET_VREG r0, r2                     // fp[A] <- value
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
   .if 0
.Lop_iget_char_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_iget_char_resume_after_read_barrier
   .endif

    NAME_END nterp_op_iget_char

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iget_short: /* 0x58 */
    NAME_START nterp_op_iget_short

   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE r0, nterp_op_iget_short_helper
.Lop_iget_short_resume:
   lsr     r2, rINST, #12              // r2<- B
   GET_VREG r3, r2                     // r3<- object we're operating on
   ubfx    r2, rINST, #8, #4           // r2<- A
   cmp     r3, #0
   beq     common_errNullObject        // object was null
   .if 0
   add     r3, r3, r0
   ldrd    r0, r1, [r3]
   CLEAR_SHADOW_PAIR r2, ip, lr
   VREG_INDEX_TO_ADDR r2, r2
   SET_VREG_WIDE_BY_ADDR r0, r1, r2    // fp[A] <- value
   .elseif 0
   ldrsh   r0, [r3, r0]
   cmp     rMR, #0
   bne     .Lop_iget_short_read_barrier
.Lop_iget_short_resume_after_read_barrier:
   SET_VREG_OBJECT r0, r2              // fp[A] <- value
   .else
   ldrsh   r0, [r3, r0]
   SET_VREG r0, r2                     // fp[A] <- value
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
   .if 0
.Lop_iget_short_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_iget_short_resume_after_read_barrier
   .endif

    NAME_END nterp_op_iget_short

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iput: /* 0x59 */
    NAME_START nterp_op_iput

   // Share slow paths for boolean and byte (strb) and slow paths for char and short (strh).
   // It does not matter to which `.Lop_iput_resume` the slow path returns.
   .if !0
   ubfx    r4, rINST, #8, #4           // r4<- A
   GET_VREG r4, r4                     // r4 <- v[A]
   .endif
   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE r0, nterp_op_iput_helper_str00
.Lop_iput_resume:
   lsr     r1, rINST, #12              // r1<- B
   GET_VREG r1, r1                     // vB (object we're operating on)
   cmp     r1, #0
   beq     common_errNullObject
   .if 0
   ubfx    r4, rINST, #8, #4           // r4<- A
   VREG_INDEX_TO_ADDR r4, r4
   GET_VREG_WIDE_BY_ADDR r2, r3, r4      // fp[A] <- value
   add     r1, r1, r0
   strd    r2, r3, [r1]
   .else
   str  r4, [r1, r0]
   WRITE_BARRIER_IF_OBJECT 0, r4, r1, .Lop_iput_skip_write_barrier, r0
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip

    NAME_END nterp_op_iput

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iput_wide: /* 0x5a */
    NAME_START nterp_op_iput_wide

   // Share slow paths for boolean and byte (strb) and slow paths for char and short (strh).
   // It does not matter to which `.Lop_iput_wide_resume` the slow path returns.
   .if !1
   ubfx    r4, rINST, #8, #4           // r4<- A
   GET_VREG r4, r4                     // r4 <- v[A]
   .endif
   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE r0, nterp_op_iput_helper_str10
.Lop_iput_wide_resume:
   lsr     r1, rINST, #12              // r1<- B
   GET_VREG r1, r1                     // vB (object we're operating on)
   cmp     r1, #0
   beq     common_errNullObject
   .if 1
   ubfx    r4, rINST, #8, #4           // r4<- A
   VREG_INDEX_TO_ADDR r4, r4
   GET_VREG_WIDE_BY_ADDR r2, r3, r4      // fp[A] <- value
   add     r1, r1, r0
   strd    r2, r3, [r1]
   .else
   str  r4, [r1, r0]
   WRITE_BARRIER_IF_OBJECT 0, r4, r1, .Lop_iput_wide_skip_write_barrier, r0
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip

    NAME_END nterp_op_iput_wide

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iput_object: /* 0x5b */
    NAME_START nterp_op_iput_object

   // Share slow paths for boolean and byte (strb) and slow paths for char and short (strh).
   // It does not matter to which `.Lop_iput_object_resume` the slow path returns.
   .if !0
   ubfx    r4, rINST, #8, #4           // r4<- A
   GET_VREG r4, r4                     // r4 <- v[A]
   .endif
   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE r0, nterp_op_iput_helper_str01
.Lop_iput_object_resume:
   lsr     r1, rINST, #12              // r1<- B
   GET_VREG r1, r1                     // vB (object we're operating on)
   cmp     r1, #0
   beq     common_errNullObject
   .if 0
   ubfx    r4, rINST, #8, #4           // r4<- A
   VREG_INDEX_TO_ADDR r4, r4
   GET_VREG_WIDE_BY_ADDR r2, r3, r4      // fp[A] <- value
   add     r1, r1, r0
   strd    r2, r3, [r1]
   .else
   str  r4, [r1, r0]
   WRITE_BARRIER_IF_OBJECT 1, r4, r1, .Lop_iput_object_skip_write_barrier, r0
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip

    NAME_END nterp_op_iput_object

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iput_boolean: /* 0x5c */
    NAME_START nterp_op_iput_boolean

   // Share slow paths for boolean and byte (strb) and slow paths for char and short (strh).
   // It does not matter to which `.Lop_iput_boolean_resume` the slow path returns.
   .if !0
   ubfx    r4, rINST, #8, #4           // r4<- A
   GET_VREG r4, r4                     // r4 <- v[A]
   .endif
   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE r0, nterp_op_iput_helper_strb00
.Lop_iput_boolean_resume:
   lsr     r1, rINST, #12              // r1<- B
   GET_VREG r1, r1                     // vB (object we're operating on)
   cmp     r1, #0
   beq     common_errNullObject
   .if 0
   ubfx    r4, rINST, #8, #4           // r4<- A
   VREG_INDEX_TO_ADDR r4, r4
   GET_VREG_WIDE_BY_ADDR r2, r3, r4      // fp[A] <- value
   add     r1, r1, r0
   strd    r2, r3, [r1]
   .else
   strb  r4, [r1, r0]
   WRITE_BARRIER_IF_OBJECT 0, r4, r1, .Lop_iput_boolean_skip_write_barrier, r0
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip

    NAME_END nterp_op_iput_boolean

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iput_byte: /* 0x5d */
    NAME_START nterp_op_iput_byte

   // Share slow paths for boolean and byte (strb) and slow paths for char and short (strh).
   // It does not matter to which `.Lop_iput_byte_resume` the slow path returns.
   .if !0
   ubfx    r4, rINST, #8, #4           // r4<- A
   GET_VREG r4, r4                     // r4 <- v[A]
   .endif
   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE r0, nterp_op_iput_helper_strb00
.Lop_iput_byte_resume:
   lsr     r1, rINST, #12              // r1<- B
   GET_VREG r1, r1                     // vB (object we're operating on)
   cmp     r1, #0
   beq     common_errNullObject
   .if 0
   ubfx    r4, rINST, #8, #4           // r4<- A
   VREG_INDEX_TO_ADDR r4, r4
   GET_VREG_WIDE_BY_ADDR r2, r3, r4      // fp[A] <- value
   add     r1, r1, r0
   strd    r2, r3, [r1]
   .else
   strb  r4, [r1, r0]
   WRITE_BARRIER_IF_OBJECT 0, r4, r1, .Lop_iput_byte_skip_write_barrier, r0
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip

    NAME_END nterp_op_iput_byte

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iput_char: /* 0x5e */
    NAME_START nterp_op_iput_char

   // Share slow paths for boolean and byte (strb) and slow paths for char and short (strh).
   // It does not matter to which `.Lop_iput_char_resume` the slow path returns.
   .if !0
   ubfx    r4, rINST, #8, #4           // r4<- A
   GET_VREG r4, r4                     // r4 <- v[A]
   .endif
   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE r0, nterp_op_iput_helper_strh00
.Lop_iput_char_resume:
   lsr     r1, rINST, #12              // r1<- B
   GET_VREG r1, r1                     // vB (object we're operating on)
   cmp     r1, #0
   beq     common_errNullObject
   .if 0
   ubfx    r4, rINST, #8, #4           // r4<- A
   VREG_INDEX_TO_ADDR r4, r4
   GET_VREG_WIDE_BY_ADDR r2, r3, r4      // fp[A] <- value
   add     r1, r1, r0
   strd    r2, r3, [r1]
   .else
   strh  r4, [r1, r0]
   WRITE_BARRIER_IF_OBJECT 0, r4, r1, .Lop_iput_char_skip_write_barrier, r0
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip

    NAME_END nterp_op_iput_char

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iput_short: /* 0x5f */
    NAME_START nterp_op_iput_short

   // Share slow paths for boolean and byte (strb) and slow paths for char and short (strh).
   // It does not matter to which `.Lop_iput_short_resume` the slow path returns.
   .if !0
   ubfx    r4, rINST, #8, #4           // r4<- A
   GET_VREG r4, r4                     // r4 <- v[A]
   .endif
   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE r0, nterp_op_iput_helper_strh00
.Lop_iput_short_resume:
   lsr     r1, rINST, #12              // r1<- B
   GET_VREG r1, r1                     // vB (object we're operating on)
   cmp     r1, #0
   beq     common_errNullObject
   .if 0
   ubfx    r4, rINST, #8, #4           // r4<- A
   VREG_INDEX_TO_ADDR r4, r4
   GET_VREG_WIDE_BY_ADDR r2, r3, r4      // fp[A] <- value
   add     r1, r1, r0
   strd    r2, r3, [r1]
   .else
   strh  r4, [r1, r0]
   WRITE_BARRIER_IF_OBJECT 0, r4, r1, .Lop_iput_short_skip_write_barrier, r0
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip

    NAME_END nterp_op_iput_short

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sget: /* 0x60 */
    NAME_START nterp_op_sget

   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE r0, nterp_op_sget_helper
.Lop_sget_resume:
   ldr     r1, [r0, #ART_FIELD_OFFSET_OFFSET]
   lsr     r2, rINST, #8               // r2 <- A
   ldr     r0, [r0, #ART_FIELD_DECLARING_CLASS_OFFSET]
   cmp     rMR, #0
   bne     .Lop_sget_read_barrier
.Lop_sget_resume_after_read_barrier:
   .if 0
   add     r0, r0, r1
   ldrd    r0, r1, [r0]
   CLEAR_SHADOW_PAIR r2, ip, lr
   VREG_INDEX_TO_ADDR r2, r2
   SET_VREG_WIDE_BY_ADDR r0, r1, r2    // fp[A] <- value
   .elseif 0
   ldr   r0, [r0, r1]
   // No need to check the marking register, we know it's not set here.
.Lop_sget_after_reference_load:
   SET_VREG_OBJECT r0, r2              // fp[A] <- value
   .else
   ldr   r0, [r0, r1]
   SET_VREG r0, r2                     // fp[A] <- value
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.Lop_sget_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   .if 0
   ldr     r0, [r0, r1]
.Lop_sget_mark_after_load:
   // Here, we know the marking register is set.
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_sget_after_reference_load
   .else
   b       .Lop_sget_resume_after_read_barrier
   .endif

    NAME_END nterp_op_sget

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sget_wide: /* 0x61 */
    NAME_START nterp_op_sget_wide

   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE r0, nterp_op_sget_wide_helper
.Lop_sget_wide_resume:
   ldr     r1, [r0, #ART_FIELD_OFFSET_OFFSET]
   lsr     r2, rINST, #8               // r2 <- A
   ldr     r0, [r0, #ART_FIELD_DECLARING_CLASS_OFFSET]
   cmp     rMR, #0
   bne     .Lop_sget_wide_read_barrier
.Lop_sget_wide_resume_after_read_barrier:
   .if 1
   add     r0, r0, r1
   ldrd    r0, r1, [r0]
   CLEAR_SHADOW_PAIR r2, ip, lr
   VREG_INDEX_TO_ADDR r2, r2
   SET_VREG_WIDE_BY_ADDR r0, r1, r2    // fp[A] <- value
   .elseif 0
   ldr   r0, [r0, r1]
   // No need to check the marking register, we know it's not set here.
.Lop_sget_wide_after_reference_load:
   SET_VREG_OBJECT r0, r2              // fp[A] <- value
   .else
   ldr   r0, [r0, r1]
   SET_VREG r0, r2                     // fp[A] <- value
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.Lop_sget_wide_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   .if 0
   ldr     r0, [r0, r1]
.Lop_sget_wide_mark_after_load:
   // Here, we know the marking register is set.
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_sget_wide_after_reference_load
   .else
   b       .Lop_sget_wide_resume_after_read_barrier
   .endif

    NAME_END nterp_op_sget_wide

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sget_object: /* 0x62 */
    NAME_START nterp_op_sget_object

   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE r0, nterp_op_sget_object_helper
.Lop_sget_object_resume:
   ldr     r1, [r0, #ART_FIELD_OFFSET_OFFSET]
   lsr     r2, rINST, #8               // r2 <- A
   ldr     r0, [r0, #ART_FIELD_DECLARING_CLASS_OFFSET]
   cmp     rMR, #0
   bne     .Lop_sget_object_read_barrier
.Lop_sget_object_resume_after_read_barrier:
   .if 0
   add     r0, r0, r1
   ldrd    r0, r1, [r0]
   CLEAR_SHADOW_PAIR r2, ip, lr
   VREG_INDEX_TO_ADDR r2, r2
   SET_VREG_WIDE_BY_ADDR r0, r1, r2    // fp[A] <- value
   .elseif 1
   ldr   r0, [r0, r1]
   // No need to check the marking register, we know it's not set here.
.Lop_sget_object_after_reference_load:
   SET_VREG_OBJECT r0, r2              // fp[A] <- value
   .else
   ldr   r0, [r0, r1]
   SET_VREG r0, r2                     // fp[A] <- value
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.Lop_sget_object_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   .if 1
   ldr     r0, [r0, r1]
.Lop_sget_object_mark_after_load:
   // Here, we know the marking register is set.
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_sget_object_after_reference_load
   .else
   b       .Lop_sget_object_resume_after_read_barrier
   .endif

    NAME_END nterp_op_sget_object

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sget_boolean: /* 0x63 */
    NAME_START nterp_op_sget_boolean

   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE r0, nterp_op_sget_boolean_helper
.Lop_sget_boolean_resume:
   ldr     r1, [r0, #ART_FIELD_OFFSET_OFFSET]
   lsr     r2, rINST, #8               // r2 <- A
   ldr     r0, [r0, #ART_FIELD_DECLARING_CLASS_OFFSET]
   cmp     rMR, #0
   bne     .Lop_sget_boolean_read_barrier
.Lop_sget_boolean_resume_after_read_barrier:
   .if 0
   add     r0, r0, r1
   ldrd    r0, r1, [r0]
   CLEAR_SHADOW_PAIR r2, ip, lr
   VREG_INDEX_TO_ADDR r2, r2
   SET_VREG_WIDE_BY_ADDR r0, r1, r2    // fp[A] <- value
   .elseif 0
   ldrb   r0, [r0, r1]
   // No need to check the marking register, we know it's not set here.
.Lop_sget_boolean_after_reference_load:
   SET_VREG_OBJECT r0, r2              // fp[A] <- value
   .else
   ldrb   r0, [r0, r1]
   SET_VREG r0, r2                     // fp[A] <- value
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.Lop_sget_boolean_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   .if 0
   ldr     r0, [r0, r1]
.Lop_sget_boolean_mark_after_load:
   // Here, we know the marking register is set.
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_sget_boolean_after_reference_load
   .else
   b       .Lop_sget_boolean_resume_after_read_barrier
   .endif

    NAME_END nterp_op_sget_boolean

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sget_byte: /* 0x64 */
    NAME_START nterp_op_sget_byte

   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE r0, nterp_op_sget_byte_helper
.Lop_sget_byte_resume:
   ldr     r1, [r0, #ART_FIELD_OFFSET_OFFSET]
   lsr     r2, rINST, #8               // r2 <- A
   ldr     r0, [r0, #ART_FIELD_DECLARING_CLASS_OFFSET]
   cmp     rMR, #0
   bne     .Lop_sget_byte_read_barrier
.Lop_sget_byte_resume_after_read_barrier:
   .if 0
   add     r0, r0, r1
   ldrd    r0, r1, [r0]
   CLEAR_SHADOW_PAIR r2, ip, lr
   VREG_INDEX_TO_ADDR r2, r2
   SET_VREG_WIDE_BY_ADDR r0, r1, r2    // fp[A] <- value
   .elseif 0
   ldrsb   r0, [r0, r1]
   // No need to check the marking register, we know it's not set here.
.Lop_sget_byte_after_reference_load:
   SET_VREG_OBJECT r0, r2              // fp[A] <- value
   .else
   ldrsb   r0, [r0, r1]
   SET_VREG r0, r2                     // fp[A] <- value
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.Lop_sget_byte_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   .if 0
   ldr     r0, [r0, r1]
.Lop_sget_byte_mark_after_load:
   // Here, we know the marking register is set.
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_sget_byte_after_reference_load
   .else
   b       .Lop_sget_byte_resume_after_read_barrier
   .endif

    NAME_END nterp_op_sget_byte

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sget_char: /* 0x65 */
    NAME_START nterp_op_sget_char

   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE r0, nterp_op_sget_char_helper
.Lop_sget_char_resume:
   ldr     r1, [r0, #ART_FIELD_OFFSET_OFFSET]
   lsr     r2, rINST, #8               // r2 <- A
   ldr     r0, [r0, #ART_FIELD_DECLARING_CLASS_OFFSET]
   cmp     rMR, #0
   bne     .Lop_sget_char_read_barrier
.Lop_sget_char_resume_after_read_barrier:
   .if 0
   add     r0, r0, r1
   ldrd    r0, r1, [r0]
   CLEAR_SHADOW_PAIR r2, ip, lr
   VREG_INDEX_TO_ADDR r2, r2
   SET_VREG_WIDE_BY_ADDR r0, r1, r2    // fp[A] <- value
   .elseif 0
   ldrh   r0, [r0, r1]
   // No need to check the marking register, we know it's not set here.
.Lop_sget_char_after_reference_load:
   SET_VREG_OBJECT r0, r2              // fp[A] <- value
   .else
   ldrh   r0, [r0, r1]
   SET_VREG r0, r2                     // fp[A] <- value
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.Lop_sget_char_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   .if 0
   ldr     r0, [r0, r1]
.Lop_sget_char_mark_after_load:
   // Here, we know the marking register is set.
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_sget_char_after_reference_load
   .else
   b       .Lop_sget_char_resume_after_read_barrier
   .endif

    NAME_END nterp_op_sget_char

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sget_short: /* 0x66 */
    NAME_START nterp_op_sget_short

   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE r0, nterp_op_sget_short_helper
.Lop_sget_short_resume:
   ldr     r1, [r0, #ART_FIELD_OFFSET_OFFSET]
   lsr     r2, rINST, #8               // r2 <- A
   ldr     r0, [r0, #ART_FIELD_DECLARING_CLASS_OFFSET]
   cmp     rMR, #0
   bne     .Lop_sget_short_read_barrier
.Lop_sget_short_resume_after_read_barrier:
   .if 0
   add     r0, r0, r1
   ldrd    r0, r1, [r0]
   CLEAR_SHADOW_PAIR r2, ip, lr
   VREG_INDEX_TO_ADDR r2, r2
   SET_VREG_WIDE_BY_ADDR r0, r1, r2    // fp[A] <- value
   .elseif 0
   ldrsh   r0, [r0, r1]
   // No need to check the marking register, we know it's not set here.
.Lop_sget_short_after_reference_load:
   SET_VREG_OBJECT r0, r2              // fp[A] <- value
   .else
   ldrsh   r0, [r0, r1]
   SET_VREG r0, r2                     // fp[A] <- value
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.Lop_sget_short_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   .if 0
   ldr     r0, [r0, r1]
.Lop_sget_short_mark_after_load:
   // Here, we know the marking register is set.
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_sget_short_after_reference_load
   .else
   b       .Lop_sget_short_resume_after_read_barrier
   .endif

    NAME_END nterp_op_sget_short

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sput: /* 0x67 */
    NAME_START nterp_op_sput

   // Share slow paths for boolean and byte (strb) and slow paths for char and short (strh).
   // It does not matter to which `.Lop_sput_resume` the slow path returns.
   .if !0
   lsr     r4, rINST, #8               // r4 <- A
   GET_VREG r4, r4                     // r4 <- v[A]
   .endif
   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE r0, nterp_op_sput_helper_str00
.Lop_sput_resume:
   ldr     r1, [r0, #ART_FIELD_OFFSET_OFFSET]
   ldr     r0, [r0, #ART_FIELD_DECLARING_CLASS_OFFSET]
   cmp     rMR, #0
   bne     .Lop_sput_read_barrier
.Lop_sput_resume_after_read_barrier:
   .if 0
   lsr     r2, rINST, #8               // r2 <- A
   VREG_INDEX_TO_ADDR r2, r2
   GET_VREG_WIDE_BY_ADDR r2, r3, r2    // fp[A] <- value
   add     r0, r0, r1
   strd    r2, r3, [r0]
   .else
   str  r4, [r0, r1]
   WRITE_BARRIER_IF_OBJECT 0, r4, r0, .Lop_sput_skip_write_barrier, r1
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.Lop_sput_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_sput_resume_after_read_barrier

    NAME_END nterp_op_sput

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sput_wide: /* 0x68 */
    NAME_START nterp_op_sput_wide

   // Share slow paths for boolean and byte (strb) and slow paths for char and short (strh).
   // It does not matter to which `.Lop_sput_wide_resume` the slow path returns.
   .if !1
   lsr     r4, rINST, #8               // r4 <- A
   GET_VREG r4, r4                     // r4 <- v[A]
   .endif
   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE r0, nterp_op_sput_helper_str10
.Lop_sput_wide_resume:
   ldr     r1, [r0, #ART_FIELD_OFFSET_OFFSET]
   ldr     r0, [r0, #ART_FIELD_DECLARING_CLASS_OFFSET]
   cmp     rMR, #0
   bne     .Lop_sput_wide_read_barrier
.Lop_sput_wide_resume_after_read_barrier:
   .if 1
   lsr     r2, rINST, #8               // r2 <- A
   VREG_INDEX_TO_ADDR r2, r2
   GET_VREG_WIDE_BY_ADDR r2, r3, r2    // fp[A] <- value
   add     r0, r0, r1
   strd    r2, r3, [r0]
   .else
   str  r4, [r0, r1]
   WRITE_BARRIER_IF_OBJECT 0, r4, r0, .Lop_sput_wide_skip_write_barrier, r1
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.Lop_sput_wide_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_sput_wide_resume_after_read_barrier

    NAME_END nterp_op_sput_wide

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sput_object: /* 0x69 */
    NAME_START nterp_op_sput_object

   // Share slow paths for boolean and byte (strb) and slow paths for char and short (strh).
   // It does not matter to which `.Lop_sput_object_resume` the slow path returns.
   .if !0
   lsr     r4, rINST, #8               // r4 <- A
   GET_VREG r4, r4                     // r4 <- v[A]
   .endif
   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE r0, nterp_op_sput_helper_str01
.Lop_sput_object_resume:
   ldr     r1, [r0, #ART_FIELD_OFFSET_OFFSET]
   ldr     r0, [r0, #ART_FIELD_DECLARING_CLASS_OFFSET]
   cmp     rMR, #0
   bne     .Lop_sput_object_read_barrier
.Lop_sput_object_resume_after_read_barrier:
   .if 0
   lsr     r2, rINST, #8               // r2 <- A
   VREG_INDEX_TO_ADDR r2, r2
   GET_VREG_WIDE_BY_ADDR r2, r3, r2    // fp[A] <- value
   add     r0, r0, r1
   strd    r2, r3, [r0]
   .else
   str  r4, [r0, r1]
   WRITE_BARRIER_IF_OBJECT 1, r4, r0, .Lop_sput_object_skip_write_barrier, r1
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.Lop_sput_object_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_sput_object_resume_after_read_barrier

    NAME_END nterp_op_sput_object

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sput_boolean: /* 0x6a */
    NAME_START nterp_op_sput_boolean

   // Share slow paths for boolean and byte (strb) and slow paths for char and short (strh).
   // It does not matter to which `.Lop_sput_boolean_resume` the slow path returns.
   .if !0
   lsr     r4, rINST, #8               // r4 <- A
   GET_VREG r4, r4                     // r4 <- v[A]
   .endif
   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE r0, nterp_op_sput_helper_strb00
.Lop_sput_boolean_resume:
   ldr     r1, [r0, #ART_FIELD_OFFSET_OFFSET]
   ldr     r0, [r0, #ART_FIELD_DECLARING_CLASS_OFFSET]
   cmp     rMR, #0
   bne     .Lop_sput_boolean_read_barrier
.Lop_sput_boolean_resume_after_read_barrier:
   .if 0
   lsr     r2, rINST, #8               // r2 <- A
   VREG_INDEX_TO_ADDR r2, r2
   GET_VREG_WIDE_BY_ADDR r2, r3, r2    // fp[A] <- value
   add     r0, r0, r1
   strd    r2, r3, [r0]
   .else
   strb  r4, [r0, r1]
   WRITE_BARRIER_IF_OBJECT 0, r4, r0, .Lop_sput_boolean_skip_write_barrier, r1
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.Lop_sput_boolean_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_sput_boolean_resume_after_read_barrier

    NAME_END nterp_op_sput_boolean

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sput_byte: /* 0x6b */
    NAME_START nterp_op_sput_byte

   // Share slow paths for boolean and byte (strb) and slow paths for char and short (strh).
   // It does not matter to which `.Lop_sput_byte_resume` the slow path returns.
   .if !0
   lsr     r4, rINST, #8               // r4 <- A
   GET_VREG r4, r4                     // r4 <- v[A]
   .endif
   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE r0, nterp_op_sput_helper_strb00
.Lop_sput_byte_resume:
   ldr     r1, [r0, #ART_FIELD_OFFSET_OFFSET]
   ldr     r0, [r0, #ART_FIELD_DECLARING_CLASS_OFFSET]
   cmp     rMR, #0
   bne     .Lop_sput_byte_read_barrier
.Lop_sput_byte_resume_after_read_barrier:
   .if 0
   lsr     r2, rINST, #8               // r2 <- A
   VREG_INDEX_TO_ADDR r2, r2
   GET_VREG_WIDE_BY_ADDR r2, r3, r2    // fp[A] <- value
   add     r0, r0, r1
   strd    r2, r3, [r0]
   .else
   strb  r4, [r0, r1]
   WRITE_BARRIER_IF_OBJECT 0, r4, r0, .Lop_sput_byte_skip_write_barrier, r1
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.Lop_sput_byte_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_sput_byte_resume_after_read_barrier

    NAME_END nterp_op_sput_byte

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sput_char: /* 0x6c */
    NAME_START nterp_op_sput_char

   // Share slow paths for boolean and byte (strb) and slow paths for char and short (strh).
   // It does not matter to which `.Lop_sput_char_resume` the slow path returns.
   .if !0
   lsr     r4, rINST, #8               // r4 <- A
   GET_VREG r4, r4                     // r4 <- v[A]
   .endif
   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE r0, nterp_op_sput_helper_strh00
.Lop_sput_char_resume:
   ldr     r1, [r0, #ART_FIELD_OFFSET_OFFSET]
   ldr     r0, [r0, #ART_FIELD_DECLARING_CLASS_OFFSET]
   cmp     rMR, #0
   bne     .Lop_sput_char_read_barrier
.Lop_sput_char_resume_after_read_barrier:
   .if 0
   lsr     r2, rINST, #8               // r2 <- A
   VREG_INDEX_TO_ADDR r2, r2
   GET_VREG_WIDE_BY_ADDR r2, r3, r2    // fp[A] <- value
   add     r0, r0, r1
   strd    r2, r3, [r0]
   .else
   strh  r4, [r0, r1]
   WRITE_BARRIER_IF_OBJECT 0, r4, r0, .Lop_sput_char_skip_write_barrier, r1
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.Lop_sput_char_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_sput_char_resume_after_read_barrier

    NAME_END nterp_op_sput_char

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sput_short: /* 0x6d */
    NAME_START nterp_op_sput_short

   // Share slow paths for boolean and byte (strb) and slow paths for char and short (strh).
   // It does not matter to which `.Lop_sput_short_resume` the slow path returns.
   .if !0
   lsr     r4, rINST, #8               // r4 <- A
   GET_VREG r4, r4                     // r4 <- v[A]
   .endif
   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE r0, nterp_op_sput_helper_strh00
.Lop_sput_short_resume:
   ldr     r1, [r0, #ART_FIELD_OFFSET_OFFSET]
   ldr     r0, [r0, #ART_FIELD_DECLARING_CLASS_OFFSET]
   cmp     rMR, #0
   bne     .Lop_sput_short_read_barrier
.Lop_sput_short_resume_after_read_barrier:
   .if 0
   lsr     r2, rINST, #8               // r2 <- A
   VREG_INDEX_TO_ADDR r2, r2
   GET_VREG_WIDE_BY_ADDR r2, r3, r2    // fp[A] <- value
   add     r0, r0, r1
   strd    r2, r3, [r0]
   .else
   strh  r4, [r0, r1]
   WRITE_BARRIER_IF_OBJECT 0, r4, r0, .Lop_sput_short_skip_write_barrier, r1
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.Lop_sput_short_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_sput_short_resume_after_read_barrier

    NAME_END nterp_op_sput_short

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_virtual: /* 0x6e */
    NAME_START nterp_op_invoke_virtual

   EXPORT_PC
   // Fast-path which gets the vtable offset from thread-local cache.
   FETCH_FROM_THREAD_CACHE r2, 2f
1:
   FETCH r1, 2
   .if !0
   and r1, r1, #0xf
   .endif
   GET_VREG r1, r1
   // Note: if r1 is null, this will be handled by our SIGSEGV handler.
   ldr r0, [r1, #MIRROR_OBJECT_CLASS_OFFSET]
   add r0, r0, #MIRROR_CLASS_VTABLE_OFFSET_32
   ldr r0, [r0, r2, lsl #2]
   b NterpCommonInvokeInstance
2:
   mov r0, rSELF
   ldr r1, [sp]
   mov r2, rPC
   bl nterp_get_method
   mov r2, r0
   b 1b

    NAME_END nterp_op_invoke_virtual

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_super: /* 0x6f */
    NAME_START nterp_op_invoke_super

   EXPORT_PC
   // Fast-path which gets the method from thread-local cache.
   FETCH_FROM_THREAD_CACHE r0, 2f
1:
   // Load the first argument (the 'this' pointer).
   FETCH r1, 2
   .if !0
   and r1, r1, #0xf
   .endif
   GET_VREG r1, r1
   cmp r1, #0
   beq common_errNullObject    // bail if null
   b NterpCommonInvokeInstance
2:
   mov r0, rSELF
   ldr r1, [sp]
   mov r2, rPC
   bl nterp_get_method
   .if 1
   b 1b
   .else
   tst r0, #1
   beq 1b
   and r0, r0, #-2 // Remove the extra bit that marks it's a String.<init> method.
   .if 0
   b NterpHandleStringInitRange
   .else
   b NterpHandleStringInit
   .endif
   .endif

    NAME_END nterp_op_invoke_super

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_direct: /* 0x70 */
    NAME_START nterp_op_invoke_direct

   EXPORT_PC
   // Fast-path which gets the method from thread-local cache.
   FETCH_FROM_THREAD_CACHE r0, 2f
1:
   // Load the first argument (the 'this' pointer).
   FETCH r1, 2
   .if !0
   and r1, r1, #0xf
   .endif
   GET_VREG r1, r1
   cmp r1, #0
   beq common_errNullObject    // bail if null
   b NterpCommonInvokeInstance
2:
   mov r0, rSELF
   ldr r1, [sp]
   mov r2, rPC
   bl nterp_get_method
   .if 0
   b 1b
   .else
   tst r0, #1
   beq 1b
   and r0, r0, #-2 // Remove the extra bit that marks it's a String.<init> method.
   .if 0
   b NterpHandleStringInitRange
   .else
   b NterpHandleStringInit
   .endif
   .endif

    NAME_END nterp_op_invoke_direct

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_static: /* 0x71 */
    NAME_START nterp_op_invoke_static

   EXPORT_PC
   // Fast-path which gets the method from thread-local cache.
   FETCH_FROM_THREAD_CACHE r0, 1f
   b NterpCommonInvokeStatic
1:
   mov r0, rSELF
   ldr r1, [sp]
   mov r2, rPC
   bl nterp_get_method
   b NterpCommonInvokeStatic

    NAME_END nterp_op_invoke_static

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_interface: /* 0x72 */
    NAME_START nterp_op_invoke_interface

   EXPORT_PC
   // Fast-path which gets the method from thread-local cache.
   FETCH_FROM_THREAD_CACHE r4, nterp_op_invoke_interface_helper
.Lop_invoke_interface_resume:
   // First argument is the 'this' pointer.
   FETCH r1, 2
   .if !0
   and r1, r1, #0xf
   .endif
   GET_VREG r1, r1
   // Note: if r1 is null, this will be handled by our SIGSEGV handler.
   ldr r2, [r1, #MIRROR_OBJECT_CLASS_OFFSET]
   // Test the first two bits of the fetched ArtMethod:
   // - If the first bit is set, this is a method on j.l.Object
   // - If the second bit is set, this is a default method.
   tst r4, #3
   bne 2f
   ldrh r3, [r4, #ART_METHOD_IMT_INDEX_OFFSET]
1:
   ldr r2, [r2, #MIRROR_CLASS_IMT_PTR_OFFSET_32]
   ldr r0, [r2, r3, lsl #2]
   .if 0
   b NterpCommonInvokeInterfaceRange
   .else
   b NterpCommonInvokeInterface
   .endif
2:
   tst r4, #1
   bne 3f
   and r4, r4, #-4
   ldrh r3, [r4, #ART_METHOD_METHOD_INDEX_OFFSET]
   and r3, r3, #ART_METHOD_IMT_MASK
   b 1b
3:
   lsr r4, r4, #16
   add r2, r2, #MIRROR_CLASS_VTABLE_OFFSET_32
   ldr r0, [r2, r4, lsl #2]
   .if 0
   b NterpCommonInvokeInstanceRange
   .else
   b NterpCommonInvokeInstance
   .endif

    NAME_END nterp_op_invoke_interface

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_73: /* 0x73 */
    NAME_START nterp_op_unused_73

    bkpt

    NAME_END nterp_op_unused_73

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_virtual_range: /* 0x74 */
    NAME_START nterp_op_invoke_virtual_range

   EXPORT_PC
   // Fast-path which gets the vtable offset from thread-local cache.
   FETCH_FROM_THREAD_CACHE r2, 2f
1:
   FETCH r1, 2
   .if !1
   and r1, r1, #0xf
   .endif
   GET_VREG r1, r1
   // Note: if r1 is null, this will be handled by our SIGSEGV handler.
   ldr r0, [r1, #MIRROR_OBJECT_CLASS_OFFSET]
   add r0, r0, #MIRROR_CLASS_VTABLE_OFFSET_32
   ldr r0, [r0, r2, lsl #2]
   b NterpCommonInvokeInstanceRange
2:
   mov r0, rSELF
   ldr r1, [sp]
   mov r2, rPC
   bl nterp_get_method
   mov r2, r0
   b 1b

    NAME_END nterp_op_invoke_virtual_range

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_super_range: /* 0x75 */
    NAME_START nterp_op_invoke_super_range

   EXPORT_PC
   // Fast-path which gets the method from thread-local cache.
   FETCH_FROM_THREAD_CACHE r0, 2f
1:
   // Load the first argument (the 'this' pointer).
   FETCH r1, 2
   .if !1
   and r1, r1, #0xf
   .endif
   GET_VREG r1, r1
   cmp r1, #0
   beq common_errNullObject    // bail if null
   b NterpCommonInvokeInstanceRange
2:
   mov r0, rSELF
   ldr r1, [sp]
   mov r2, rPC
   bl nterp_get_method
   .if 1
   b 1b
   .else
   tst r0, #1
   beq 1b
   and r0, r0, #-2 // Remove the extra bit that marks it's a String.<init> method.
   .if 1
   b NterpHandleStringInitRange
   .else
   b NterpHandleStringInit
   .endif
   .endif

    NAME_END nterp_op_invoke_super_range

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_direct_range: /* 0x76 */
    NAME_START nterp_op_invoke_direct_range

   EXPORT_PC
   // Fast-path which gets the method from thread-local cache.
   FETCH_FROM_THREAD_CACHE r0, 2f
1:
   // Load the first argument (the 'this' pointer).
   FETCH r1, 2
   .if !1
   and r1, r1, #0xf
   .endif
   GET_VREG r1, r1
   cmp r1, #0
   beq common_errNullObject    // bail if null
   b NterpCommonInvokeInstanceRange
2:
   mov r0, rSELF
   ldr r1, [sp]
   mov r2, rPC
   bl nterp_get_method
   .if 0
   b 1b
   .else
   tst r0, #1
   beq 1b
   and r0, r0, #-2 // Remove the extra bit that marks it's a String.<init> method.
   .if 1
   b NterpHandleStringInitRange
   .else
   b NterpHandleStringInit
   .endif
   .endif

    NAME_END nterp_op_invoke_direct_range

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_static_range: /* 0x77 */
    NAME_START nterp_op_invoke_static_range

   EXPORT_PC
   // Fast-path which gets the method from thread-local cache.
   FETCH_FROM_THREAD_CACHE r0, 1f
   b NterpCommonInvokeStaticRange
1:
   mov r0, rSELF
   ldr r1, [sp]
   mov r2, rPC
   bl nterp_get_method
   b NterpCommonInvokeStaticRange

    NAME_END nterp_op_invoke_static_range

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_interface_range: /* 0x78 */
    NAME_START nterp_op_invoke_interface_range

   EXPORT_PC
   // Fast-path which gets the method from thread-local cache.
   FETCH_FROM_THREAD_CACHE r4, nterp_op_invoke_interface_range_helper
.Lop_invoke_interface_range_resume:
   // First argument is the 'this' pointer.
   FETCH r1, 2
   .if !1
   and r1, r1, #0xf
   .endif
   GET_VREG r1, r1
   // Note: if r1 is null, this will be handled by our SIGSEGV handler.
   ldr r2, [r1, #MIRROR_OBJECT_CLASS_OFFSET]
   // Test the first two bits of the fetched ArtMethod:
   // - If the first bit is set, this is a method on j.l.Object
   // - If the second bit is set, this is a default method.
   tst r4, #3
   bne 2f
   ldrh r3, [r4, #ART_METHOD_IMT_INDEX_OFFSET]
1:
   ldr r2, [r2, #MIRROR_CLASS_IMT_PTR_OFFSET_32]
   ldr r0, [r2, r3, lsl #2]
   .if 1
   b NterpCommonInvokeInterfaceRange
   .else
   b NterpCommonInvokeInterface
   .endif
2:
   tst r4, #1
   bne 3f
   and r4, r4, #-4
   ldrh r3, [r4, #ART_METHOD_METHOD_INDEX_OFFSET]
   and r3, r3, #ART_METHOD_IMT_MASK
   b 1b
3:
   lsr r4, r4, #16
   add r2, r2, #MIRROR_CLASS_VTABLE_OFFSET_32
   ldr r0, [r2, r4, lsl #2]
   .if 1
   b NterpCommonInvokeInstanceRange
   .else
   b NterpCommonInvokeInstance
   .endif

    NAME_END nterp_op_invoke_interface_range

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_79: /* 0x79 */
    NAME_START nterp_op_unused_79

    bkpt

    NAME_END nterp_op_unused_79

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_7a: /* 0x7a */
    NAME_START nterp_op_unused_7a

    bkpt

    NAME_END nterp_op_unused_7a

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_neg_int: /* 0x7b */
    NAME_START nterp_op_neg_int

    /*
     * Generic 32-bit unary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = op r0".
     * This could be an ARM instruction or a function call.
     *
     * for: neg-int, not-int, neg-float, int-to-float, float-to-int,
     *      int-to-byte, int-to-char, int-to-short
     */
    /* unop vA, vB */
    mov     r3, rINST, lsr #12          @ r3<- B
    ubfx    r4, rINST, #8, #4           @ r4<- A
    GET_VREG r0, r3                     @ r0<- vB
                               @ optional op; may set condition codes
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST
    rsb     r0, r0, #0                              @ r0<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG r0, r4                     @ vAA<- r0
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 8-9 instructions */

    NAME_END nterp_op_neg_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_not_int: /* 0x7c */
    NAME_START nterp_op_not_int

    /*
     * Generic 32-bit unary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = op r0".
     * This could be an ARM instruction or a function call.
     *
     * for: neg-int, not-int, neg-float, int-to-float, float-to-int,
     *      int-to-byte, int-to-char, int-to-short
     */
    /* unop vA, vB */
    mov     r3, rINST, lsr #12          @ r3<- B
    ubfx    r4, rINST, #8, #4           @ r4<- A
    GET_VREG r0, r3                     @ r0<- vB
                               @ optional op; may set condition codes
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST
    mvn     r0, r0                              @ r0<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG r0, r4                     @ vAA<- r0
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 8-9 instructions */

    NAME_END nterp_op_not_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_neg_long: /* 0x7d */
    NAME_START nterp_op_neg_long

    /*
     * Generic 64-bit unary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = op r0/r1".
     * This could be an ARM instruction or a function call.
     *
     * For: neg-long, not-long, neg-double, long-to-double, double-to-long
     */
    /* unop vA, vB */
    mov     r3, rINST, lsr #12          @ r3<- B
    ubfx    rINST, rINST, #8, #4        @ rINST<- A
    VREG_INDEX_TO_ADDR r3, r3           @ r3<- &fp[B]
    VREG_INDEX_TO_ADDR r4, rINST        @ r4<- &fp[A]
    GET_VREG_WIDE_BY_ADDR r0, r1, r3    @ r0/r1<- vAA
    CLEAR_SHADOW_PAIR rINST, ip, lr     @ Zero shadow regs
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST
    rsbs    r0, r0, #0                           @ optional op; may set condition codes
    rsc     r1, r1, #0                              @ r0/r1<- op, r2-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_WIDE_BY_ADDR r0, r1, r4    @ vAA<- r0/r1
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 10-11 instructions */

    NAME_END nterp_op_neg_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_not_long: /* 0x7e */
    NAME_START nterp_op_not_long

    /*
     * Generic 64-bit unary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = op r0/r1".
     * This could be an ARM instruction or a function call.
     *
     * For: neg-long, not-long, neg-double, long-to-double, double-to-long
     */
    /* unop vA, vB */
    mov     r3, rINST, lsr #12          @ r3<- B
    ubfx    rINST, rINST, #8, #4        @ rINST<- A
    VREG_INDEX_TO_ADDR r3, r3           @ r3<- &fp[B]
    VREG_INDEX_TO_ADDR r4, rINST        @ r4<- &fp[A]
    GET_VREG_WIDE_BY_ADDR r0, r1, r3    @ r0/r1<- vAA
    CLEAR_SHADOW_PAIR rINST, ip, lr     @ Zero shadow regs
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST
    mvn     r0, r0                           @ optional op; may set condition codes
    mvn     r1, r1                              @ r0/r1<- op, r2-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_WIDE_BY_ADDR r0, r1, r4    @ vAA<- r0/r1
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 10-11 instructions */

    NAME_END nterp_op_not_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_neg_float: /* 0x7f */
    NAME_START nterp_op_neg_float

    /*
     * Generic 32-bit unary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = op r0".
     * This could be an ARM instruction or a function call.
     *
     * for: neg-int, not-int, neg-float, int-to-float, float-to-int,
     *      int-to-byte, int-to-char, int-to-short
     */
    /* unop vA, vB */
    mov     r3, rINST, lsr #12          @ r3<- B
    ubfx    r4, rINST, #8, #4           @ r4<- A
    GET_VREG r0, r3                     @ r0<- vB
                               @ optional op; may set condition codes
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST
    add     r0, r0, #0x80000000                              @ r0<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG r0, r4                     @ vAA<- r0
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 8-9 instructions */

    NAME_END nterp_op_neg_float

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_neg_double: /* 0x80 */
    NAME_START nterp_op_neg_double

    /*
     * Generic 64-bit unary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = op r0/r1".
     * This could be an ARM instruction or a function call.
     *
     * For: neg-long, not-long, neg-double, long-to-double, double-to-long
     */
    /* unop vA, vB */
    mov     r3, rINST, lsr #12          @ r3<- B
    ubfx    rINST, rINST, #8, #4        @ rINST<- A
    VREG_INDEX_TO_ADDR r3, r3           @ r3<- &fp[B]
    VREG_INDEX_TO_ADDR r4, rINST        @ r4<- &fp[A]
    GET_VREG_WIDE_BY_ADDR r0, r1, r3    @ r0/r1<- vAA
    CLEAR_SHADOW_PAIR rINST, ip, lr     @ Zero shadow regs
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST
                               @ optional op; may set condition codes
    add     r1, r1, #0x80000000                              @ r0/r1<- op, r2-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_WIDE_BY_ADDR r0, r1, r4    @ vAA<- r0/r1
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 10-11 instructions */

    NAME_END nterp_op_neg_double

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_int_to_long: /* 0x81 */
    NAME_START nterp_op_int_to_long

    /*
     * Generic 32bit-to-64bit unary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = op r0", where
     * "result" is a 64-bit quantity in r0/r1.
     *
     * For: int-to-long, int-to-double, float-to-long, float-to-double
     */
    /* unop vA, vB */
    mov     r3, rINST, lsr #12          @ r3<- B
    ubfx    rINST, rINST, #8, #4        @ rINST<- A
    GET_VREG r0, r3                     @ r0<- vB
    VREG_INDEX_TO_ADDR r4, rINST        @ r4<- &fp[A]
                               @ optional op; may set condition codes
    CLEAR_SHADOW_PAIR rINST, ip, lr     @ Zero shadow regs
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST
    mov     r1, r0, asr #31                              @ r0<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_WIDE_BY_ADDR r0, r1, r4    @ vA/vA+1<- r0/r1
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 9-10 instructions */

    NAME_END nterp_op_int_to_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_int_to_float: /* 0x82 */
    NAME_START nterp_op_int_to_float

    /*
     * Generic 32-bit unary floating-point operation.  Provide an "instr"
     * line that specifies an instruction that performs "s1 = op s0".
     *
     * for: int-to-float, float-to-int
     */
    /* unop vA, vB */
    mov     r3, rINST, lsr #12          @ r3<- B
    VREG_INDEX_TO_ADDR r3, r3           @ r3<- &vB
    GET_VREG_FLOAT_BY_ADDR s0, r3       @ s0<- vB
    ubfx    r4, rINST, #8, #4           @ r4<- A
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST
    vcvt.f32.s32  s1, s0                              @ s1<- op
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_FLOAT s1, r4, lr           @ vA<- s1
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_int_to_float

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_int_to_double: /* 0x83 */
    NAME_START nterp_op_int_to_double

    /*
     * Generic 32bit-to-64bit floating point unary operation.  Provide an
     * "instr" line that specifies an instruction that performs "d0 = op s0".
     *
     * For: int-to-double, float-to-double
     */
    /* unop vA, vB */
    mov     r3, rINST, lsr #12          @ r3<- B
    VREG_INDEX_TO_ADDR r3, r3           @ r3<- &vB
    GET_VREG_FLOAT_BY_ADDR s0, r3       @ s0<- vB
    ubfx    r4, rINST, #8, #4           @ r4<- A
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST
    vcvt.f64.s32  d0, s0                              @ d0<- op
    CLEAR_SHADOW_PAIR r4, ip, lr        @ Zero shadow regs
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    VREG_INDEX_TO_ADDR r4, r4           @ r4<- &vA
    SET_VREG_DOUBLE_BY_ADDR d0, r4      @ vA<- d0
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_int_to_double

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_long_to_int: /* 0x84 */
    NAME_START nterp_op_long_to_int

/* we ignore the high word, making this equivalent to a 32-bit reg move */
    /* for move, move-object, long-to-int */
    /* op vA, vB */
    mov     r1, rINST, lsr #12          @ r1<- B from 15:12
    ubfx    r0, rINST, #8, #4           @ r0<- A from 11:8
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST
    GET_VREG r2, r1                     @ r2<- fp[B]
    GET_INST_OPCODE ip                  @ ip<- opcode from rINST
    .if 0
    SET_VREG_OBJECT r2, r0              @ fp[A]<- r2
    .else
    SET_VREG r2, r0                     @ fp[A]<- r2
    .endif
    GOTO_OPCODE ip                      @ execute next instruction

/*
 * We use "mul r0, r1, r0" instead of "r0, r0, r1". The latter was illegal in old versions.
 * Also, for T32, this operand order allows using a 16-bit instruction (encoding T1) while the
 * other order would require 32-bit instruction (encoding T2).
 */

    NAME_END nterp_op_long_to_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_long_to_float: /* 0x85 */
    NAME_START nterp_op_long_to_float

    /*
     * Generic 64bit-to-32bit unary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = op r0/r1", where
     * "result" is a 32-bit quantity in r0.
     *
     * For: long-to-float
     *
     * (This would work for long-to-int, but that instruction is actually
     * an exact match for op_move.)
     */
    /* unop vA, vB */
    mov     r3, rINST, lsr #12          @ r3<- B
    ubfx    r4, rINST, #8, #4           @ r4<- A
    VREG_INDEX_TO_ADDR r3, r3           @ r3<- &fp[B]
    GET_VREG_WIDE_BY_ADDR r0, r1, r3    @ r0/r1<- vB/vB+1
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST
                               @ optional op; may set condition codes
    bl      __aeabi_l2f                              @ r0<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG r0, r4                     @ vA<- r0
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 9-10 instructions */

    NAME_END nterp_op_long_to_float

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_long_to_double: /* 0x86 */
    NAME_START nterp_op_long_to_double

    /*
     * Specialised 64-bit floating point operation.
     *
     * Note: The result will be returned in d2.
     *
     * For: long-to-double
     */
    mov     r3, rINST, lsr #12          @ r3<- B
    ubfx    r4, rINST, #8, #4           @ r4<- A
    CLEAR_SHADOW_PAIR r4, ip, lr        @ Zero shadow regs
    VREG_INDEX_TO_ADDR r3, r3           @ r3<- &fp[B]
    VREG_INDEX_TO_ADDR r4, r4           @ r4<- &fp[A]
    GET_VREG_DOUBLE_BY_ADDR d0, r3      @ d0<- vBB
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST

    vcvt.f64.s32    d1, s1              @ d1<- (double)(vAAh)
    vcvt.f64.u32    d2, s0              @ d2<- (double)(vAAl)
    vldr            d3, constvalop_long_to_double
    vmla.f64        d2, d1, d3          @ d2<- vAAh*2^32 + vAAl

    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_DOUBLE_BY_ADDR d2, r4      @ vAA<- d2
    GOTO_OPCODE ip                      @ jump to next instruction

    /* literal pool helper */
constvalop_long_to_double:
    .8byte          0x41f0000000000000

    NAME_END nterp_op_long_to_double

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_float_to_int: /* 0x87 */
    NAME_START nterp_op_float_to_int

    /*
     * Generic 32-bit unary floating-point operation.  Provide an "instr"
     * line that specifies an instruction that performs "s1 = op s0".
     *
     * for: int-to-float, float-to-int
     */
    /* unop vA, vB */
    mov     r3, rINST, lsr #12          @ r3<- B
    VREG_INDEX_TO_ADDR r3, r3           @ r3<- &vB
    GET_VREG_FLOAT_BY_ADDR s0, r3       @ s0<- vB
    ubfx    r4, rINST, #8, #4           @ r4<- A
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST
    vcvt.s32.f32 s1, s0                              @ s1<- op
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_FLOAT s1, r4, lr           @ vA<- s1
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_float_to_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_float_to_long: /* 0x88 */
    NAME_START nterp_op_float_to_long

    /*
     * Generic 32bit-to-64bit unary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = op r0", where
     * "result" is a 64-bit quantity in r0/r1.
     *
     * For: int-to-long, int-to-double, float-to-long, float-to-double
     */
    /* unop vA, vB */
    mov     r3, rINST, lsr #12          @ r3<- B
    ubfx    rINST, rINST, #8, #4        @ rINST<- A
    GET_VREG r0, r3                     @ r0<- vB
    VREG_INDEX_TO_ADDR r4, rINST        @ r4<- &fp[A]
                               @ optional op; may set condition codes
    CLEAR_SHADOW_PAIR rINST, ip, lr     @ Zero shadow regs
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST
    bl      nterp_f2l_doconv                              @ r0<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_WIDE_BY_ADDR r0, r1, r4    @ vA/vA+1<- r0/r1
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 9-10 instructions */

    NAME_END nterp_op_float_to_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_float_to_double: /* 0x89 */
    NAME_START nterp_op_float_to_double

    /*
     * Generic 32bit-to-64bit floating point unary operation.  Provide an
     * "instr" line that specifies an instruction that performs "d0 = op s0".
     *
     * For: int-to-double, float-to-double
     */
    /* unop vA, vB */
    mov     r3, rINST, lsr #12          @ r3<- B
    VREG_INDEX_TO_ADDR r3, r3           @ r3<- &vB
    GET_VREG_FLOAT_BY_ADDR s0, r3       @ s0<- vB
    ubfx    r4, rINST, #8, #4           @ r4<- A
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST
    vcvt.f64.f32  d0, s0                              @ d0<- op
    CLEAR_SHADOW_PAIR r4, ip, lr        @ Zero shadow regs
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    VREG_INDEX_TO_ADDR r4, r4           @ r4<- &vA
    SET_VREG_DOUBLE_BY_ADDR d0, r4      @ vA<- d0
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_float_to_double

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_double_to_int: /* 0x8a */
    NAME_START nterp_op_double_to_int

    /*
     * Generic 64bit-to-32bit unary floating point operation.  Provide an
     * "instr" line that specifies an instruction that performs "s0 = op d0".
     *
     * For: double-to-int, double-to-float
     */
    /* unop vA, vB */
    mov     r3, rINST, lsr #12          @ r3<- B
    VREG_INDEX_TO_ADDR r3, r3           @ r3<- &vB
    GET_VREG_DOUBLE_BY_ADDR d0, r3      @ d0<- vB
    ubfx    r4, rINST, #8, #4           @ r4<- A
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST
    vcvt.s32.f64  s0, d0                              @ s0<- op
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_FLOAT s0, r4, lr           @ vA<- s0
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_double_to_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_double_to_long: /* 0x8b */
    NAME_START nterp_op_double_to_long

    /*
     * Generic 64-bit unary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = op r0/r1".
     * This could be an ARM instruction or a function call.
     *
     * For: neg-long, not-long, neg-double, long-to-double, double-to-long
     */
    /* unop vA, vB */
    mov     r3, rINST, lsr #12          @ r3<- B
    ubfx    rINST, rINST, #8, #4        @ rINST<- A
    VREG_INDEX_TO_ADDR r3, r3           @ r3<- &fp[B]
    VREG_INDEX_TO_ADDR r4, rINST        @ r4<- &fp[A]
    GET_VREG_WIDE_BY_ADDR r0, r1, r3    @ r0/r1<- vAA
    CLEAR_SHADOW_PAIR rINST, ip, lr     @ Zero shadow regs
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST
                               @ optional op; may set condition codes
    bl      nterp_d2l_doconv                              @ r0/r1<- op, r2-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_WIDE_BY_ADDR r0, r1, r4    @ vAA<- r0/r1
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 10-11 instructions */

    NAME_END nterp_op_double_to_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_double_to_float: /* 0x8c */
    NAME_START nterp_op_double_to_float

    /*
     * Generic 64bit-to-32bit unary floating point operation.  Provide an
     * "instr" line that specifies an instruction that performs "s0 = op d0".
     *
     * For: double-to-int, double-to-float
     */
    /* unop vA, vB */
    mov     r3, rINST, lsr #12          @ r3<- B
    VREG_INDEX_TO_ADDR r3, r3           @ r3<- &vB
    GET_VREG_DOUBLE_BY_ADDR d0, r3      @ d0<- vB
    ubfx    r4, rINST, #8, #4           @ r4<- A
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST
    vcvt.f32.f64  s0, d0                              @ s0<- op
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_FLOAT s0, r4, lr           @ vA<- s0
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_double_to_float

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_int_to_byte: /* 0x8d */
    NAME_START nterp_op_int_to_byte

    /*
     * Generic 32-bit unary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = op r0".
     * This could be an ARM instruction or a function call.
     *
     * for: neg-int, not-int, neg-float, int-to-float, float-to-int,
     *      int-to-byte, int-to-char, int-to-short
     */
    /* unop vA, vB */
    mov     r3, rINST, lsr #12          @ r3<- B
    ubfx    r4, rINST, #8, #4           @ r4<- A
    GET_VREG r0, r3                     @ r0<- vB
                               @ optional op; may set condition codes
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST
    sxtb    r0, r0                              @ r0<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG r0, r4                     @ vAA<- r0
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 8-9 instructions */

    NAME_END nterp_op_int_to_byte

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_int_to_char: /* 0x8e */
    NAME_START nterp_op_int_to_char

    /*
     * Generic 32-bit unary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = op r0".
     * This could be an ARM instruction or a function call.
     *
     * for: neg-int, not-int, neg-float, int-to-float, float-to-int,
     *      int-to-byte, int-to-char, int-to-short
     */
    /* unop vA, vB */
    mov     r3, rINST, lsr #12          @ r3<- B
    ubfx    r4, rINST, #8, #4           @ r4<- A
    GET_VREG r0, r3                     @ r0<- vB
                               @ optional op; may set condition codes
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST
    uxth    r0, r0                              @ r0<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG r0, r4                     @ vAA<- r0
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 8-9 instructions */

    NAME_END nterp_op_int_to_char

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_int_to_short: /* 0x8f */
    NAME_START nterp_op_int_to_short

    /*
     * Generic 32-bit unary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = op r0".
     * This could be an ARM instruction or a function call.
     *
     * for: neg-int, not-int, neg-float, int-to-float, float-to-int,
     *      int-to-byte, int-to-char, int-to-short
     */
    /* unop vA, vB */
    mov     r3, rINST, lsr #12          @ r3<- B
    ubfx    r4, rINST, #8, #4           @ r4<- A
    GET_VREG r0, r3                     @ r0<- vB
                               @ optional op; may set condition codes
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST
    sxth    r0, r0                              @ r0<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG r0, r4                     @ vAA<- r0
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 8-9 instructions */

    NAME_END nterp_op_int_to_short

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_add_int: /* 0x90 */
    NAME_START nterp_op_add_int

    /*
     * Generic 32-bit binary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = r0 op r1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than r0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (r1).  Useful for integer division and modulus.  Note that we
     * *don't* check for (INT_MIN / -1) here, because the ARM math lib
     * handles it correctly.
     *
     * For: add-int, sub-int, mul-int, div-int, rem-int, and-int, or-int,
     *      xor-int, shl-int, shr-int, ushr-int, add-float, sub-float,
     *      mul-float, div-float, rem-float
     */
    /* binop vAA, vBB, vCC */
    FETCH r0, 1                         @ r0<- CCBB
    mov     r4, rINST, lsr #8           @ r4<- AA
    mov     r3, r0, lsr #8              @ r3<- CC
    and     r2, r0, #255                @ r2<- BB
    GET_VREG r1, r3                     @ r1<- vCC
    GET_VREG r0, r2                     @ r0<- vBB
    .if 0
    cmp     r1, #0                      @ is second operand zero?
    beq     common_errDivideByZero
    .endif

    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
                               @ optional op; may set condition codes
    add     r0, r0, r1                              @ r0<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG r0, r4                @ vAA<- r0
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 11-14 instructions */

    NAME_END nterp_op_add_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sub_int: /* 0x91 */
    NAME_START nterp_op_sub_int

    /*
     * Generic 32-bit binary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = r0 op r1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than r0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (r1).  Useful for integer division and modulus.  Note that we
     * *don't* check for (INT_MIN / -1) here, because the ARM math lib
     * handles it correctly.
     *
     * For: add-int, sub-int, mul-int, div-int, rem-int, and-int, or-int,
     *      xor-int, shl-int, shr-int, ushr-int, add-float, sub-float,
     *      mul-float, div-float, rem-float
     */
    /* binop vAA, vBB, vCC */
    FETCH r0, 1                         @ r0<- CCBB
    mov     r4, rINST, lsr #8           @ r4<- AA
    mov     r3, r0, lsr #8              @ r3<- CC
    and     r2, r0, #255                @ r2<- BB
    GET_VREG r1, r3                     @ r1<- vCC
    GET_VREG r0, r2                     @ r0<- vBB
    .if 0
    cmp     r1, #0                      @ is second operand zero?
    beq     common_errDivideByZero
    .endif

    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
                               @ optional op; may set condition codes
    sub     r0, r0, r1                              @ r0<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG r0, r4                @ vAA<- r0
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 11-14 instructions */

    NAME_END nterp_op_sub_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_mul_int: /* 0x92 */
    NAME_START nterp_op_mul_int

    /*
     * Generic 32-bit binary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = r0 op r1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than r0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (r1).  Useful for integer division and modulus.  Note that we
     * *don't* check for (INT_MIN / -1) here, because the ARM math lib
     * handles it correctly.
     *
     * For: add-int, sub-int, mul-int, div-int, rem-int, and-int, or-int,
     *      xor-int, shl-int, shr-int, ushr-int, add-float, sub-float,
     *      mul-float, div-float, rem-float
     */
    /* binop vAA, vBB, vCC */
    FETCH r0, 1                         @ r0<- CCBB
    mov     r4, rINST, lsr #8           @ r4<- AA
    mov     r3, r0, lsr #8              @ r3<- CC
    and     r2, r0, #255                @ r2<- BB
    GET_VREG r1, r3                     @ r1<- vCC
    GET_VREG r0, r2                     @ r0<- vBB
    .if 0
    cmp     r1, #0                      @ is second operand zero?
    beq     common_errDivideByZero
    .endif

    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
                               @ optional op; may set condition codes
    mul     r0, r1, r0                              @ r0<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG r0, r4                @ vAA<- r0
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 11-14 instructions */

    NAME_END nterp_op_mul_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_div_int: /* 0x93 */
    NAME_START nterp_op_div_int

    /*
     * Specialized 32-bit binary operation
     *
     * Performs "r0 = r0 div r1". The selection between sdiv or the gcc helper
     * depends on the compile time value of __ARM_ARCH_EXT_IDIV__ (defined for
     * ARMv7 CPUs that have hardware division support).
     *
     * div-int
     *
     */
    FETCH r0, 1                         @ r0<- CCBB
    mov     r4, rINST, lsr #8           @ r4<- AA
    mov     r3, r0, lsr #8              @ r3<- CC
    and     r2, r0, #255                @ r2<- BB
    GET_VREG r1, r3                     @ r1<- vCC
    GET_VREG r0, r2                     @ r0<- vBB
    cmp     r1, #0                      @ is second operand zero?
    beq     common_errDivideByZero

    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
#ifdef __ARM_ARCH_EXT_IDIV__
    sdiv    r0, r0, r1                  @ r0<- op
#else
    bl    __aeabi_idiv                  @ r0<- op, r0-r3 changed
#endif
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG r0, r4                     @ vAA<- r0
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 11-14 instructions */

    NAME_END nterp_op_div_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_rem_int: /* 0x94 */
    NAME_START nterp_op_rem_int

    /*
     * Specialized 32-bit binary operation
     *
     * Performs "r1 = r0 rem r1". The selection between sdiv block or the gcc helper
     * depends on the compile time value of __ARM_ARCH_EXT_IDIV__ (defined for
     * ARMv7 CPUs that have hardware division support).
     *
     * NOTE: idivmod returns quotient in r0 and remainder in r1
     *
     * rem-int
     *
     */
    FETCH r0, 1                         @ r0<- CCBB
    mov     r4, rINST, lsr #8           @ r4<- AA
    mov     r3, r0, lsr #8              @ r3<- CC
    and     r2, r0, #255                @ r2<- BB
    GET_VREG r1, r3                     @ r1<- vCC
    GET_VREG r0, r2                     @ r0<- vBB
    cmp     r1, #0                      @ is second operand zero?
    beq     common_errDivideByZero

    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
#ifdef __ARM_ARCH_EXT_IDIV__
    sdiv    r2, r0, r1
    mls     r1, r1, r2, r0                 @ r1<- op, r0-r2 changed
#else
    bl      __aeabi_idivmod                @ r1<- op, r0-r3 changed
#endif
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG r1, r4                     @ vAA<- r1
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 11-14 instructions */

    NAME_END nterp_op_rem_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_and_int: /* 0x95 */
    NAME_START nterp_op_and_int

    /*
     * Generic 32-bit binary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = r0 op r1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than r0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (r1).  Useful for integer division and modulus.  Note that we
     * *don't* check for (INT_MIN / -1) here, because the ARM math lib
     * handles it correctly.
     *
     * For: add-int, sub-int, mul-int, div-int, rem-int, and-int, or-int,
     *      xor-int, shl-int, shr-int, ushr-int, add-float, sub-float,
     *      mul-float, div-float, rem-float
     */
    /* binop vAA, vBB, vCC */
    FETCH r0, 1                         @ r0<- CCBB
    mov     r4, rINST, lsr #8           @ r4<- AA
    mov     r3, r0, lsr #8              @ r3<- CC
    and     r2, r0, #255                @ r2<- BB
    GET_VREG r1, r3                     @ r1<- vCC
    GET_VREG r0, r2                     @ r0<- vBB
    .if 0
    cmp     r1, #0                      @ is second operand zero?
    beq     common_errDivideByZero
    .endif

    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
                               @ optional op; may set condition codes
    and     r0, r0, r1                              @ r0<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG r0, r4                @ vAA<- r0
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 11-14 instructions */

    NAME_END nterp_op_and_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_or_int: /* 0x96 */
    NAME_START nterp_op_or_int

    /*
     * Generic 32-bit binary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = r0 op r1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than r0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (r1).  Useful for integer division and modulus.  Note that we
     * *don't* check for (INT_MIN / -1) here, because the ARM math lib
     * handles it correctly.
     *
     * For: add-int, sub-int, mul-int, div-int, rem-int, and-int, or-int,
     *      xor-int, shl-int, shr-int, ushr-int, add-float, sub-float,
     *      mul-float, div-float, rem-float
     */
    /* binop vAA, vBB, vCC */
    FETCH r0, 1                         @ r0<- CCBB
    mov     r4, rINST, lsr #8           @ r4<- AA
    mov     r3, r0, lsr #8              @ r3<- CC
    and     r2, r0, #255                @ r2<- BB
    GET_VREG r1, r3                     @ r1<- vCC
    GET_VREG r0, r2                     @ r0<- vBB
    .if 0
    cmp     r1, #0                      @ is second operand zero?
    beq     common_errDivideByZero
    .endif

    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
                               @ optional op; may set condition codes
    orr     r0, r0, r1                              @ r0<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG r0, r4                @ vAA<- r0
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 11-14 instructions */

    NAME_END nterp_op_or_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_xor_int: /* 0x97 */
    NAME_START nterp_op_xor_int

    /*
     * Generic 32-bit binary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = r0 op r1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than r0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (r1).  Useful for integer division and modulus.  Note that we
     * *don't* check for (INT_MIN / -1) here, because the ARM math lib
     * handles it correctly.
     *
     * For: add-int, sub-int, mul-int, div-int, rem-int, and-int, or-int,
     *      xor-int, shl-int, shr-int, ushr-int, add-float, sub-float,
     *      mul-float, div-float, rem-float
     */
    /* binop vAA, vBB, vCC */
    FETCH r0, 1                         @ r0<- CCBB
    mov     r4, rINST, lsr #8           @ r4<- AA
    mov     r3, r0, lsr #8              @ r3<- CC
    and     r2, r0, #255                @ r2<- BB
    GET_VREG r1, r3                     @ r1<- vCC
    GET_VREG r0, r2                     @ r0<- vBB
    .if 0
    cmp     r1, #0                      @ is second operand zero?
    beq     common_errDivideByZero
    .endif

    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
                               @ optional op; may set condition codes
    eor     r0, r0, r1                              @ r0<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG r0, r4                @ vAA<- r0
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 11-14 instructions */

    NAME_END nterp_op_xor_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_shl_int: /* 0x98 */
    NAME_START nterp_op_shl_int

    /*
     * Generic 32-bit binary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = r0 op r1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than r0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (r1).  Useful for integer division and modulus.  Note that we
     * *don't* check for (INT_MIN / -1) here, because the ARM math lib
     * handles it correctly.
     *
     * For: add-int, sub-int, mul-int, div-int, rem-int, and-int, or-int,
     *      xor-int, shl-int, shr-int, ushr-int, add-float, sub-float,
     *      mul-float, div-float, rem-float
     */
    /* binop vAA, vBB, vCC */
    FETCH r0, 1                         @ r0<- CCBB
    mov     r4, rINST, lsr #8           @ r4<- AA
    mov     r3, r0, lsr #8              @ r3<- CC
    and     r2, r0, #255                @ r2<- BB
    GET_VREG r1, r3                     @ r1<- vCC
    GET_VREG r0, r2                     @ r0<- vBB
    .if 0
    cmp     r1, #0                      @ is second operand zero?
    beq     common_errDivideByZero
    .endif

    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    and     r1, r1, #31                           @ optional op; may set condition codes
    mov     r0, r0, lsl r1                              @ r0<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG r0, r4                @ vAA<- r0
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 11-14 instructions */

    NAME_END nterp_op_shl_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_shr_int: /* 0x99 */
    NAME_START nterp_op_shr_int

    /*
     * Generic 32-bit binary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = r0 op r1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than r0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (r1).  Useful for integer division and modulus.  Note that we
     * *don't* check for (INT_MIN / -1) here, because the ARM math lib
     * handles it correctly.
     *
     * For: add-int, sub-int, mul-int, div-int, rem-int, and-int, or-int,
     *      xor-int, shl-int, shr-int, ushr-int, add-float, sub-float,
     *      mul-float, div-float, rem-float
     */
    /* binop vAA, vBB, vCC */
    FETCH r0, 1                         @ r0<- CCBB
    mov     r4, rINST, lsr #8           @ r4<- AA
    mov     r3, r0, lsr #8              @ r3<- CC
    and     r2, r0, #255                @ r2<- BB
    GET_VREG r1, r3                     @ r1<- vCC
    GET_VREG r0, r2                     @ r0<- vBB
    .if 0
    cmp     r1, #0                      @ is second operand zero?
    beq     common_errDivideByZero
    .endif

    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    and     r1, r1, #31                           @ optional op; may set condition codes
    mov     r0, r0, asr r1                              @ r0<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG r0, r4                @ vAA<- r0
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 11-14 instructions */

    NAME_END nterp_op_shr_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_ushr_int: /* 0x9a */
    NAME_START nterp_op_ushr_int

    /*
     * Generic 32-bit binary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = r0 op r1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than r0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (r1).  Useful for integer division and modulus.  Note that we
     * *don't* check for (INT_MIN / -1) here, because the ARM math lib
     * handles it correctly.
     *
     * For: add-int, sub-int, mul-int, div-int, rem-int, and-int, or-int,
     *      xor-int, shl-int, shr-int, ushr-int, add-float, sub-float,
     *      mul-float, div-float, rem-float
     */
    /* binop vAA, vBB, vCC */
    FETCH r0, 1                         @ r0<- CCBB
    mov     r4, rINST, lsr #8           @ r4<- AA
    mov     r3, r0, lsr #8              @ r3<- CC
    and     r2, r0, #255                @ r2<- BB
    GET_VREG r1, r3                     @ r1<- vCC
    GET_VREG r0, r2                     @ r0<- vBB
    .if 0
    cmp     r1, #0                      @ is second operand zero?
    beq     common_errDivideByZero
    .endif

    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    and     r1, r1, #31                           @ optional op; may set condition codes
    mov     r0, r0, lsr r1                              @ r0<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG r0, r4                @ vAA<- r0
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 11-14 instructions */

    NAME_END nterp_op_ushr_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_add_long: /* 0x9b */
    NAME_START nterp_op_add_long

    /*
     * Generic 64-bit binary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = r0-r1 op r2-r3".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than r0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (r1).  Useful for integer division and modulus.
     *
     * for: add-long, sub-long, div-long, rem-long, and-long, or-long,
     *      xor-long, add-double, sub-double, mul-double, div-double,
     *      rem-double
     *
     * IMPORTANT: you may specify "chkzero" or "preinstr" but not both.
     */
    /* binop vAA, vBB, vCC */
    FETCH r0, 1                         @ r0<- CCBB
    mov     rINST, rINST, lsr #8        @ rINST<- AA
    and     r2, r0, #255                @ r2<- BB
    mov     r3, r0, lsr #8              @ r3<- CC
    VREG_INDEX_TO_ADDR r4, rINST        @ r4<- &fp[AA]
    VREG_INDEX_TO_ADDR r2, r2           @ r2<- &fp[BB]
    VREG_INDEX_TO_ADDR r3, r3           @ r3<- &fp[CC]
    GET_VREG_WIDE_BY_ADDR r0, r1, r2    @ r0/r1<- vBB/vBB+1
    GET_VREG_WIDE_BY_ADDR r2, r3, r3    @ r2/r3<- vCC/vCC+1
    .if 0
    orrs    ip, r2, r3                  @ second arg (r2-r3) is zero?
    beq     common_errDivideByZero
    .endif
    CLEAR_SHADOW_PAIR rINST, lr, ip     @ Zero out the shadow regs
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    adds    r0, r0, r2                           @ optional op; may set condition codes
    adc     r1, r1, r3                              @ result<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_WIDE_BY_ADDR r0,r1,r4  @ vAA/vAA+1<,  r0/r1
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 14-17 instructions */

    NAME_END nterp_op_add_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sub_long: /* 0x9c */
    NAME_START nterp_op_sub_long

    /*
     * Generic 64-bit binary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = r0-r1 op r2-r3".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than r0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (r1).  Useful for integer division and modulus.
     *
     * for: add-long, sub-long, div-long, rem-long, and-long, or-long,
     *      xor-long, add-double, sub-double, mul-double, div-double,
     *      rem-double
     *
     * IMPORTANT: you may specify "chkzero" or "preinstr" but not both.
     */
    /* binop vAA, vBB, vCC */
    FETCH r0, 1                         @ r0<- CCBB
    mov     rINST, rINST, lsr #8        @ rINST<- AA
    and     r2, r0, #255                @ r2<- BB
    mov     r3, r0, lsr #8              @ r3<- CC
    VREG_INDEX_TO_ADDR r4, rINST        @ r4<- &fp[AA]
    VREG_INDEX_TO_ADDR r2, r2           @ r2<- &fp[BB]
    VREG_INDEX_TO_ADDR r3, r3           @ r3<- &fp[CC]
    GET_VREG_WIDE_BY_ADDR r0, r1, r2    @ r0/r1<- vBB/vBB+1
    GET_VREG_WIDE_BY_ADDR r2, r3, r3    @ r2/r3<- vCC/vCC+1
    .if 0
    orrs    ip, r2, r3                  @ second arg (r2-r3) is zero?
    beq     common_errDivideByZero
    .endif
    CLEAR_SHADOW_PAIR rINST, lr, ip     @ Zero out the shadow regs
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    subs    r0, r0, r2                           @ optional op; may set condition codes
    sbc     r1, r1, r3                              @ result<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_WIDE_BY_ADDR r0,r1,r4  @ vAA/vAA+1<,  r0/r1
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 14-17 instructions */

    NAME_END nterp_op_sub_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_mul_long: /* 0x9d */
    NAME_START nterp_op_mul_long

    /*
     * Signed 64-bit integer multiply.
     *
     * Consider WXxYZ (r1r0 x r3r2) with a long multiply:
     *        WX
     *      x YZ
     *  --------
     *     ZW ZX
     *  YW YX
     *
     * The low word of the result holds ZX, the high word holds
     * (ZW+YX) + (the high overflow from ZX).  YW doesn't matter because
     * it doesn't fit in the low 64 bits.
     *
     * Unlike most ARM math operations, multiply instructions have
     * restrictions on using the same register more than once (Rd and Rn
     * cannot be the same).
     */
    /* mul-long vAA, vBB, vCC */
    FETCH r0, 1                         @ r0<- CCBB
    and     r2, r0, #255                @ r2<- BB
    mov     r3, r0, lsr #8              @ r3<- CC
    VREG_INDEX_TO_ADDR r2, r2           @ r2<- &fp[BB]
    VREG_INDEX_TO_ADDR r3, r3           @ r3<- &fp[CC]
    GET_VREG_WIDE_BY_ADDR r0, r1, r2    @ r0/r1<- vBB/vBB+1
    GET_VREG_WIDE_BY_ADDR r2, r3, r3    @ r2/r3<- vCC/vCC+1
    mul     ip, r0, r3                  @ ip<- YxX
    umull   r0, lr, r2, r0              @ r0/lr <- ZxX RdLo == Rn - this is OK.
    mla     r3, r1, r2, ip              @ r3<- YxX + (ZxW)
    mov     r4, rINST, lsr #8           @ r4<- AA
    add     r1, r3, lr                  @ r1<- lr + low(ZxW + (YxX))
    CLEAR_SHADOW_PAIR r4, lr, ip        @ Zero out the shadow regs
    VREG_INDEX_TO_ADDR r4, r4           @ r2<- &fp[AA]
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_WIDE_BY_ADDR r0, r1 , r4   @ vAA/vAA+1<- r1/r2
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_mul_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_div_long: /* 0x9e */
    NAME_START nterp_op_div_long

    /*
     * Generic 64-bit binary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = r0-r1 op r2-r3".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than r0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (r1).  Useful for integer division and modulus.
     *
     * for: add-long, sub-long, div-long, rem-long, and-long, or-long,
     *      xor-long, add-double, sub-double, mul-double, div-double,
     *      rem-double
     *
     * IMPORTANT: you may specify "chkzero" or "preinstr" but not both.
     */
    /* binop vAA, vBB, vCC */
    FETCH r0, 1                         @ r0<- CCBB
    mov     rINST, rINST, lsr #8        @ rINST<- AA
    and     r2, r0, #255                @ r2<- BB
    mov     r3, r0, lsr #8              @ r3<- CC
    VREG_INDEX_TO_ADDR r4, rINST        @ r4<- &fp[AA]
    VREG_INDEX_TO_ADDR r2, r2           @ r2<- &fp[BB]
    VREG_INDEX_TO_ADDR r3, r3           @ r3<- &fp[CC]
    GET_VREG_WIDE_BY_ADDR r0, r1, r2    @ r0/r1<- vBB/vBB+1
    GET_VREG_WIDE_BY_ADDR r2, r3, r3    @ r2/r3<- vCC/vCC+1
    .if 1
    orrs    ip, r2, r3                  @ second arg (r2-r3) is zero?
    beq     common_errDivideByZero
    .endif
    CLEAR_SHADOW_PAIR rINST, lr, ip     @ Zero out the shadow regs
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
                               @ optional op; may set condition codes
    bl      __aeabi_ldivmod                              @ result<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_WIDE_BY_ADDR r0,r1,r4  @ vAA/vAA+1<,  r0/r1
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 14-17 instructions */

    NAME_END nterp_op_div_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_rem_long: /* 0x9f */
    NAME_START nterp_op_rem_long

/* ldivmod returns quotient in r0/r1 and remainder in r2/r3 */
    /*
     * Generic 64-bit binary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = r0-r1 op r2-r3".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than r0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (r1).  Useful for integer division and modulus.
     *
     * for: add-long, sub-long, div-long, rem-long, and-long, or-long,
     *      xor-long, add-double, sub-double, mul-double, div-double,
     *      rem-double
     *
     * IMPORTANT: you may specify "chkzero" or "preinstr" but not both.
     */
    /* binop vAA, vBB, vCC */
    FETCH r0, 1                         @ r0<- CCBB
    mov     rINST, rINST, lsr #8        @ rINST<- AA
    and     r2, r0, #255                @ r2<- BB
    mov     r3, r0, lsr #8              @ r3<- CC
    VREG_INDEX_TO_ADDR r4, rINST        @ r4<- &fp[AA]
    VREG_INDEX_TO_ADDR r2, r2           @ r2<- &fp[BB]
    VREG_INDEX_TO_ADDR r3, r3           @ r3<- &fp[CC]
    GET_VREG_WIDE_BY_ADDR r0, r1, r2    @ r0/r1<- vBB/vBB+1
    GET_VREG_WIDE_BY_ADDR r2, r3, r3    @ r2/r3<- vCC/vCC+1
    .if 1
    orrs    ip, r2, r3                  @ second arg (r2-r3) is zero?
    beq     common_errDivideByZero
    .endif
    CLEAR_SHADOW_PAIR rINST, lr, ip     @ Zero out the shadow regs
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
                               @ optional op; may set condition codes
    bl      __aeabi_ldivmod                              @ result<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_WIDE_BY_ADDR r2,r3,r4  @ vAA/vAA+1<,  r2/r3
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 14-17 instructions */

    NAME_END nterp_op_rem_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_and_long: /* 0xa0 */
    NAME_START nterp_op_and_long

    /*
     * Generic 64-bit binary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = r0-r1 op r2-r3".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than r0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (r1).  Useful for integer division and modulus.
     *
     * for: add-long, sub-long, div-long, rem-long, and-long, or-long,
     *      xor-long, add-double, sub-double, mul-double, div-double,
     *      rem-double
     *
     * IMPORTANT: you may specify "chkzero" or "preinstr" but not both.
     */
    /* binop vAA, vBB, vCC */
    FETCH r0, 1                         @ r0<- CCBB
    mov     rINST, rINST, lsr #8        @ rINST<- AA
    and     r2, r0, #255                @ r2<- BB
    mov     r3, r0, lsr #8              @ r3<- CC
    VREG_INDEX_TO_ADDR r4, rINST        @ r4<- &fp[AA]
    VREG_INDEX_TO_ADDR r2, r2           @ r2<- &fp[BB]
    VREG_INDEX_TO_ADDR r3, r3           @ r3<- &fp[CC]
    GET_VREG_WIDE_BY_ADDR r0, r1, r2    @ r0/r1<- vBB/vBB+1
    GET_VREG_WIDE_BY_ADDR r2, r3, r3    @ r2/r3<- vCC/vCC+1
    .if 0
    orrs    ip, r2, r3                  @ second arg (r2-r3) is zero?
    beq     common_errDivideByZero
    .endif
    CLEAR_SHADOW_PAIR rINST, lr, ip     @ Zero out the shadow regs
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    and     r0, r0, r2                           @ optional op; may set condition codes
    and     r1, r1, r3                              @ result<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_WIDE_BY_ADDR r0,r1,r4  @ vAA/vAA+1<,  r0/r1
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 14-17 instructions */

    NAME_END nterp_op_and_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_or_long: /* 0xa1 */
    NAME_START nterp_op_or_long

    /*
     * Generic 64-bit binary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = r0-r1 op r2-r3".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than r0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (r1).  Useful for integer division and modulus.
     *
     * for: add-long, sub-long, div-long, rem-long, and-long, or-long,
     *      xor-long, add-double, sub-double, mul-double, div-double,
     *      rem-double
     *
     * IMPORTANT: you may specify "chkzero" or "preinstr" but not both.
     */
    /* binop vAA, vBB, vCC */
    FETCH r0, 1                         @ r0<- CCBB
    mov     rINST, rINST, lsr #8        @ rINST<- AA
    and     r2, r0, #255                @ r2<- BB
    mov     r3, r0, lsr #8              @ r3<- CC
    VREG_INDEX_TO_ADDR r4, rINST        @ r4<- &fp[AA]
    VREG_INDEX_TO_ADDR r2, r2           @ r2<- &fp[BB]
    VREG_INDEX_TO_ADDR r3, r3           @ r3<- &fp[CC]
    GET_VREG_WIDE_BY_ADDR r0, r1, r2    @ r0/r1<- vBB/vBB+1
    GET_VREG_WIDE_BY_ADDR r2, r3, r3    @ r2/r3<- vCC/vCC+1
    .if 0
    orrs    ip, r2, r3                  @ second arg (r2-r3) is zero?
    beq     common_errDivideByZero
    .endif
    CLEAR_SHADOW_PAIR rINST, lr, ip     @ Zero out the shadow regs
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    orr     r0, r0, r2                           @ optional op; may set condition codes
    orr     r1, r1, r3                              @ result<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_WIDE_BY_ADDR r0,r1,r4  @ vAA/vAA+1<,  r0/r1
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 14-17 instructions */

    NAME_END nterp_op_or_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_xor_long: /* 0xa2 */
    NAME_START nterp_op_xor_long

    /*
     * Generic 64-bit binary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = r0-r1 op r2-r3".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than r0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (r1).  Useful for integer division and modulus.
     *
     * for: add-long, sub-long, div-long, rem-long, and-long, or-long,
     *      xor-long, add-double, sub-double, mul-double, div-double,
     *      rem-double
     *
     * IMPORTANT: you may specify "chkzero" or "preinstr" but not both.
     */
    /* binop vAA, vBB, vCC */
    FETCH r0, 1                         @ r0<- CCBB
    mov     rINST, rINST, lsr #8        @ rINST<- AA
    and     r2, r0, #255                @ r2<- BB
    mov     r3, r0, lsr #8              @ r3<- CC
    VREG_INDEX_TO_ADDR r4, rINST        @ r4<- &fp[AA]
    VREG_INDEX_TO_ADDR r2, r2           @ r2<- &fp[BB]
    VREG_INDEX_TO_ADDR r3, r3           @ r3<- &fp[CC]
    GET_VREG_WIDE_BY_ADDR r0, r1, r2    @ r0/r1<- vBB/vBB+1
    GET_VREG_WIDE_BY_ADDR r2, r3, r3    @ r2/r3<- vCC/vCC+1
    .if 0
    orrs    ip, r2, r3                  @ second arg (r2-r3) is zero?
    beq     common_errDivideByZero
    .endif
    CLEAR_SHADOW_PAIR rINST, lr, ip     @ Zero out the shadow regs
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    eor     r0, r0, r2                           @ optional op; may set condition codes
    eor     r1, r1, r3                              @ result<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_WIDE_BY_ADDR r0,r1,r4  @ vAA/vAA+1<,  r0/r1
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 14-17 instructions */

    NAME_END nterp_op_xor_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_shl_long: /* 0xa3 */
    NAME_START nterp_op_shl_long

    /*
     * Long integer shift.  This is different from the generic 32/64-bit
     * binary operations because vAA/vBB are 64-bit but vCC (the shift
     * distance) is 32-bit.  Also, Dalvik requires us to mask off the low
     * 6 bits of the shift distance.
     */
    /* shl-long vAA, vBB, vCC */
    FETCH r0, 1                         @ r0<- CCBB
    mov     r4, rINST, lsr #8           @ r4<- AA
    and     r3, r0, #255                @ r3<- BB
    mov     r0, r0, lsr #8              @ r0<- CC
    VREG_INDEX_TO_ADDR r3, r3           @ r3<- &fp[BB]
    GET_VREG r2, r0                     @ r2<- vCC
    GET_VREG_WIDE_BY_ADDR r0, r1, r3    @ r0/r1<- vBB/vBB+1
    CLEAR_SHADOW_PAIR r4, lr, ip        @ Zero out the shadow regs
    and     r2, r2, #63                 @ r2<- r2 & 0x3f
    VREG_INDEX_TO_ADDR r4, r4           @ r4<- &fp[AA]
    mov     r1, r1, asl r2              @ r1<- r1 << r2
    rsb     r3, r2, #32                 @ r3<- 32 - r2
    orr     r1, r1, r0, lsr r3          @ r1<- r1 | (r0 << (32-r2))
    subs    ip, r2, #32                 @ ip<- r2 - 32
    it      pl
    movpl   r1, r0, asl ip              @ if r2 >= 32, r1<- r0 << (r2-32)
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    mov     r0, r0, asl r2              @ r0<- r0 << r2
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_WIDE_BY_ADDR r0, r1, r4    @ vAA/vAA+1<- r0/r1
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_shl_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_shr_long: /* 0xa4 */
    NAME_START nterp_op_shr_long

    /*
     * Long integer shift.  This is different from the generic 32/64-bit
     * binary operations because vAA/vBB are 64-bit but vCC (the shift
     * distance) is 32-bit.  Also, Dalvik requires us to mask off the low
     * 6 bits of the shift distance.
     */
    /* shr-long vAA, vBB, vCC */
    FETCH r0, 1                         @ r0<- CCBB
    mov     r4, rINST, lsr #8           @ r4<- AA
    and     r3, r0, #255                @ r3<- BB
    mov     r0, r0, lsr #8              @ r0<- CC
    VREG_INDEX_TO_ADDR r3, r3           @ r3<- &fp[BB]
    GET_VREG r2, r0                     @ r2<- vCC
    GET_VREG_WIDE_BY_ADDR r0, r1, r3    @ r0/r1<- vBB/vBB+1
    CLEAR_SHADOW_PAIR r4, lr, ip        @ Zero out the shadow regs
    and     r2, r2, #63                 @ r0<- r0 & 0x3f
    VREG_INDEX_TO_ADDR r4, r4           @ r4<- &fp[AA]
    mov     r0, r0, lsr r2              @ r0<- r2 >> r2
    rsb     r3, r2, #32                 @ r3<- 32 - r2
    orr     r0, r0, r1, lsl r3          @ r0<- r0 | (r1 << (32-r2))
    subs    ip, r2, #32                 @ ip<- r2 - 32
    it      pl
    movpl   r0, r1, asr ip              @ if r2 >= 32, r0<-r1 >> (r2-32)
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    mov     r1, r1, asr r2              @ r1<- r1 >> r2
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_WIDE_BY_ADDR r0, r1, r4    @ vAA/vAA+1<- r0/r1
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_shr_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_ushr_long: /* 0xa5 */
    NAME_START nterp_op_ushr_long

    /*
     * Long integer shift.  This is different from the generic 32/64-bit
     * binary operations because vAA/vBB are 64-bit but vCC (the shift
     * distance) is 32-bit.  Also, Dalvik requires us to mask off the low
     * 6 bits of the shift distance.
     */
    /* ushr-long vAA, vBB, vCC */
    FETCH r0, 1                         @ r0<- CCBB
    mov     r4, rINST, lsr #8           @ r4<- AA
    and     r3, r0, #255                @ r3<- BB
    mov     r0, r0, lsr #8              @ r0<- CC
    VREG_INDEX_TO_ADDR r3, r3           @ r3<- &fp[BB]
    GET_VREG r2, r0                     @ r2<- vCC
    GET_VREG_WIDE_BY_ADDR r0, r1, r3    @ r0/r1<- vBB/vBB+1
    CLEAR_SHADOW_PAIR r4, lr, ip        @ Zero out the shadow regs
    and     r2, r2, #63                 @ r0<- r0 & 0x3f
    VREG_INDEX_TO_ADDR r4, r4           @ r4<- &fp[AA]
    mov     r0, r0, lsr r2              @ r0<- r2 >> r2
    rsb     r3, r2, #32                 @ r3<- 32 - r2
    orr     r0, r0, r1, lsl r3          @ r0<- r0 | (r1 << (32-r2))
    subs    ip, r2, #32                 @ ip<- r2 - 32
    it      pl
    movpl   r0, r1, lsr ip              @ if r2 >= 32, r0<-r1 >>> (r2-32)
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    mov     r1, r1, lsr r2              @ r1<- r1 >>> r2
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_WIDE_BY_ADDR r0, r1, r4    @ vAA/vAA+1<- r0/r1
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_ushr_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_add_float: /* 0xa6 */
    NAME_START nterp_op_add_float

    /*
     * Generic 32-bit floating-point operation.  Provide an "instr" line that
     * specifies an instruction that performs "s2 = s0 op s1".  Because we
     * use the "softfp" ABI, this must be an instruction, not a function call.
     *
     * For: add-float, sub-float, mul-float, div-float
     */
    /* floatop vAA, vBB, vCC */
    FETCH r0, 1                         @ r0<- CCBB
    mov     r4, rINST, lsr #8           @ r4<- AA
    mov     r3, r0, lsr #8              @ r3<- CC
    and     r2, r0, #255                @ r2<- BB
    VREG_INDEX_TO_ADDR r3, r3           @ r3<- &vCC
    VREG_INDEX_TO_ADDR r2, r2           @ r2<- &vBB
    GET_VREG_FLOAT_BY_ADDR s1, r3       @ s1<- vCC
    GET_VREG_FLOAT_BY_ADDR s0, r2       @ s0<- vBB

    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    fadds   s2, s0, s1                              @ s2<- op
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_FLOAT s2, r4, lr           @ vAA<- s2
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_add_float

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sub_float: /* 0xa7 */
    NAME_START nterp_op_sub_float

    /*
     * Generic 32-bit floating-point operation.  Provide an "instr" line that
     * specifies an instruction that performs "s2 = s0 op s1".  Because we
     * use the "softfp" ABI, this must be an instruction, not a function call.
     *
     * For: add-float, sub-float, mul-float, div-float
     */
    /* floatop vAA, vBB, vCC */
    FETCH r0, 1                         @ r0<- CCBB
    mov     r4, rINST, lsr #8           @ r4<- AA
    mov     r3, r0, lsr #8              @ r3<- CC
    and     r2, r0, #255                @ r2<- BB
    VREG_INDEX_TO_ADDR r3, r3           @ r3<- &vCC
    VREG_INDEX_TO_ADDR r2, r2           @ r2<- &vBB
    GET_VREG_FLOAT_BY_ADDR s1, r3       @ s1<- vCC
    GET_VREG_FLOAT_BY_ADDR s0, r2       @ s0<- vBB

    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    fsubs   s2, s0, s1                              @ s2<- op
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_FLOAT s2, r4, lr           @ vAA<- s2
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_sub_float

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_mul_float: /* 0xa8 */
    NAME_START nterp_op_mul_float

    /*
     * Generic 32-bit floating-point operation.  Provide an "instr" line that
     * specifies an instruction that performs "s2 = s0 op s1".  Because we
     * use the "softfp" ABI, this must be an instruction, not a function call.
     *
     * For: add-float, sub-float, mul-float, div-float
     */
    /* floatop vAA, vBB, vCC */
    FETCH r0, 1                         @ r0<- CCBB
    mov     r4, rINST, lsr #8           @ r4<- AA
    mov     r3, r0, lsr #8              @ r3<- CC
    and     r2, r0, #255                @ r2<- BB
    VREG_INDEX_TO_ADDR r3, r3           @ r3<- &vCC
    VREG_INDEX_TO_ADDR r2, r2           @ r2<- &vBB
    GET_VREG_FLOAT_BY_ADDR s1, r3       @ s1<- vCC
    GET_VREG_FLOAT_BY_ADDR s0, r2       @ s0<- vBB

    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    fmuls   s2, s0, s1                              @ s2<- op
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_FLOAT s2, r4, lr           @ vAA<- s2
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_mul_float

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_div_float: /* 0xa9 */
    NAME_START nterp_op_div_float

    /*
     * Generic 32-bit floating-point operation.  Provide an "instr" line that
     * specifies an instruction that performs "s2 = s0 op s1".  Because we
     * use the "softfp" ABI, this must be an instruction, not a function call.
     *
     * For: add-float, sub-float, mul-float, div-float
     */
    /* floatop vAA, vBB, vCC */
    FETCH r0, 1                         @ r0<- CCBB
    mov     r4, rINST, lsr #8           @ r4<- AA
    mov     r3, r0, lsr #8              @ r3<- CC
    and     r2, r0, #255                @ r2<- BB
    VREG_INDEX_TO_ADDR r3, r3           @ r3<- &vCC
    VREG_INDEX_TO_ADDR r2, r2           @ r2<- &vBB
    GET_VREG_FLOAT_BY_ADDR s1, r3       @ s1<- vCC
    GET_VREG_FLOAT_BY_ADDR s0, r2       @ s0<- vBB

    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    fdivs   s2, s0, s1                              @ s2<- op
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_FLOAT s2, r4, lr           @ vAA<- s2
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_div_float

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_rem_float: /* 0xaa */
    NAME_START nterp_op_rem_float

/* EABI doesn't define a float remainder function, but libm does */
    /*
     * Generic 32-bit binary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = r0 op r1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than r0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (r1).  Useful for integer division and modulus.  Note that we
     * *don't* check for (INT_MIN / -1) here, because the ARM math lib
     * handles it correctly.
     *
     * For: add-int, sub-int, mul-int, div-int, rem-int, and-int, or-int,
     *      xor-int, shl-int, shr-int, ushr-int, add-float, sub-float,
     *      mul-float, div-float, rem-float
     */
    /* binop vAA, vBB, vCC */
    FETCH r0, 1                         @ r0<- CCBB
    mov     r4, rINST, lsr #8           @ r4<- AA
    mov     r3, r0, lsr #8              @ r3<- CC
    and     r2, r0, #255                @ r2<- BB
    GET_VREG r1, r3                     @ r1<- vCC
    GET_VREG r0, r2                     @ r0<- vBB
    .if 0
    cmp     r1, #0                      @ is second operand zero?
    beq     common_errDivideByZero
    .endif

    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
                               @ optional op; may set condition codes
    bl      fmodf                              @ r0<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG r0, r4                @ vAA<- r0
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 11-14 instructions */

    NAME_END nterp_op_rem_float

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_add_double: /* 0xab */
    NAME_START nterp_op_add_double

    /*
     * Generic 64-bit double-precision floating point binary operation.
     * Provide an "instr" line that specifies an instruction that performs
     * "d2 = d0 op d1".
     *
     * for: add-double, sub-double, mul-double, div-double
     */
    /* doubleop vAA, vBB, vCC */
    FETCH r0, 1                         @ r0<- CCBB
    mov     r4, rINST, lsr #8           @ r4<- AA
    mov     r3, r0, lsr #8              @ r3<- CC
    and     r2, r0, #255                @ r2<- BB
    VREG_INDEX_TO_ADDR r3, r3           @ r3<- &vCC
    VREG_INDEX_TO_ADDR r2, r2           @ r2<- &vBB
    GET_VREG_DOUBLE_BY_ADDR d1, r3      @ d1<- vCC
    GET_VREG_DOUBLE_BY_ADDR d0, r2      @ d0<- vBB
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    faddd   d2, d0, d1                              @ d2<- op
    CLEAR_SHADOW_PAIR r4, ip, lr        @ Zero shadow regs
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    VREG_INDEX_TO_ADDR r4, r4           @ r4<- &vAA
    SET_VREG_DOUBLE_BY_ADDR d2, r4      @ vAA<- d2
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_add_double

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sub_double: /* 0xac */
    NAME_START nterp_op_sub_double

    /*
     * Generic 64-bit double-precision floating point binary operation.
     * Provide an "instr" line that specifies an instruction that performs
     * "d2 = d0 op d1".
     *
     * for: add-double, sub-double, mul-double, div-double
     */
    /* doubleop vAA, vBB, vCC */
    FETCH r0, 1                         @ r0<- CCBB
    mov     r4, rINST, lsr #8           @ r4<- AA
    mov     r3, r0, lsr #8              @ r3<- CC
    and     r2, r0, #255                @ r2<- BB
    VREG_INDEX_TO_ADDR r3, r3           @ r3<- &vCC
    VREG_INDEX_TO_ADDR r2, r2           @ r2<- &vBB
    GET_VREG_DOUBLE_BY_ADDR d1, r3      @ d1<- vCC
    GET_VREG_DOUBLE_BY_ADDR d0, r2      @ d0<- vBB
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    fsubd   d2, d0, d1                              @ d2<- op
    CLEAR_SHADOW_PAIR r4, ip, lr        @ Zero shadow regs
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    VREG_INDEX_TO_ADDR r4, r4           @ r4<- &vAA
    SET_VREG_DOUBLE_BY_ADDR d2, r4      @ vAA<- d2
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_sub_double

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_mul_double: /* 0xad */
    NAME_START nterp_op_mul_double

    /*
     * Generic 64-bit double-precision floating point binary operation.
     * Provide an "instr" line that specifies an instruction that performs
     * "d2 = d0 op d1".
     *
     * for: add-double, sub-double, mul-double, div-double
     */
    /* doubleop vAA, vBB, vCC */
    FETCH r0, 1                         @ r0<- CCBB
    mov     r4, rINST, lsr #8           @ r4<- AA
    mov     r3, r0, lsr #8              @ r3<- CC
    and     r2, r0, #255                @ r2<- BB
    VREG_INDEX_TO_ADDR r3, r3           @ r3<- &vCC
    VREG_INDEX_TO_ADDR r2, r2           @ r2<- &vBB
    GET_VREG_DOUBLE_BY_ADDR d1, r3      @ d1<- vCC
    GET_VREG_DOUBLE_BY_ADDR d0, r2      @ d0<- vBB
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    fmuld   d2, d0, d1                              @ d2<- op
    CLEAR_SHADOW_PAIR r4, ip, lr        @ Zero shadow regs
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    VREG_INDEX_TO_ADDR r4, r4           @ r4<- &vAA
    SET_VREG_DOUBLE_BY_ADDR d2, r4      @ vAA<- d2
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_mul_double

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_div_double: /* 0xae */
    NAME_START nterp_op_div_double

    /*
     * Generic 64-bit double-precision floating point binary operation.
     * Provide an "instr" line that specifies an instruction that performs
     * "d2 = d0 op d1".
     *
     * for: add-double, sub-double, mul-double, div-double
     */
    /* doubleop vAA, vBB, vCC */
    FETCH r0, 1                         @ r0<- CCBB
    mov     r4, rINST, lsr #8           @ r4<- AA
    mov     r3, r0, lsr #8              @ r3<- CC
    and     r2, r0, #255                @ r2<- BB
    VREG_INDEX_TO_ADDR r3, r3           @ r3<- &vCC
    VREG_INDEX_TO_ADDR r2, r2           @ r2<- &vBB
    GET_VREG_DOUBLE_BY_ADDR d1, r3      @ d1<- vCC
    GET_VREG_DOUBLE_BY_ADDR d0, r2      @ d0<- vBB
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
    fdivd   d2, d0, d1                              @ d2<- op
    CLEAR_SHADOW_PAIR r4, ip, lr        @ Zero shadow regs
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    VREG_INDEX_TO_ADDR r4, r4           @ r4<- &vAA
    SET_VREG_DOUBLE_BY_ADDR d2, r4      @ vAA<- d2
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_div_double

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_rem_double: /* 0xaf */
    NAME_START nterp_op_rem_double

/* EABI doesn't define a double remainder function, but libm does */
    /*
     * Generic 64-bit binary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = r0-r1 op r2-r3".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than r0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (r1).  Useful for integer division and modulus.
     *
     * for: add-long, sub-long, div-long, rem-long, and-long, or-long,
     *      xor-long, add-double, sub-double, mul-double, div-double,
     *      rem-double
     *
     * IMPORTANT: you may specify "chkzero" or "preinstr" but not both.
     */
    /* binop vAA, vBB, vCC */
    FETCH r0, 1                         @ r0<- CCBB
    mov     rINST, rINST, lsr #8        @ rINST<- AA
    and     r2, r0, #255                @ r2<- BB
    mov     r3, r0, lsr #8              @ r3<- CC
    VREG_INDEX_TO_ADDR r4, rINST        @ r4<- &fp[AA]
    VREG_INDEX_TO_ADDR r2, r2           @ r2<- &fp[BB]
    VREG_INDEX_TO_ADDR r3, r3           @ r3<- &fp[CC]
    GET_VREG_WIDE_BY_ADDR r0, r1, r2    @ r0/r1<- vBB/vBB+1
    GET_VREG_WIDE_BY_ADDR r2, r3, r3    @ r2/r3<- vCC/vCC+1
    .if 0
    orrs    ip, r2, r3                  @ second arg (r2-r3) is zero?
    beq     common_errDivideByZero
    .endif
    CLEAR_SHADOW_PAIR rINST, lr, ip     @ Zero out the shadow regs
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST
                               @ optional op; may set condition codes
    bl      fmod                              @ result<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_WIDE_BY_ADDR r0,r1,r4  @ vAA/vAA+1<,  r0/r1
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 14-17 instructions */

    NAME_END nterp_op_rem_double

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_add_int_2addr: /* 0xb0 */
    NAME_START nterp_op_add_int_2addr

    /*
     * Generic 32-bit "/2addr" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = r0 op r1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than r0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (r1).  Useful for integer division and modulus.
     *
     * For: add-int/2addr, sub-int/2addr, mul-int/2addr, div-int/2addr,
     *      rem-int/2addr, and-int/2addr, or-int/2addr, xor-int/2addr,
     *      shl-int/2addr, shr-int/2addr, ushr-int/2addr, add-float/2addr,
     *      sub-float/2addr, mul-float/2addr, div-float/2addr, rem-float/2addr
     */
    /* binop/2addr vA, vB */
    mov     r3, rINST, lsr #12          @ r3<- B
    ubfx    r4, rINST, #8, #4           @ r4<- A
    GET_VREG r1, r3                     @ r1<- vB
    GET_VREG r0, r4                     @ r0<- vA
    .if 0
    cmp     r1, #0                      @ is second operand zero?
    beq     common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST

                               @ optional op; may set condition codes
    add     r0, r0, r1                              @ r0<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG r0, r4                @ vAA<- r0
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 10-13 instructions */

    NAME_END nterp_op_add_int_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sub_int_2addr: /* 0xb1 */
    NAME_START nterp_op_sub_int_2addr

    /*
     * Generic 32-bit "/2addr" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = r0 op r1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than r0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (r1).  Useful for integer division and modulus.
     *
     * For: add-int/2addr, sub-int/2addr, mul-int/2addr, div-int/2addr,
     *      rem-int/2addr, and-int/2addr, or-int/2addr, xor-int/2addr,
     *      shl-int/2addr, shr-int/2addr, ushr-int/2addr, add-float/2addr,
     *      sub-float/2addr, mul-float/2addr, div-float/2addr, rem-float/2addr
     */
    /* binop/2addr vA, vB */
    mov     r3, rINST, lsr #12          @ r3<- B
    ubfx    r4, rINST, #8, #4           @ r4<- A
    GET_VREG r1, r3                     @ r1<- vB
    GET_VREG r0, r4                     @ r0<- vA
    .if 0
    cmp     r1, #0                      @ is second operand zero?
    beq     common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST

                               @ optional op; may set condition codes
    sub     r0, r0, r1                              @ r0<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG r0, r4                @ vAA<- r0
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 10-13 instructions */

    NAME_END nterp_op_sub_int_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_mul_int_2addr: /* 0xb2 */
    NAME_START nterp_op_mul_int_2addr

    /*
     * Generic 32-bit "/2addr" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = r0 op r1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than r0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (r1).  Useful for integer division and modulus.
     *
     * For: add-int/2addr, sub-int/2addr, mul-int/2addr, div-int/2addr,
     *      rem-int/2addr, and-int/2addr, or-int/2addr, xor-int/2addr,
     *      shl-int/2addr, shr-int/2addr, ushr-int/2addr, add-float/2addr,
     *      sub-float/2addr, mul-float/2addr, div-float/2addr, rem-float/2addr
     */
    /* binop/2addr vA, vB */
    mov     r3, rINST, lsr #12          @ r3<- B
    ubfx    r4, rINST, #8, #4           @ r4<- A
    GET_VREG r1, r3                     @ r1<- vB
    GET_VREG r0, r4                     @ r0<- vA
    .if 0
    cmp     r1, #0                      @ is second operand zero?
    beq     common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST

                               @ optional op; may set condition codes
    mul     r0, r1, r0                              @ r0<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG r0, r4                @ vAA<- r0
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 10-13 instructions */

    NAME_END nterp_op_mul_int_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_div_int_2addr: /* 0xb3 */
    NAME_START nterp_op_div_int_2addr

    /*
     * Specialized 32-bit binary operation
     *
     * Performs "r0 = r0 div r1". The selection between sdiv or the gcc helper
     * depends on the compile time value of __ARM_ARCH_EXT_IDIV__ (defined for
     * ARMv7 CPUs that have hardware division support).
     *
     * div-int/2addr
     *
     */
    mov     r3, rINST, lsr #12          @ r3<- B
    ubfx    r4, rINST, #8, #4           @ r4<- A
    GET_VREG r1, r3                     @ r1<- vB
    GET_VREG r0, r4                     @ r0<- vA
    cmp     r1, #0                      @ is second operand zero?
    beq     common_errDivideByZero
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST

#ifdef __ARM_ARCH_EXT_IDIV__
    sdiv    r0, r0, r1                  @ r0<- op
#else
    bl       __aeabi_idiv               @ r0<- op, r0-r3 changed
#endif
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG r0, r4                     @ vAA<- r0
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 10-13 instructions */

    NAME_END nterp_op_div_int_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_rem_int_2addr: /* 0xb4 */
    NAME_START nterp_op_rem_int_2addr

    /*
     * Specialized 32-bit binary operation
     *
     * Performs "r1 = r0 rem r1". The selection between sdiv block or the gcc helper
     * depends on the compile time value of __ARM_ARCH_EXT_IDIV__ (defined for
     * ARMv7 CPUs that have hardware division support).
     *
     * NOTE: idivmod returns quotient in r0 and remainder in r1
     *
     * rem-int/2addr
     *
     */
    mov     r3, rINST, lsr #12          @ r3<- B
    ubfx    r4, rINST, #8, #4           @ r4<- A
    GET_VREG r1, r3                     @ r1<- vB
    GET_VREG r0, r4                     @ r0<- vA
    cmp     r1, #0                      @ is second operand zero?
    beq     common_errDivideByZero
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST

#ifdef __ARM_ARCH_EXT_IDIV__
    sdiv    r2, r0, r1
    mls     r1, r1, r2, r0              @ r1<- op
#else
    bl      __aeabi_idivmod             @ r1<- op, r0-r3 changed
#endif
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG r1, r4                     @ vAA<- r1
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 10-13 instructions */

    NAME_END nterp_op_rem_int_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_and_int_2addr: /* 0xb5 */
    NAME_START nterp_op_and_int_2addr

    /*
     * Generic 32-bit "/2addr" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = r0 op r1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than r0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (r1).  Useful for integer division and modulus.
     *
     * For: add-int/2addr, sub-int/2addr, mul-int/2addr, div-int/2addr,
     *      rem-int/2addr, and-int/2addr, or-int/2addr, xor-int/2addr,
     *      shl-int/2addr, shr-int/2addr, ushr-int/2addr, add-float/2addr,
     *      sub-float/2addr, mul-float/2addr, div-float/2addr, rem-float/2addr
     */
    /* binop/2addr vA, vB */
    mov     r3, rINST, lsr #12          @ r3<- B
    ubfx    r4, rINST, #8, #4           @ r4<- A
    GET_VREG r1, r3                     @ r1<- vB
    GET_VREG r0, r4                     @ r0<- vA
    .if 0
    cmp     r1, #0                      @ is second operand zero?
    beq     common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST

                               @ optional op; may set condition codes
    and     r0, r0, r1                              @ r0<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG r0, r4                @ vAA<- r0
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 10-13 instructions */

    NAME_END nterp_op_and_int_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_or_int_2addr: /* 0xb6 */
    NAME_START nterp_op_or_int_2addr

    /*
     * Generic 32-bit "/2addr" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = r0 op r1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than r0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (r1).  Useful for integer division and modulus.
     *
     * For: add-int/2addr, sub-int/2addr, mul-int/2addr, div-int/2addr,
     *      rem-int/2addr, and-int/2addr, or-int/2addr, xor-int/2addr,
     *      shl-int/2addr, shr-int/2addr, ushr-int/2addr, add-float/2addr,
     *      sub-float/2addr, mul-float/2addr, div-float/2addr, rem-float/2addr
     */
    /* binop/2addr vA, vB */
    mov     r3, rINST, lsr #12          @ r3<- B
    ubfx    r4, rINST, #8, #4           @ r4<- A
    GET_VREG r1, r3                     @ r1<- vB
    GET_VREG r0, r4                     @ r0<- vA
    .if 0
    cmp     r1, #0                      @ is second operand zero?
    beq     common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST

                               @ optional op; may set condition codes
    orr     r0, r0, r1                              @ r0<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG r0, r4                @ vAA<- r0
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 10-13 instructions */

    NAME_END nterp_op_or_int_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_xor_int_2addr: /* 0xb7 */
    NAME_START nterp_op_xor_int_2addr

    /*
     * Generic 32-bit "/2addr" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = r0 op r1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than r0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (r1).  Useful for integer division and modulus.
     *
     * For: add-int/2addr, sub-int/2addr, mul-int/2addr, div-int/2addr,
     *      rem-int/2addr, and-int/2addr, or-int/2addr, xor-int/2addr,
     *      shl-int/2addr, shr-int/2addr, ushr-int/2addr, add-float/2addr,
     *      sub-float/2addr, mul-float/2addr, div-float/2addr, rem-float/2addr
     */
    /* binop/2addr vA, vB */
    mov     r3, rINST, lsr #12          @ r3<- B
    ubfx    r4, rINST, #8, #4           @ r4<- A
    GET_VREG r1, r3                     @ r1<- vB
    GET_VREG r0, r4                     @ r0<- vA
    .if 0
    cmp     r1, #0                      @ is second operand zero?
    beq     common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST

                               @ optional op; may set condition codes
    eor     r0, r0, r1                              @ r0<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG r0, r4                @ vAA<- r0
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 10-13 instructions */

    NAME_END nterp_op_xor_int_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_shl_int_2addr: /* 0xb8 */
    NAME_START nterp_op_shl_int_2addr

    /*
     * Generic 32-bit "/2addr" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = r0 op r1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than r0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (r1).  Useful for integer division and modulus.
     *
     * For: add-int/2addr, sub-int/2addr, mul-int/2addr, div-int/2addr,
     *      rem-int/2addr, and-int/2addr, or-int/2addr, xor-int/2addr,
     *      shl-int/2addr, shr-int/2addr, ushr-int/2addr, add-float/2addr,
     *      sub-float/2addr, mul-float/2addr, div-float/2addr, rem-float/2addr
     */
    /* binop/2addr vA, vB */
    mov     r3, rINST, lsr #12          @ r3<- B
    ubfx    r4, rINST, #8, #4           @ r4<- A
    GET_VREG r1, r3                     @ r1<- vB
    GET_VREG r0, r4                     @ r0<- vA
    .if 0
    cmp     r1, #0                      @ is second operand zero?
    beq     common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST

    and     r1, r1, #31                           @ optional op; may set condition codes
    mov     r0, r0, lsl r1                              @ r0<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG r0, r4                @ vAA<- r0
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 10-13 instructions */

    NAME_END nterp_op_shl_int_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_shr_int_2addr: /* 0xb9 */
    NAME_START nterp_op_shr_int_2addr

    /*
     * Generic 32-bit "/2addr" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = r0 op r1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than r0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (r1).  Useful for integer division and modulus.
     *
     * For: add-int/2addr, sub-int/2addr, mul-int/2addr, div-int/2addr,
     *      rem-int/2addr, and-int/2addr, or-int/2addr, xor-int/2addr,
     *      shl-int/2addr, shr-int/2addr, ushr-int/2addr, add-float/2addr,
     *      sub-float/2addr, mul-float/2addr, div-float/2addr, rem-float/2addr
     */
    /* binop/2addr vA, vB */
    mov     r3, rINST, lsr #12          @ r3<- B
    ubfx    r4, rINST, #8, #4           @ r4<- A
    GET_VREG r1, r3                     @ r1<- vB
    GET_VREG r0, r4                     @ r0<- vA
    .if 0
    cmp     r1, #0                      @ is second operand zero?
    beq     common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST

    and     r1, r1, #31                           @ optional op; may set condition codes
    mov     r0, r0, asr r1                              @ r0<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG r0, r4                @ vAA<- r0
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 10-13 instructions */

    NAME_END nterp_op_shr_int_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_ushr_int_2addr: /* 0xba */
    NAME_START nterp_op_ushr_int_2addr

    /*
     * Generic 32-bit "/2addr" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = r0 op r1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than r0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (r1).  Useful for integer division and modulus.
     *
     * For: add-int/2addr, sub-int/2addr, mul-int/2addr, div-int/2addr,
     *      rem-int/2addr, and-int/2addr, or-int/2addr, xor-int/2addr,
     *      shl-int/2addr, shr-int/2addr, ushr-int/2addr, add-float/2addr,
     *      sub-float/2addr, mul-float/2addr, div-float/2addr, rem-float/2addr
     */
    /* binop/2addr vA, vB */
    mov     r3, rINST, lsr #12          @ r3<- B
    ubfx    r4, rINST, #8, #4           @ r4<- A
    GET_VREG r1, r3                     @ r1<- vB
    GET_VREG r0, r4                     @ r0<- vA
    .if 0
    cmp     r1, #0                      @ is second operand zero?
    beq     common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST

    and     r1, r1, #31                           @ optional op; may set condition codes
    mov     r0, r0, lsr r1                              @ r0<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG r0, r4                @ vAA<- r0
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 10-13 instructions */

    NAME_END nterp_op_ushr_int_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_add_long_2addr: /* 0xbb */
    NAME_START nterp_op_add_long_2addr

    /*
     * Generic 64-bit "/2addr" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = r0-r1 op r2-r3".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than r0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (r1).  Useful for integer division and modulus.
     *
     * For: add-long/2addr, sub-long/2addr, div-long/2addr, rem-long/2addr,
     *      and-long/2addr, or-long/2addr, xor-long/2addr, add-double/2addr,
     *      sub-double/2addr, mul-double/2addr, div-double/2addr,
     *      rem-double/2addr
     */
    /* binop/2addr vA, vB */
    mov     r1, rINST, lsr #12          @ r1<- B
    ubfx    rINST, rINST, #8, #4        @ rINST<- A
    VREG_INDEX_TO_ADDR r1, r1           @ r1<- &fp[B]
    VREG_INDEX_TO_ADDR r4, rINST        @ r4<- &fp[A]
    GET_VREG_WIDE_BY_ADDR r2, r3, r1    @ r2/r3<- vBB/vBB+1
    GET_VREG_WIDE_BY_ADDR r0, r1, r4    @ r0/r1<- vAA/vAA+1
    .if 0
    orrs    ip, r2, r3                  @ second arg (r2-r3) is zero?
    beq     common_errDivideByZero
    .endif
    CLEAR_SHADOW_PAIR rINST, ip, lr     @ Zero shadow regs
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST
    adds    r0, r0, r2                           @ optional op; may set condition codes
    adc     r1, r1, r3                              @ result<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_WIDE_BY_ADDR r0,r1,r4  @ vAA/vAA+1<- r0/r1
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 12-15 instructions */

    NAME_END nterp_op_add_long_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sub_long_2addr: /* 0xbc */
    NAME_START nterp_op_sub_long_2addr

    /*
     * Generic 64-bit "/2addr" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = r0-r1 op r2-r3".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than r0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (r1).  Useful for integer division and modulus.
     *
     * For: add-long/2addr, sub-long/2addr, div-long/2addr, rem-long/2addr,
     *      and-long/2addr, or-long/2addr, xor-long/2addr, add-double/2addr,
     *      sub-double/2addr, mul-double/2addr, div-double/2addr,
     *      rem-double/2addr
     */
    /* binop/2addr vA, vB */
    mov     r1, rINST, lsr #12          @ r1<- B
    ubfx    rINST, rINST, #8, #4        @ rINST<- A
    VREG_INDEX_TO_ADDR r1, r1           @ r1<- &fp[B]
    VREG_INDEX_TO_ADDR r4, rINST        @ r4<- &fp[A]
    GET_VREG_WIDE_BY_ADDR r2, r3, r1    @ r2/r3<- vBB/vBB+1
    GET_VREG_WIDE_BY_ADDR r0, r1, r4    @ r0/r1<- vAA/vAA+1
    .if 0
    orrs    ip, r2, r3                  @ second arg (r2-r3) is zero?
    beq     common_errDivideByZero
    .endif
    CLEAR_SHADOW_PAIR rINST, ip, lr     @ Zero shadow regs
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST
    subs    r0, r0, r2                           @ optional op; may set condition codes
    sbc     r1, r1, r3                              @ result<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_WIDE_BY_ADDR r0,r1,r4  @ vAA/vAA+1<- r0/r1
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 12-15 instructions */

    NAME_END nterp_op_sub_long_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_mul_long_2addr: /* 0xbd */
    NAME_START nterp_op_mul_long_2addr

    /*
     * Signed 64-bit integer multiply, "/2addr" version.
     *
     * See op_mul_long for an explanation.
     *
     * We get a little tight on registers, so to avoid looking up &fp[A]
     * again we stuff it into rINST.
     */
    /* mul-long/2addr vA, vB */
    mov     r1, rINST, lsr #12          @ r1<- B
    ubfx    r4, rINST, #8, #4           @ r4<- A
    VREG_INDEX_TO_ADDR r1, r1           @ r1<- &fp[B]
    VREG_INDEX_TO_ADDR rINST, r4        @ rINST<- &fp[A]
    GET_VREG_WIDE_BY_ADDR r2, r3, r1    @ r2/r3<- vBB/vBB+1
    GET_VREG_WIDE_BY_ADDR r0, r1, rINST @ r0/r1<- vAA/vAA+1
    mul     ip, r0, r3                  @ ip<- YxX
    umull   r0, lr, r2, r0              @ r0/lr <- ZxX RdLo == Rn - this is OK.
    mla     r3, r1, r2, ip              @ r3<- YxX + (ZxW)
    mov     r4, rINST                   @ Save vAA before FETCH_ADVANCE_INST
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST
    add     r1, r3, lr                  @ r1<- lr + low(ZxW + (YxX))
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_WIDE_BY_ADDR r0, r1, r4    @ vAA/vAA+1<- r0/r1
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_mul_long_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_div_long_2addr: /* 0xbe */
    NAME_START nterp_op_div_long_2addr

    /*
     * Generic 64-bit "/2addr" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = r0-r1 op r2-r3".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than r0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (r1).  Useful for integer division and modulus.
     *
     * For: add-long/2addr, sub-long/2addr, div-long/2addr, rem-long/2addr,
     *      and-long/2addr, or-long/2addr, xor-long/2addr, add-double/2addr,
     *      sub-double/2addr, mul-double/2addr, div-double/2addr,
     *      rem-double/2addr
     */
    /* binop/2addr vA, vB */
    mov     r1, rINST, lsr #12          @ r1<- B
    ubfx    rINST, rINST, #8, #4        @ rINST<- A
    VREG_INDEX_TO_ADDR r1, r1           @ r1<- &fp[B]
    VREG_INDEX_TO_ADDR r4, rINST        @ r4<- &fp[A]
    GET_VREG_WIDE_BY_ADDR r2, r3, r1    @ r2/r3<- vBB/vBB+1
    GET_VREG_WIDE_BY_ADDR r0, r1, r4    @ r0/r1<- vAA/vAA+1
    .if 1
    orrs    ip, r2, r3                  @ second arg (r2-r3) is zero?
    beq     common_errDivideByZero
    .endif
    CLEAR_SHADOW_PAIR rINST, ip, lr     @ Zero shadow regs
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST
                               @ optional op; may set condition codes
    bl      __aeabi_ldivmod                              @ result<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_WIDE_BY_ADDR r0,r1,r4  @ vAA/vAA+1<- r0/r1
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 12-15 instructions */

    NAME_END nterp_op_div_long_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_rem_long_2addr: /* 0xbf */
    NAME_START nterp_op_rem_long_2addr

/* ldivmod returns quotient in r0/r1 and remainder in r2/r3 */
    /*
     * Generic 64-bit "/2addr" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = r0-r1 op r2-r3".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than r0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (r1).  Useful for integer division and modulus.
     *
     * For: add-long/2addr, sub-long/2addr, div-long/2addr, rem-long/2addr,
     *      and-long/2addr, or-long/2addr, xor-long/2addr, add-double/2addr,
     *      sub-double/2addr, mul-double/2addr, div-double/2addr,
     *      rem-double/2addr
     */
    /* binop/2addr vA, vB */
    mov     r1, rINST, lsr #12          @ r1<- B
    ubfx    rINST, rINST, #8, #4        @ rINST<- A
    VREG_INDEX_TO_ADDR r1, r1           @ r1<- &fp[B]
    VREG_INDEX_TO_ADDR r4, rINST        @ r4<- &fp[A]
    GET_VREG_WIDE_BY_ADDR r2, r3, r1    @ r2/r3<- vBB/vBB+1
    GET_VREG_WIDE_BY_ADDR r0, r1, r4    @ r0/r1<- vAA/vAA+1
    .if 1
    orrs    ip, r2, r3                  @ second arg (r2-r3) is zero?
    beq     common_errDivideByZero
    .endif
    CLEAR_SHADOW_PAIR rINST, ip, lr     @ Zero shadow regs
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST
                               @ optional op; may set condition codes
    bl      __aeabi_ldivmod                              @ result<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_WIDE_BY_ADDR r2,r3,r4  @ vAA/vAA+1<- r2/r3
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 12-15 instructions */

    NAME_END nterp_op_rem_long_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_and_long_2addr: /* 0xc0 */
    NAME_START nterp_op_and_long_2addr

    /*
     * Generic 64-bit "/2addr" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = r0-r1 op r2-r3".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than r0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (r1).  Useful for integer division and modulus.
     *
     * For: add-long/2addr, sub-long/2addr, div-long/2addr, rem-long/2addr,
     *      and-long/2addr, or-long/2addr, xor-long/2addr, add-double/2addr,
     *      sub-double/2addr, mul-double/2addr, div-double/2addr,
     *      rem-double/2addr
     */
    /* binop/2addr vA, vB */
    mov     r1, rINST, lsr #12          @ r1<- B
    ubfx    rINST, rINST, #8, #4        @ rINST<- A
    VREG_INDEX_TO_ADDR r1, r1           @ r1<- &fp[B]
    VREG_INDEX_TO_ADDR r4, rINST        @ r4<- &fp[A]
    GET_VREG_WIDE_BY_ADDR r2, r3, r1    @ r2/r3<- vBB/vBB+1
    GET_VREG_WIDE_BY_ADDR r0, r1, r4    @ r0/r1<- vAA/vAA+1
    .if 0
    orrs    ip, r2, r3                  @ second arg (r2-r3) is zero?
    beq     common_errDivideByZero
    .endif
    CLEAR_SHADOW_PAIR rINST, ip, lr     @ Zero shadow regs
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST
    and     r0, r0, r2                           @ optional op; may set condition codes
    and     r1, r1, r3                              @ result<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_WIDE_BY_ADDR r0,r1,r4  @ vAA/vAA+1<- r0/r1
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 12-15 instructions */

    NAME_END nterp_op_and_long_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_or_long_2addr: /* 0xc1 */
    NAME_START nterp_op_or_long_2addr

    /*
     * Generic 64-bit "/2addr" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = r0-r1 op r2-r3".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than r0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (r1).  Useful for integer division and modulus.
     *
     * For: add-long/2addr, sub-long/2addr, div-long/2addr, rem-long/2addr,
     *      and-long/2addr, or-long/2addr, xor-long/2addr, add-double/2addr,
     *      sub-double/2addr, mul-double/2addr, div-double/2addr,
     *      rem-double/2addr
     */
    /* binop/2addr vA, vB */
    mov     r1, rINST, lsr #12          @ r1<- B
    ubfx    rINST, rINST, #8, #4        @ rINST<- A
    VREG_INDEX_TO_ADDR r1, r1           @ r1<- &fp[B]
    VREG_INDEX_TO_ADDR r4, rINST        @ r4<- &fp[A]
    GET_VREG_WIDE_BY_ADDR r2, r3, r1    @ r2/r3<- vBB/vBB+1
    GET_VREG_WIDE_BY_ADDR r0, r1, r4    @ r0/r1<- vAA/vAA+1
    .if 0
    orrs    ip, r2, r3                  @ second arg (r2-r3) is zero?
    beq     common_errDivideByZero
    .endif
    CLEAR_SHADOW_PAIR rINST, ip, lr     @ Zero shadow regs
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST
    orr     r0, r0, r2                           @ optional op; may set condition codes
    orr     r1, r1, r3                              @ result<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_WIDE_BY_ADDR r0,r1,r4  @ vAA/vAA+1<- r0/r1
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 12-15 instructions */

    NAME_END nterp_op_or_long_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_xor_long_2addr: /* 0xc2 */
    NAME_START nterp_op_xor_long_2addr

    /*
     * Generic 64-bit "/2addr" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = r0-r1 op r2-r3".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than r0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (r1).  Useful for integer division and modulus.
     *
     * For: add-long/2addr, sub-long/2addr, div-long/2addr, rem-long/2addr,
     *      and-long/2addr, or-long/2addr, xor-long/2addr, add-double/2addr,
     *      sub-double/2addr, mul-double/2addr, div-double/2addr,
     *      rem-double/2addr
     */
    /* binop/2addr vA, vB */
    mov     r1, rINST, lsr #12          @ r1<- B
    ubfx    rINST, rINST, #8, #4        @ rINST<- A
    VREG_INDEX_TO_ADDR r1, r1           @ r1<- &fp[B]
    VREG_INDEX_TO_ADDR r4, rINST        @ r4<- &fp[A]
    GET_VREG_WIDE_BY_ADDR r2, r3, r1    @ r2/r3<- vBB/vBB+1
    GET_VREG_WIDE_BY_ADDR r0, r1, r4    @ r0/r1<- vAA/vAA+1
    .if 0
    orrs    ip, r2, r3                  @ second arg (r2-r3) is zero?
    beq     common_errDivideByZero
    .endif
    CLEAR_SHADOW_PAIR rINST, ip, lr     @ Zero shadow regs
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST
    eor     r0, r0, r2                           @ optional op; may set condition codes
    eor     r1, r1, r3                              @ result<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_WIDE_BY_ADDR r0,r1,r4  @ vAA/vAA+1<- r0/r1
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 12-15 instructions */

    NAME_END nterp_op_xor_long_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_shl_long_2addr: /* 0xc3 */
    NAME_START nterp_op_shl_long_2addr

    /*
     * Long integer shift, 2addr version.  vA is 64-bit value/result, vB is
     * 32-bit shift distance.
     */
    /* shl-long/2addr vA, vB */
    mov     r3, rINST, lsr #12          @ r3<- B
    ubfx    r4, rINST, #8, #4           @ r4<- A
    GET_VREG r2, r3                     @ r2<- vB
    CLEAR_SHADOW_PAIR r4, lr, ip        @ Zero out the shadow regs
    VREG_INDEX_TO_ADDR r4, r4           @ r4<- &fp[A]
    and     r2, r2, #63                 @ r2<- r2 & 0x3f
    GET_VREG_WIDE_BY_ADDR r0, r1, r4    @ r0/r1<- vAA/vAA+1
    mov     r1, r1, asl r2              @ r1<- r1 << r2
    rsb     r3, r2, #32                 @ r3<- 32 - r2
    orr     r1, r1, r0, lsr r3          @ r1<- r1 | (r0 << (32-r2))
    subs    ip, r2, #32                 @ ip<- r2 - 32
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST
    it      pl
    movpl   r1, r0, asl ip              @ if r2 >= 32, r1<- r0 << (r2-32)
    mov     r0, r0, asl r2              @ r0<- r0 << r2
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_WIDE_BY_ADDR r0, r1, r4    @ vAA/vAA+1<- r0/r1
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_shl_long_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_shr_long_2addr: /* 0xc4 */
    NAME_START nterp_op_shr_long_2addr

    /*
     * Long integer shift, 2addr version.  vA is 64-bit value/result, vB is
     * 32-bit shift distance.
     */
    /* shr-long/2addr vA, vB */
    mov     r3, rINST, lsr #12          @ r3<- B
    ubfx    r4, rINST, #8, #4           @ r4<- A
    GET_VREG r2, r3                     @ r2<- vB
    CLEAR_SHADOW_PAIR r4, lr, ip        @ Zero out the shadow regs
    VREG_INDEX_TO_ADDR r4, r4           @ r4<- &fp[A]
    and     r2, r2, #63                 @ r2<- r2 & 0x3f
    GET_VREG_WIDE_BY_ADDR r0, r1, r4    @ r0/r1<- vAA/vAA+1
    mov     r0, r0, lsr r2              @ r0<- r2 >> r2
    rsb     r3, r2, #32                 @ r3<- 32 - r2
    orr     r0, r0, r1, lsl r3          @ r0<- r0 | (r1 << (32-r2))
    subs    ip, r2, #32                 @ ip<- r2 - 32
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST
    it      pl
    movpl   r0, r1, asr ip              @ if r2 >= 32, r0<-r1 >> (r2-32)
    mov     r1, r1, asr r2              @ r1<- r1 >> r2
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_WIDE_BY_ADDR r0, r1, r4    @ vAA/vAA+1<- r0/r1
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_shr_long_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_ushr_long_2addr: /* 0xc5 */
    NAME_START nterp_op_ushr_long_2addr

    /*
     * Long integer shift, 2addr version.  vA is 64-bit value/result, vB is
     * 32-bit shift distance.
     */
    /* ushr-long/2addr vA, vB */
    mov     r3, rINST, lsr #12          @ r3<- B
    ubfx    r4, rINST, #8, #4           @ r4<- A
    GET_VREG r2, r3                     @ r2<- vB
    CLEAR_SHADOW_PAIR r4, lr, ip        @ Zero out the shadow regs
    VREG_INDEX_TO_ADDR r4, r4           @ r4<- &fp[A]
    and     r2, r2, #63                 @ r2<- r2 & 0x3f
    GET_VREG_WIDE_BY_ADDR r0, r1, r4    @ r0/r1<- vAA/vAA+1
    mov     r0, r0, lsr r2              @ r0<- r2 >> r2
    rsb     r3, r2, #32                 @ r3<- 32 - r2
    orr     r0, r0, r1, lsl r3          @ r0<- r0 | (r1 << (32-r2))
    subs    ip, r2, #32                 @ ip<- r2 - 32
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST
    it      pl
    movpl   r0, r1, lsr ip              @ if r2 >= 32, r0<-r1 >>> (r2-32)
    mov     r1, r1, lsr r2              @ r1<- r1 >>> r2
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_WIDE_BY_ADDR r0, r1, r4    @ vAA/vAA+1<- r0/r1
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_ushr_long_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_add_float_2addr: /* 0xc6 */
    NAME_START nterp_op_add_float_2addr

    /*
     * Generic 32-bit floating point "/2addr" binary operation.  Provide
     * an "instr" line that specifies an instruction that performs
     * "s2 = s0 op s1".
     *
     * For: add-float/2addr, sub-float/2addr, mul-float/2addr, div-float/2addr
     */
    /* binop/2addr vA, vB */
    mov     r3, rINST, lsr #12          @ r3<- B
    ubfx    r4, rINST, #8, #4           @ r4<- A
    VREG_INDEX_TO_ADDR r3, r3           @ r3<- &vB
    VREG_INDEX_TO_ADDR r4, r4           @ r4<- &vA
    GET_VREG_FLOAT_BY_ADDR s1, r3       @ s1<- vB
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST
    GET_VREG_FLOAT_BY_ADDR s0, r4       @ s0<- vA
    fadds   s2, s0, s1                              @ s2<- op
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_FLOAT_BY_ADDR s2, r4       @ vAA<- s2 No need to clear as it's 2addr
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_add_float_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sub_float_2addr: /* 0xc7 */
    NAME_START nterp_op_sub_float_2addr

    /*
     * Generic 32-bit floating point "/2addr" binary operation.  Provide
     * an "instr" line that specifies an instruction that performs
     * "s2 = s0 op s1".
     *
     * For: add-float/2addr, sub-float/2addr, mul-float/2addr, div-float/2addr
     */
    /* binop/2addr vA, vB */
    mov     r3, rINST, lsr #12          @ r3<- B
    ubfx    r4, rINST, #8, #4           @ r4<- A
    VREG_INDEX_TO_ADDR r3, r3           @ r3<- &vB
    VREG_INDEX_TO_ADDR r4, r4           @ r4<- &vA
    GET_VREG_FLOAT_BY_ADDR s1, r3       @ s1<- vB
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST
    GET_VREG_FLOAT_BY_ADDR s0, r4       @ s0<- vA
    fsubs   s2, s0, s1                              @ s2<- op
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_FLOAT_BY_ADDR s2, r4       @ vAA<- s2 No need to clear as it's 2addr
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_sub_float_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_mul_float_2addr: /* 0xc8 */
    NAME_START nterp_op_mul_float_2addr

    /*
     * Generic 32-bit floating point "/2addr" binary operation.  Provide
     * an "instr" line that specifies an instruction that performs
     * "s2 = s0 op s1".
     *
     * For: add-float/2addr, sub-float/2addr, mul-float/2addr, div-float/2addr
     */
    /* binop/2addr vA, vB */
    mov     r3, rINST, lsr #12          @ r3<- B
    ubfx    r4, rINST, #8, #4           @ r4<- A
    VREG_INDEX_TO_ADDR r3, r3           @ r3<- &vB
    VREG_INDEX_TO_ADDR r4, r4           @ r4<- &vA
    GET_VREG_FLOAT_BY_ADDR s1, r3       @ s1<- vB
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST
    GET_VREG_FLOAT_BY_ADDR s0, r4       @ s0<- vA
    fmuls   s2, s0, s1                              @ s2<- op
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_FLOAT_BY_ADDR s2, r4       @ vAA<- s2 No need to clear as it's 2addr
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_mul_float_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_div_float_2addr: /* 0xc9 */
    NAME_START nterp_op_div_float_2addr

    /*
     * Generic 32-bit floating point "/2addr" binary operation.  Provide
     * an "instr" line that specifies an instruction that performs
     * "s2 = s0 op s1".
     *
     * For: add-float/2addr, sub-float/2addr, mul-float/2addr, div-float/2addr
     */
    /* binop/2addr vA, vB */
    mov     r3, rINST, lsr #12          @ r3<- B
    ubfx    r4, rINST, #8, #4           @ r4<- A
    VREG_INDEX_TO_ADDR r3, r3           @ r3<- &vB
    VREG_INDEX_TO_ADDR r4, r4           @ r4<- &vA
    GET_VREG_FLOAT_BY_ADDR s1, r3       @ s1<- vB
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST
    GET_VREG_FLOAT_BY_ADDR s0, r4       @ s0<- vA
    fdivs   s2, s0, s1                              @ s2<- op
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_FLOAT_BY_ADDR s2, r4       @ vAA<- s2 No need to clear as it's 2addr
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_div_float_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_rem_float_2addr: /* 0xca */
    NAME_START nterp_op_rem_float_2addr

/* EABI doesn't define a float remainder function, but libm does */
    /*
     * Generic 32-bit "/2addr" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = r0 op r1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than r0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (r1).  Useful for integer division and modulus.
     *
     * For: add-int/2addr, sub-int/2addr, mul-int/2addr, div-int/2addr,
     *      rem-int/2addr, and-int/2addr, or-int/2addr, xor-int/2addr,
     *      shl-int/2addr, shr-int/2addr, ushr-int/2addr, add-float/2addr,
     *      sub-float/2addr, mul-float/2addr, div-float/2addr, rem-float/2addr
     */
    /* binop/2addr vA, vB */
    mov     r3, rINST, lsr #12          @ r3<- B
    ubfx    r4, rINST, #8, #4           @ r4<- A
    GET_VREG r1, r3                     @ r1<- vB
    GET_VREG r0, r4                     @ r0<- vA
    .if 0
    cmp     r1, #0                      @ is second operand zero?
    beq     common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST

                               @ optional op; may set condition codes
    bl      fmodf                              @ r0<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG r0, r4                @ vAA<- r0
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 10-13 instructions */

    NAME_END nterp_op_rem_float_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_add_double_2addr: /* 0xcb */
    NAME_START nterp_op_add_double_2addr

    /*
     * Generic 64-bit floating point "/2addr" binary operation.  Provide
     * an "instr" line that specifies an instruction that performs
     * "d2 = d0 op d1".
     *
     * For: add-double/2addr, sub-double/2addr, mul-double/2addr,
     *      div-double/2addr
     */
    /* binop/2addr vA, vB */
    mov     r3, rINST, lsr #12          @ r3<- B
    ubfx    r4, rINST, #8, #4           @ r4<- A
    VREG_INDEX_TO_ADDR r3, r3           @ r3<- &vB
    CLEAR_SHADOW_PAIR r4, ip, r0        @ Zero out shadow regs
    GET_VREG_DOUBLE_BY_ADDR d1, r3      @ d1<- vB
    VREG_INDEX_TO_ADDR r4, r4           @ r4<- &vA
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST
    GET_VREG_DOUBLE_BY_ADDR d0, r4      @ d0<- vA
    faddd   d2, d0, d1                              @ d2<- op
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_DOUBLE_BY_ADDR d2, r4      @ vAA<- d2
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_add_double_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sub_double_2addr: /* 0xcc */
    NAME_START nterp_op_sub_double_2addr

    /*
     * Generic 64-bit floating point "/2addr" binary operation.  Provide
     * an "instr" line that specifies an instruction that performs
     * "d2 = d0 op d1".
     *
     * For: add-double/2addr, sub-double/2addr, mul-double/2addr,
     *      div-double/2addr
     */
    /* binop/2addr vA, vB */
    mov     r3, rINST, lsr #12          @ r3<- B
    ubfx    r4, rINST, #8, #4           @ r4<- A
    VREG_INDEX_TO_ADDR r3, r3           @ r3<- &vB
    CLEAR_SHADOW_PAIR r4, ip, r0        @ Zero out shadow regs
    GET_VREG_DOUBLE_BY_ADDR d1, r3      @ d1<- vB
    VREG_INDEX_TO_ADDR r4, r4           @ r4<- &vA
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST
    GET_VREG_DOUBLE_BY_ADDR d0, r4      @ d0<- vA
    fsubd   d2, d0, d1                              @ d2<- op
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_DOUBLE_BY_ADDR d2, r4      @ vAA<- d2
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_sub_double_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_mul_double_2addr: /* 0xcd */
    NAME_START nterp_op_mul_double_2addr

    /*
     * Generic 64-bit floating point "/2addr" binary operation.  Provide
     * an "instr" line that specifies an instruction that performs
     * "d2 = d0 op d1".
     *
     * For: add-double/2addr, sub-double/2addr, mul-double/2addr,
     *      div-double/2addr
     */
    /* binop/2addr vA, vB */
    mov     r3, rINST, lsr #12          @ r3<- B
    ubfx    r4, rINST, #8, #4           @ r4<- A
    VREG_INDEX_TO_ADDR r3, r3           @ r3<- &vB
    CLEAR_SHADOW_PAIR r4, ip, r0        @ Zero out shadow regs
    GET_VREG_DOUBLE_BY_ADDR d1, r3      @ d1<- vB
    VREG_INDEX_TO_ADDR r4, r4           @ r4<- &vA
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST
    GET_VREG_DOUBLE_BY_ADDR d0, r4      @ d0<- vA
    fmuld   d2, d0, d1                              @ d2<- op
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_DOUBLE_BY_ADDR d2, r4      @ vAA<- d2
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_mul_double_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_div_double_2addr: /* 0xce */
    NAME_START nterp_op_div_double_2addr

    /*
     * Generic 64-bit floating point "/2addr" binary operation.  Provide
     * an "instr" line that specifies an instruction that performs
     * "d2 = d0 op d1".
     *
     * For: add-double/2addr, sub-double/2addr, mul-double/2addr,
     *      div-double/2addr
     */
    /* binop/2addr vA, vB */
    mov     r3, rINST, lsr #12          @ r3<- B
    ubfx    r4, rINST, #8, #4           @ r4<- A
    VREG_INDEX_TO_ADDR r3, r3           @ r3<- &vB
    CLEAR_SHADOW_PAIR r4, ip, r0        @ Zero out shadow regs
    GET_VREG_DOUBLE_BY_ADDR d1, r3      @ d1<- vB
    VREG_INDEX_TO_ADDR r4, r4           @ r4<- &vA
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST
    GET_VREG_DOUBLE_BY_ADDR d0, r4      @ d0<- vA
    fdivd   d2, d0, d1                              @ d2<- op
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_DOUBLE_BY_ADDR d2, r4      @ vAA<- d2
    GOTO_OPCODE ip                      @ jump to next instruction

    NAME_END nterp_op_div_double_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_rem_double_2addr: /* 0xcf */
    NAME_START nterp_op_rem_double_2addr

/* EABI doesn't define a double remainder function, but libm does */
    /*
     * Generic 64-bit "/2addr" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = r0-r1 op r2-r3".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than r0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (r1).  Useful for integer division and modulus.
     *
     * For: add-long/2addr, sub-long/2addr, div-long/2addr, rem-long/2addr,
     *      and-long/2addr, or-long/2addr, xor-long/2addr, add-double/2addr,
     *      sub-double/2addr, mul-double/2addr, div-double/2addr,
     *      rem-double/2addr
     */
    /* binop/2addr vA, vB */
    mov     r1, rINST, lsr #12          @ r1<- B
    ubfx    rINST, rINST, #8, #4        @ rINST<- A
    VREG_INDEX_TO_ADDR r1, r1           @ r1<- &fp[B]
    VREG_INDEX_TO_ADDR r4, rINST        @ r4<- &fp[A]
    GET_VREG_WIDE_BY_ADDR r2, r3, r1    @ r2/r3<- vBB/vBB+1
    GET_VREG_WIDE_BY_ADDR r0, r1, r4    @ r0/r1<- vAA/vAA+1
    .if 0
    orrs    ip, r2, r3                  @ second arg (r2-r3) is zero?
    beq     common_errDivideByZero
    .endif
    CLEAR_SHADOW_PAIR rINST, ip, lr     @ Zero shadow regs
    FETCH_ADVANCE_INST 1                @ advance rPC, load rINST
                               @ optional op; may set condition codes
    bl      fmod                              @ result<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG_WIDE_BY_ADDR r0,r1,r4  @ vAA/vAA+1<- r0/r1
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 12-15 instructions */

    NAME_END nterp_op_rem_double_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_add_int_lit16: /* 0xd0 */
    NAME_START nterp_op_add_int_lit16

    /*
     * Generic 32-bit "lit16" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = r0 op r1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than r0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (r1).  Useful for integer division and modulus.
     *
     * For: add-int/lit16, rsub-int, mul-int/lit16, div-int/lit16,
     *      rem-int/lit16, and-int/lit16, or-int/lit16, xor-int/lit16
     */
    /* binop/lit16 vA, vB, #+CCCC */
    FETCH_S r1, 1                       @ r1<- ssssCCCC (sign-extended)
    mov     r2, rINST, lsr #12          @ r2<- B
    ubfx    r4, rINST, #8, #4           @ r4<- A
    GET_VREG r0, r2                     @ r0<- vB
    .if 0
    cmp     r1, #0                      @ is second operand zero?
    beq     common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST

    add     r0, r0, r1                              @ r0<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG r0, r4                @ vAA<- r0
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 10-13 instructions */

    NAME_END nterp_op_add_int_lit16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_rsub_int: /* 0xd1 */
    NAME_START nterp_op_rsub_int

/* this op is "rsub-int", but can be thought of as "rsub-int/lit16" */
    /*
     * Generic 32-bit "lit16" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = r0 op r1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than r0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (r1).  Useful for integer division and modulus.
     *
     * For: add-int/lit16, rsub-int, mul-int/lit16, div-int/lit16,
     *      rem-int/lit16, and-int/lit16, or-int/lit16, xor-int/lit16
     */
    /* binop/lit16 vA, vB, #+CCCC */
    FETCH_S r1, 1                       @ r1<- ssssCCCC (sign-extended)
    mov     r2, rINST, lsr #12          @ r2<- B
    ubfx    r4, rINST, #8, #4           @ r4<- A
    GET_VREG r0, r2                     @ r0<- vB
    .if 0
    cmp     r1, #0                      @ is second operand zero?
    beq     common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST

    rsb     r0, r0, r1                              @ r0<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG r0, r4                @ vAA<- r0
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 10-13 instructions */

    NAME_END nterp_op_rsub_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_mul_int_lit16: /* 0xd2 */
    NAME_START nterp_op_mul_int_lit16

    /*
     * Generic 32-bit "lit16" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = r0 op r1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than r0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (r1).  Useful for integer division and modulus.
     *
     * For: add-int/lit16, rsub-int, mul-int/lit16, div-int/lit16,
     *      rem-int/lit16, and-int/lit16, or-int/lit16, xor-int/lit16
     */
    /* binop/lit16 vA, vB, #+CCCC */
    FETCH_S r1, 1                       @ r1<- ssssCCCC (sign-extended)
    mov     r2, rINST, lsr #12          @ r2<- B
    ubfx    r4, rINST, #8, #4           @ r4<- A
    GET_VREG r0, r2                     @ r0<- vB
    .if 0
    cmp     r1, #0                      @ is second operand zero?
    beq     common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST

    mul     r0, r1, r0                              @ r0<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG r0, r4                @ vAA<- r0
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 10-13 instructions */

    NAME_END nterp_op_mul_int_lit16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_div_int_lit16: /* 0xd3 */
    NAME_START nterp_op_div_int_lit16

    /*
     * Specialized 32-bit binary operation
     *
     * Performs "r0 = r0 div r1". The selection between sdiv or the gcc helper
     * depends on the compile time value of __ARM_ARCH_EXT_IDIV__ (defined for
     * ARMv7 CPUs that have hardware division support).
     *
     * div-int/lit16
     *
     */
    FETCH_S r1, 1                       @ r1<- ssssCCCC (sign-extended)
    mov     r2, rINST, lsr #12          @ r2<- B
    ubfx    r4, rINST, #8, #4           @ r4<- A
    GET_VREG r0, r2                     @ r0<- vB
    cmp     r1, #0                      @ is second operand zero?
    beq     common_errDivideByZero
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST

#ifdef __ARM_ARCH_EXT_IDIV__
    sdiv    r0, r0, r1                  @ r0<- op
#else
    bl       __aeabi_idiv               @ r0<- op, r0-r3 changed
#endif
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG r0, r4                     @ vAA<- r0
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 10-13 instructions */

    NAME_END nterp_op_div_int_lit16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_rem_int_lit16: /* 0xd4 */
    NAME_START nterp_op_rem_int_lit16

    /*
     * Specialized 32-bit binary operation
     *
     * Performs "r1 = r0 rem r1". The selection between sdiv block or the gcc helper
     * depends on the compile time value of __ARM_ARCH_EXT_IDIV__ (defined for
     * ARMv7 CPUs that have hardware division support).
     *
     * NOTE: idivmod returns quotient in r0 and remainder in r1
     *
     * rem-int/lit16
     *
     */
    FETCH_S r1, 1                       @ r1<- ssssCCCC (sign-extended)
    mov     r2, rINST, lsr #12          @ r2<- B
    ubfx    r4, rINST, #8, #4           @ r4<- A
    GET_VREG r0, r2                     @ r0<- vB
    cmp     r1, #0                      @ is second operand zero?
    beq     common_errDivideByZero
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST

#ifdef __ARM_ARCH_EXT_IDIV__
    sdiv    r2, r0, r1
    mls     r1, r1, r2, r0              @ r1<- op
#else
    bl     __aeabi_idivmod              @ r1<- op, r0-r3 changed
#endif
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG r1, r4                     @ vAA<- r1
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 10-13 instructions */

    NAME_END nterp_op_rem_int_lit16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_and_int_lit16: /* 0xd5 */
    NAME_START nterp_op_and_int_lit16

    /*
     * Generic 32-bit "lit16" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = r0 op r1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than r0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (r1).  Useful for integer division and modulus.
     *
     * For: add-int/lit16, rsub-int, mul-int/lit16, div-int/lit16,
     *      rem-int/lit16, and-int/lit16, or-int/lit16, xor-int/lit16
     */
    /* binop/lit16 vA, vB, #+CCCC */
    FETCH_S r1, 1                       @ r1<- ssssCCCC (sign-extended)
    mov     r2, rINST, lsr #12          @ r2<- B
    ubfx    r4, rINST, #8, #4           @ r4<- A
    GET_VREG r0, r2                     @ r0<- vB
    .if 0
    cmp     r1, #0                      @ is second operand zero?
    beq     common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST

    and     r0, r0, r1                              @ r0<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG r0, r4                @ vAA<- r0
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 10-13 instructions */

    NAME_END nterp_op_and_int_lit16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_or_int_lit16: /* 0xd6 */
    NAME_START nterp_op_or_int_lit16

    /*
     * Generic 32-bit "lit16" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = r0 op r1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than r0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (r1).  Useful for integer division and modulus.
     *
     * For: add-int/lit16, rsub-int, mul-int/lit16, div-int/lit16,
     *      rem-int/lit16, and-int/lit16, or-int/lit16, xor-int/lit16
     */
    /* binop/lit16 vA, vB, #+CCCC */
    FETCH_S r1, 1                       @ r1<- ssssCCCC (sign-extended)
    mov     r2, rINST, lsr #12          @ r2<- B
    ubfx    r4, rINST, #8, #4           @ r4<- A
    GET_VREG r0, r2                     @ r0<- vB
    .if 0
    cmp     r1, #0                      @ is second operand zero?
    beq     common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST

    orr     r0, r0, r1                              @ r0<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG r0, r4                @ vAA<- r0
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 10-13 instructions */

    NAME_END nterp_op_or_int_lit16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_xor_int_lit16: /* 0xd7 */
    NAME_START nterp_op_xor_int_lit16

    /*
     * Generic 32-bit "lit16" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = r0 op r1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than r0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (r1).  Useful for integer division and modulus.
     *
     * For: add-int/lit16, rsub-int, mul-int/lit16, div-int/lit16,
     *      rem-int/lit16, and-int/lit16, or-int/lit16, xor-int/lit16
     */
    /* binop/lit16 vA, vB, #+CCCC */
    FETCH_S r1, 1                       @ r1<- ssssCCCC (sign-extended)
    mov     r2, rINST, lsr #12          @ r2<- B
    ubfx    r4, rINST, #8, #4           @ r4<- A
    GET_VREG r0, r2                     @ r0<- vB
    .if 0
    cmp     r1, #0                      @ is second operand zero?
    beq     common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST

    eor     r0, r0, r1                              @ r0<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG r0, r4                @ vAA<- r0
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 10-13 instructions */

    NAME_END nterp_op_xor_int_lit16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_add_int_lit8: /* 0xd8 */
    NAME_START nterp_op_add_int_lit8

    /*
     * Generic 32-bit "lit8" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = r0 op r1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than r0, you can override "result".)
     *
     * You can override "extract" if the extraction of the literal value
     * from r3 to r1 is not the default "asr r1, r3, #8". The extraction
     * can be omitted completely if the shift is embedded in "instr".
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (r1).  Useful for integer division and modulus.
     *
     * For: add-int/lit8, rsub-int/lit8, mul-int/lit8, div-int/lit8,
     *      rem-int/lit8, and-int/lit8, or-int/lit8, xor-int/lit8,
     *      shl-int/lit8, shr-int/lit8, ushr-int/lit8
     */
    /* binop/lit8 vAA, vBB, #+CC */
    FETCH_S r3, 1                       @ r3<- ssssCCBB (sign-extended for CC)
    mov     r4, rINST, lsr #8           @ r4<- AA
    and     r2, r3, #255                @ r2<- BB
    GET_VREG r0, r2                     @ r0<- vBB
                                @ optional; typically r1<- ssssssCC (sign extended)
    .if 0
    @cmp     r1, #0                     @ is second operand zero?
    beq     common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST

    add     r0, r0, r3, asr #8                              @ r0<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG r0, r4                @ vAA<- r0
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 10-12 instructions */

    NAME_END nterp_op_add_int_lit8

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_rsub_int_lit8: /* 0xd9 */
    NAME_START nterp_op_rsub_int_lit8

    /*
     * Generic 32-bit "lit8" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = r0 op r1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than r0, you can override "result".)
     *
     * You can override "extract" if the extraction of the literal value
     * from r3 to r1 is not the default "asr r1, r3, #8". The extraction
     * can be omitted completely if the shift is embedded in "instr".
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (r1).  Useful for integer division and modulus.
     *
     * For: add-int/lit8, rsub-int/lit8, mul-int/lit8, div-int/lit8,
     *      rem-int/lit8, and-int/lit8, or-int/lit8, xor-int/lit8,
     *      shl-int/lit8, shr-int/lit8, ushr-int/lit8
     */
    /* binop/lit8 vAA, vBB, #+CC */
    FETCH_S r3, 1                       @ r3<- ssssCCBB (sign-extended for CC)
    mov     r4, rINST, lsr #8           @ r4<- AA
    and     r2, r3, #255                @ r2<- BB
    GET_VREG r0, r2                     @ r0<- vBB
                                @ optional; typically r1<- ssssssCC (sign extended)
    .if 0
    @cmp     r1, #0                     @ is second operand zero?
    beq     common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST

    rsb     r0, r0, r3, asr #8                              @ r0<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG r0, r4                @ vAA<- r0
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 10-12 instructions */

    NAME_END nterp_op_rsub_int_lit8

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_mul_int_lit8: /* 0xda */
    NAME_START nterp_op_mul_int_lit8

    /*
     * Generic 32-bit "lit8" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = r0 op r1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than r0, you can override "result".)
     *
     * You can override "extract" if the extraction of the literal value
     * from r3 to r1 is not the default "asr r1, r3, #8". The extraction
     * can be omitted completely if the shift is embedded in "instr".
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (r1).  Useful for integer division and modulus.
     *
     * For: add-int/lit8, rsub-int/lit8, mul-int/lit8, div-int/lit8,
     *      rem-int/lit8, and-int/lit8, or-int/lit8, xor-int/lit8,
     *      shl-int/lit8, shr-int/lit8, ushr-int/lit8
     */
    /* binop/lit8 vAA, vBB, #+CC */
    FETCH_S r3, 1                       @ r3<- ssssCCBB (sign-extended for CC)
    mov     r4, rINST, lsr #8           @ r4<- AA
    and     r2, r3, #255                @ r2<- BB
    GET_VREG r0, r2                     @ r0<- vBB
    asr     r1, r3, #8                            @ optional; typically r1<- ssssssCC (sign extended)
    .if 0
    @cmp     r1, #0                     @ is second operand zero?
    beq     common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST

    mul     r0, r1, r0                              @ r0<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG r0, r4                @ vAA<- r0
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 10-12 instructions */

    NAME_END nterp_op_mul_int_lit8

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_div_int_lit8: /* 0xdb */
    NAME_START nterp_op_div_int_lit8

    /*
     * Specialized 32-bit binary operation
     *
     * Performs "r0 = r0 div r1". The selection between sdiv or the gcc helper
     * depends on the compile time value of __ARM_ARCH_EXT_IDIV__ (defined for
     * ARMv7 CPUs that have hardware division support).
     *
     * div-int/lit8
     *
     */
    FETCH_S r3, 1                       @ r3<- ssssCCBB (sign-extended for CC
    mov     r4, rINST, lsr #8           @ r4<- AA
    and     r2, r3, #255                @ r2<- BB
    GET_VREG r0, r2                     @ r0<- vBB
    movs    r1, r3, asr #8              @ r1<- ssssssCC (sign extended)
    @cmp     r1, #0                     @ is second operand zero?
    beq     common_errDivideByZero
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST

#ifdef __ARM_ARCH_EXT_IDIV__
    sdiv    r0, r0, r1                  @ r0<- op
#else
    bl   __aeabi_idiv                   @ r0<- op, r0-r3 changed
#endif
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG r0, r4                     @ vAA<- r0
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 10-12 instructions */

    NAME_END nterp_op_div_int_lit8

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_rem_int_lit8: /* 0xdc */
    NAME_START nterp_op_rem_int_lit8

    /*
     * Specialized 32-bit binary operation
     *
     * Performs "r1 = r0 rem r1". The selection between sdiv block or the gcc helper
     * depends on the compile time value of __ARM_ARCH_EXT_IDIV__ (defined for
     * ARMv7 CPUs that have hardware division support).
     *
     * NOTE: idivmod returns quotient in r0 and remainder in r1
     *
     * rem-int/lit8
     *
     */
    FETCH_S r3, 1                       @ r3<- ssssCCBB (sign-extended for CC)
    mov     r4, rINST, lsr #8           @ r4<- AA
    and     r2, r3, #255                @ r2<- BB
    GET_VREG r0, r2                     @ r0<- vBB
    movs    r1, r3, asr #8              @ r1<- ssssssCC (sign extended)
    @cmp     r1, #0                     @ is second operand zero?
    beq     common_errDivideByZero
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST

#ifdef __ARM_ARCH_EXT_IDIV__
    sdiv    r2, r0, r1
    mls     r1, r1, r2, r0              @ r1<- op
#else
    bl       __aeabi_idivmod            @ r1<- op, r0-r3 changed
#endif
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG r1, r4                     @ vAA<- r1
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 10-12 instructions */

    NAME_END nterp_op_rem_int_lit8

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_and_int_lit8: /* 0xdd */
    NAME_START nterp_op_and_int_lit8

    /*
     * Generic 32-bit "lit8" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = r0 op r1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than r0, you can override "result".)
     *
     * You can override "extract" if the extraction of the literal value
     * from r3 to r1 is not the default "asr r1, r3, #8". The extraction
     * can be omitted completely if the shift is embedded in "instr".
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (r1).  Useful for integer division and modulus.
     *
     * For: add-int/lit8, rsub-int/lit8, mul-int/lit8, div-int/lit8,
     *      rem-int/lit8, and-int/lit8, or-int/lit8, xor-int/lit8,
     *      shl-int/lit8, shr-int/lit8, ushr-int/lit8
     */
    /* binop/lit8 vAA, vBB, #+CC */
    FETCH_S r3, 1                       @ r3<- ssssCCBB (sign-extended for CC)
    mov     r4, rINST, lsr #8           @ r4<- AA
    and     r2, r3, #255                @ r2<- BB
    GET_VREG r0, r2                     @ r0<- vBB
                                @ optional; typically r1<- ssssssCC (sign extended)
    .if 0
    @cmp     r1, #0                     @ is second operand zero?
    beq     common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST

    and     r0, r0, r3, asr #8                              @ r0<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG r0, r4                @ vAA<- r0
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 10-12 instructions */

    NAME_END nterp_op_and_int_lit8

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_or_int_lit8: /* 0xde */
    NAME_START nterp_op_or_int_lit8

    /*
     * Generic 32-bit "lit8" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = r0 op r1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than r0, you can override "result".)
     *
     * You can override "extract" if the extraction of the literal value
     * from r3 to r1 is not the default "asr r1, r3, #8". The extraction
     * can be omitted completely if the shift is embedded in "instr".
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (r1).  Useful for integer division and modulus.
     *
     * For: add-int/lit8, rsub-int/lit8, mul-int/lit8, div-int/lit8,
     *      rem-int/lit8, and-int/lit8, or-int/lit8, xor-int/lit8,
     *      shl-int/lit8, shr-int/lit8, ushr-int/lit8
     */
    /* binop/lit8 vAA, vBB, #+CC */
    FETCH_S r3, 1                       @ r3<- ssssCCBB (sign-extended for CC)
    mov     r4, rINST, lsr #8           @ r4<- AA
    and     r2, r3, #255                @ r2<- BB
    GET_VREG r0, r2                     @ r0<- vBB
                                @ optional; typically r1<- ssssssCC (sign extended)
    .if 0
    @cmp     r1, #0                     @ is second operand zero?
    beq     common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST

    orr     r0, r0, r3, asr #8                              @ r0<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG r0, r4                @ vAA<- r0
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 10-12 instructions */

    NAME_END nterp_op_or_int_lit8

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_xor_int_lit8: /* 0xdf */
    NAME_START nterp_op_xor_int_lit8

    /*
     * Generic 32-bit "lit8" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = r0 op r1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than r0, you can override "result".)
     *
     * You can override "extract" if the extraction of the literal value
     * from r3 to r1 is not the default "asr r1, r3, #8". The extraction
     * can be omitted completely if the shift is embedded in "instr".
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (r1).  Useful for integer division and modulus.
     *
     * For: add-int/lit8, rsub-int/lit8, mul-int/lit8, div-int/lit8,
     *      rem-int/lit8, and-int/lit8, or-int/lit8, xor-int/lit8,
     *      shl-int/lit8, shr-int/lit8, ushr-int/lit8
     */
    /* binop/lit8 vAA, vBB, #+CC */
    FETCH_S r3, 1                       @ r3<- ssssCCBB (sign-extended for CC)
    mov     r4, rINST, lsr #8           @ r4<- AA
    and     r2, r3, #255                @ r2<- BB
    GET_VREG r0, r2                     @ r0<- vBB
                                @ optional; typically r1<- ssssssCC (sign extended)
    .if 0
    @cmp     r1, #0                     @ is second operand zero?
    beq     common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST

    eor     r0, r0, r3, asr #8                              @ r0<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG r0, r4                @ vAA<- r0
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 10-12 instructions */

    NAME_END nterp_op_xor_int_lit8

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_shl_int_lit8: /* 0xe0 */
    NAME_START nterp_op_shl_int_lit8

    /*
     * Generic 32-bit "lit8" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = r0 op r1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than r0, you can override "result".)
     *
     * You can override "extract" if the extraction of the literal value
     * from r3 to r1 is not the default "asr r1, r3, #8". The extraction
     * can be omitted completely if the shift is embedded in "instr".
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (r1).  Useful for integer division and modulus.
     *
     * For: add-int/lit8, rsub-int/lit8, mul-int/lit8, div-int/lit8,
     *      rem-int/lit8, and-int/lit8, or-int/lit8, xor-int/lit8,
     *      shl-int/lit8, shr-int/lit8, ushr-int/lit8
     */
    /* binop/lit8 vAA, vBB, #+CC */
    FETCH_S r3, 1                       @ r3<- ssssCCBB (sign-extended for CC)
    mov     r4, rINST, lsr #8           @ r4<- AA
    and     r2, r3, #255                @ r2<- BB
    GET_VREG r0, r2                     @ r0<- vBB
    ubfx    r1, r3, #8, #5                            @ optional; typically r1<- ssssssCC (sign extended)
    .if 0
    @cmp     r1, #0                     @ is second operand zero?
    beq     common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST

    mov     r0, r0, lsl r1                              @ r0<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG r0, r4                @ vAA<- r0
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 10-12 instructions */

    NAME_END nterp_op_shl_int_lit8

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_shr_int_lit8: /* 0xe1 */
    NAME_START nterp_op_shr_int_lit8

    /*
     * Generic 32-bit "lit8" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = r0 op r1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than r0, you can override "result".)
     *
     * You can override "extract" if the extraction of the literal value
     * from r3 to r1 is not the default "asr r1, r3, #8". The extraction
     * can be omitted completely if the shift is embedded in "instr".
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (r1).  Useful for integer division and modulus.
     *
     * For: add-int/lit8, rsub-int/lit8, mul-int/lit8, div-int/lit8,
     *      rem-int/lit8, and-int/lit8, or-int/lit8, xor-int/lit8,
     *      shl-int/lit8, shr-int/lit8, ushr-int/lit8
     */
    /* binop/lit8 vAA, vBB, #+CC */
    FETCH_S r3, 1                       @ r3<- ssssCCBB (sign-extended for CC)
    mov     r4, rINST, lsr #8           @ r4<- AA
    and     r2, r3, #255                @ r2<- BB
    GET_VREG r0, r2                     @ r0<- vBB
    ubfx    r1, r3, #8, #5                            @ optional; typically r1<- ssssssCC (sign extended)
    .if 0
    @cmp     r1, #0                     @ is second operand zero?
    beq     common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST

    mov     r0, r0, asr r1                              @ r0<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG r0, r4                @ vAA<- r0
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 10-12 instructions */

    NAME_END nterp_op_shr_int_lit8

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_ushr_int_lit8: /* 0xe2 */
    NAME_START nterp_op_ushr_int_lit8

    /*
     * Generic 32-bit "lit8" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = r0 op r1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than r0, you can override "result".)
     *
     * You can override "extract" if the extraction of the literal value
     * from r3 to r1 is not the default "asr r1, r3, #8". The extraction
     * can be omitted completely if the shift is embedded in "instr".
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (r1).  Useful for integer division and modulus.
     *
     * For: add-int/lit8, rsub-int/lit8, mul-int/lit8, div-int/lit8,
     *      rem-int/lit8, and-int/lit8, or-int/lit8, xor-int/lit8,
     *      shl-int/lit8, shr-int/lit8, ushr-int/lit8
     */
    /* binop/lit8 vAA, vBB, #+CC */
    FETCH_S r3, 1                       @ r3<- ssssCCBB (sign-extended for CC)
    mov     r4, rINST, lsr #8           @ r4<- AA
    and     r2, r3, #255                @ r2<- BB
    GET_VREG r0, r2                     @ r0<- vBB
    ubfx    r1, r3, #8, #5                            @ optional; typically r1<- ssssssCC (sign extended)
    .if 0
    @cmp     r1, #0                     @ is second operand zero?
    beq     common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 2                @ advance rPC, load rINST

    mov     r0, r0, lsr r1                              @ r0<- op, r0-r3 changed
    GET_INST_OPCODE ip                  @ extract opcode from rINST
    SET_VREG r0, r4                @ vAA<- r0
    GOTO_OPCODE ip                      @ jump to next instruction
    /* 10-12 instructions */

    NAME_END nterp_op_ushr_int_lit8

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_e3: /* 0xe3 */
    NAME_START nterp_op_unused_e3

    bkpt

    NAME_END nterp_op_unused_e3

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_e4: /* 0xe4 */
    NAME_START nterp_op_unused_e4

    bkpt

    NAME_END nterp_op_unused_e4

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_e5: /* 0xe5 */
    NAME_START nterp_op_unused_e5

    bkpt

    NAME_END nterp_op_unused_e5

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_e6: /* 0xe6 */
    NAME_START nterp_op_unused_e6

    bkpt

    NAME_END nterp_op_unused_e6

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_e7: /* 0xe7 */
    NAME_START nterp_op_unused_e7

    bkpt

    NAME_END nterp_op_unused_e7

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_e8: /* 0xe8 */
    NAME_START nterp_op_unused_e8

    bkpt

    NAME_END nterp_op_unused_e8

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_e9: /* 0xe9 */
    NAME_START nterp_op_unused_e9

    bkpt

    NAME_END nterp_op_unused_e9

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_ea: /* 0xea */
    NAME_START nterp_op_unused_ea

    bkpt

    NAME_END nterp_op_unused_ea

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_eb: /* 0xeb */
    NAME_START nterp_op_unused_eb

    bkpt

    NAME_END nterp_op_unused_eb

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_ec: /* 0xec */
    NAME_START nterp_op_unused_ec

    bkpt

    NAME_END nterp_op_unused_ec

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_ed: /* 0xed */
    NAME_START nterp_op_unused_ed

    bkpt

    NAME_END nterp_op_unused_ed

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_ee: /* 0xee */
    NAME_START nterp_op_unused_ee

    bkpt

    NAME_END nterp_op_unused_ee

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_ef: /* 0xef */
    NAME_START nterp_op_unused_ef

    bkpt

    NAME_END nterp_op_unused_ef

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_f0: /* 0xf0 */
    NAME_START nterp_op_unused_f0

    bkpt

    NAME_END nterp_op_unused_f0

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_f1: /* 0xf1 */
    NAME_START nterp_op_unused_f1

    bkpt

    NAME_END nterp_op_unused_f1

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_f2: /* 0xf2 */
    NAME_START nterp_op_unused_f2

    bkpt

    NAME_END nterp_op_unused_f2

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_f3: /* 0xf3 */
    NAME_START nterp_op_unused_f3

    bkpt

    NAME_END nterp_op_unused_f3

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_f4: /* 0xf4 */
    NAME_START nterp_op_unused_f4

    bkpt

    NAME_END nterp_op_unused_f4

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_f5: /* 0xf5 */
    NAME_START nterp_op_unused_f5

    bkpt

    NAME_END nterp_op_unused_f5

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_f6: /* 0xf6 */
    NAME_START nterp_op_unused_f6

    bkpt

    NAME_END nterp_op_unused_f6

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_f7: /* 0xf7 */
    NAME_START nterp_op_unused_f7

    bkpt

    NAME_END nterp_op_unused_f7

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_f8: /* 0xf8 */
    NAME_START nterp_op_unused_f8

    bkpt

    NAME_END nterp_op_unused_f8

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_f9: /* 0xf9 */
    NAME_START nterp_op_unused_f9

    bkpt

    NAME_END nterp_op_unused_f9

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_polymorphic: /* 0xfa */
    NAME_START nterp_op_invoke_polymorphic

   EXPORT_PC
   // No need to fetch the target method.
   // Load the first argument (the 'this' pointer).
   FETCH r1, 2
   and r1, r1, #0xf
   GET_VREG r1, r1
   cmp r1, #0
   beq common_errNullObject    // bail if null
   b NterpCommonInvokePolymorphic

    NAME_END nterp_op_invoke_polymorphic

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_polymorphic_range: /* 0xfb */
    NAME_START nterp_op_invoke_polymorphic_range

   EXPORT_PC
   // No need to fetch the target method.
   // Load the first argument (the 'this' pointer).
   FETCH r1, 2
   GET_VREG r1, r1
   cmp r1, #0
   beq common_errNullObject    // bail if null
   b NterpCommonInvokePolymorphicRange

    NAME_END nterp_op_invoke_polymorphic_range

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_custom: /* 0xfc */
    NAME_START nterp_op_invoke_custom

   EXPORT_PC
   FETCH r0, 1 // call_site index, first argument of runtime call.
   b NterpCommonInvokeCustom

    NAME_END nterp_op_invoke_custom

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_custom_range: /* 0xfd */
    NAME_START nterp_op_invoke_custom_range

   EXPORT_PC
   FETCH r0, 1 // call_site index, first argument of runtime call.
   b NterpCommonInvokeCustomRange

    NAME_END nterp_op_invoke_custom_range

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_const_method_handle: /* 0xfe */
    NAME_START nterp_op_const_method_handle

   // Fast-path which gets the object from thread-local cache.
   FETCH_FROM_THREAD_CACHE r0, 2f
   cmp rMR, #0
   bne 3f
1:
   mov     r1, rINST, lsr #8           @ r1<- AA
   .if 0
   FETCH_ADVANCE_INST 3                // advance rPC, load rINST
   .else
   FETCH_ADVANCE_INST 2                // advance rPC, load rINST
   .endif
   GET_INST_OPCODE ip                  // extract opcode from rINST
   SET_VREG_OBJECT r0, r1              // vAA <- value
   GOTO_OPCODE ip                      // jump to next instruction
2:
   EXPORT_PC
   mov r0, rSELF
   ldr r1, [sp]
   mov r2, rPC
   bl nterp_load_object
   b 1b
3:
   bl art_quick_read_barrier_mark_reg00
   b 1b

    NAME_END nterp_op_const_method_handle

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_const_method_type: /* 0xff */
    NAME_START nterp_op_const_method_type

   // Fast-path which gets the object from thread-local cache.
   FETCH_FROM_THREAD_CACHE r0, 2f
   cmp rMR, #0
   bne 3f
1:
   mov     r1, rINST, lsr #8           @ r1<- AA
   .if 0
   FETCH_ADVANCE_INST 3                // advance rPC, load rINST
   .else
   FETCH_ADVANCE_INST 2                // advance rPC, load rINST
   .endif
   GET_INST_OPCODE ip                  // extract opcode from rINST
   SET_VREG_OBJECT r0, r1              // vAA <- value
   GOTO_OPCODE ip                      // jump to next instruction
2:
   EXPORT_PC
   mov r0, rSELF
   ldr r1, [sp]
   mov r2, rPC
   bl nterp_load_object
   b 1b
3:
   bl art_quick_read_barrier_mark_reg00
   b 1b

    NAME_END nterp_op_const_method_type

    .balign MTERP_HANDLER_SIZE

    .type artNterpAsmInstructionEnd, #object
    .hidden artNterpAsmInstructionEnd
    .global artNterpAsmInstructionEnd
artNterpAsmInstructionEnd:
    // artNterpAsmInstructionEnd is used as landing pad for exception handling.
    FETCH_INST
    GET_INST_OPCODE ip
    GOTO_OPCODE ip

    NAME_START nterp_op_iget_boolean_helper
   mov     r0, rSELF
   ldr     r1, [sp]
   mov     r2, rPC
   mov     r3, #0
   EXPORT_PC
   bl      nterp_get_instance_field_offset
   cmp     r0, #0
   bge     .Lop_iget_boolean_resume
   CLEAR_INSTANCE_VOLATILE_MARKER r0
   lsr     r2, rINST, #12              // r2<- B
   GET_VREG r3, r2                     // r3<- object we're operating on
   ubfx    r2, rINST, #8, #4           // r2<- A
   cmp     r3, #0
   beq     common_errNullObject            // object was null
   .if 0
   add     ip, r3, r0
   ATOMIC_LOAD64 ip, r0, r1, r3, .Lop_iget_boolean_slow_path_atomic_load
   dmb     ish
   CLEAR_SHADOW_PAIR r2, ip, lr
   VREG_INDEX_TO_ADDR r2, r2
   SET_VREG_WIDE_BY_ADDR r0, r1, r2    // fp[A] <- value
   .else
   ldrb   r0, [r3, r0]
   dmb     ish
   .if 0
   cmp     rMR, #0
   bne     .Lop_iget_boolean_read_barrier
   SET_VREG_OBJECT r0, r2              // fp[A] <- value
   .else
   SET_VREG r0, r2                     // fp[A] <- value
   .endif
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip

    NAME_END nterp_op_iget_boolean_helper
    NAME_START nterp_op_iget_byte_helper
   mov     r0, rSELF
   ldr     r1, [sp]
   mov     r2, rPC
   mov     r3, #0
   EXPORT_PC
   bl      nterp_get_instance_field_offset
   cmp     r0, #0
   bge     .Lop_iget_byte_resume
   CLEAR_INSTANCE_VOLATILE_MARKER r0
   lsr     r2, rINST, #12              // r2<- B
   GET_VREG r3, r2                     // r3<- object we're operating on
   ubfx    r2, rINST, #8, #4           // r2<- A
   cmp     r3, #0
   beq     common_errNullObject            // object was null
   .if 0
   add     ip, r3, r0
   ATOMIC_LOAD64 ip, r0, r1, r3, .Lop_iget_byte_slow_path_atomic_load
   dmb     ish
   CLEAR_SHADOW_PAIR r2, ip, lr
   VREG_INDEX_TO_ADDR r2, r2
   SET_VREG_WIDE_BY_ADDR r0, r1, r2    // fp[A] <- value
   .else
   ldrsb   r0, [r3, r0]
   dmb     ish
   .if 0
   cmp     rMR, #0
   bne     .Lop_iget_byte_read_barrier
   SET_VREG_OBJECT r0, r2              // fp[A] <- value
   .else
   SET_VREG r0, r2                     // fp[A] <- value
   .endif
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip

    NAME_END nterp_op_iget_byte_helper
    NAME_START nterp_op_iget_char_helper
   mov     r0, rSELF
   ldr     r1, [sp]
   mov     r2, rPC
   mov     r3, #0
   EXPORT_PC
   bl      nterp_get_instance_field_offset
   cmp     r0, #0
   bge     .Lop_iget_char_resume
   CLEAR_INSTANCE_VOLATILE_MARKER r0
   lsr     r2, rINST, #12              // r2<- B
   GET_VREG r3, r2                     // r3<- object we're operating on
   ubfx    r2, rINST, #8, #4           // r2<- A
   cmp     r3, #0
   beq     common_errNullObject            // object was null
   .if 0
   add     ip, r3, r0
   ATOMIC_LOAD64 ip, r0, r1, r3, .Lop_iget_char_slow_path_atomic_load
   dmb     ish
   CLEAR_SHADOW_PAIR r2, ip, lr
   VREG_INDEX_TO_ADDR r2, r2
   SET_VREG_WIDE_BY_ADDR r0, r1, r2    // fp[A] <- value
   .else
   ldrh   r0, [r3, r0]
   dmb     ish
   .if 0
   cmp     rMR, #0
   bne     .Lop_iget_char_read_barrier
   SET_VREG_OBJECT r0, r2              // fp[A] <- value
   .else
   SET_VREG r0, r2                     // fp[A] <- value
   .endif
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip

    NAME_END nterp_op_iget_char_helper
    NAME_START nterp_op_iget_helper
   mov     r0, rSELF
   ldr     r1, [sp]
   mov     r2, rPC
   mov     r3, #0
   EXPORT_PC
   bl      nterp_get_instance_field_offset
   cmp     r0, #0
   bge     .Lop_iget_resume
   CLEAR_INSTANCE_VOLATILE_MARKER r0
   lsr     r2, rINST, #12              // r2<- B
   GET_VREG r3, r2                     // r3<- object we're operating on
   ubfx    r2, rINST, #8, #4           // r2<- A
   cmp     r3, #0
   beq     common_errNullObject            // object was null
   .if 0
   add     ip, r3, r0
   ATOMIC_LOAD64 ip, r0, r1, r3, .Lop_iget_slow_path_atomic_load
   dmb     ish
   CLEAR_SHADOW_PAIR r2, ip, lr
   VREG_INDEX_TO_ADDR r2, r2
   SET_VREG_WIDE_BY_ADDR r0, r1, r2    // fp[A] <- value
   .else
   ldr   r0, [r3, r0]
   dmb     ish
   .if 0
   cmp     rMR, #0
   bne     .Lop_iget_read_barrier
   SET_VREG_OBJECT r0, r2              // fp[A] <- value
   .else
   SET_VREG r0, r2                     // fp[A] <- value
   .endif
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip

    NAME_END nterp_op_iget_helper
    NAME_START nterp_op_iget_object_helper
   mov     r0, rSELF
   ldr     r1, [sp]
   mov     r2, rPC
   mov     r3, #0
   EXPORT_PC
   bl      nterp_get_instance_field_offset
   cmp     r0, #0
   bge     .Lop_iget_object_resume
   CLEAR_INSTANCE_VOLATILE_MARKER r0
   lsr     r2, rINST, #12              // r2<- B
   GET_VREG r3, r2                     // r3<- object we're operating on
   ubfx    r2, rINST, #8, #4           // r2<- A
   cmp     r3, #0
   beq     common_errNullObject            // object was null
   .if 0
   add     ip, r3, r0
   ATOMIC_LOAD64 ip, r0, r1, r3, .Lop_iget_object_slow_path_atomic_load
   dmb     ish
   CLEAR_SHADOW_PAIR r2, ip, lr
   VREG_INDEX_TO_ADDR r2, r2
   SET_VREG_WIDE_BY_ADDR r0, r1, r2    // fp[A] <- value
   .else
   ldr   r0, [r3, r0]
   dmb     ish
   .if 1
   cmp     rMR, #0
   bne     .Lop_iget_object_read_barrier
   SET_VREG_OBJECT r0, r2              // fp[A] <- value
   .else
   SET_VREG r0, r2                     // fp[A] <- value
   .endif
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip

    NAME_END nterp_op_iget_object_helper
    NAME_START nterp_op_iget_short_helper
   mov     r0, rSELF
   ldr     r1, [sp]
   mov     r2, rPC
   mov     r3, #0
   EXPORT_PC
   bl      nterp_get_instance_field_offset
   cmp     r0, #0
   bge     .Lop_iget_short_resume
   CLEAR_INSTANCE_VOLATILE_MARKER r0
   lsr     r2, rINST, #12              // r2<- B
   GET_VREG r3, r2                     // r3<- object we're operating on
   ubfx    r2, rINST, #8, #4           // r2<- A
   cmp     r3, #0
   beq     common_errNullObject            // object was null
   .if 0
   add     ip, r3, r0
   ATOMIC_LOAD64 ip, r0, r1, r3, .Lop_iget_short_slow_path_atomic_load
   dmb     ish
   CLEAR_SHADOW_PAIR r2, ip, lr
   VREG_INDEX_TO_ADDR r2, r2
   SET_VREG_WIDE_BY_ADDR r0, r1, r2    // fp[A] <- value
   .else
   ldrsh   r0, [r3, r0]
   dmb     ish
   .if 0
   cmp     rMR, #0
   bne     .Lop_iget_short_read_barrier
   SET_VREG_OBJECT r0, r2              // fp[A] <- value
   .else
   SET_VREG r0, r2                     // fp[A] <- value
   .endif
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip

    NAME_END nterp_op_iget_short_helper
    NAME_START nterp_op_iget_wide_helper
   mov     r0, rSELF
   ldr     r1, [sp]
   mov     r2, rPC
   mov     r3, #0
   EXPORT_PC
   bl      nterp_get_instance_field_offset
   cmp     r0, #0
   bge     .Lop_iget_wide_resume
   CLEAR_INSTANCE_VOLATILE_MARKER r0
   lsr     r2, rINST, #12              // r2<- B
   GET_VREG r3, r2                     // r3<- object we're operating on
   ubfx    r2, rINST, #8, #4           // r2<- A
   cmp     r3, #0
   beq     common_errNullObject            // object was null
   .if 1
   add     ip, r3, r0
   ATOMIC_LOAD64 ip, r0, r1, r3, .Lop_iget_wide_slow_path_atomic_load
   dmb     ish
   CLEAR_SHADOW_PAIR r2, ip, lr
   VREG_INDEX_TO_ADDR r2, r2
   SET_VREG_WIDE_BY_ADDR r0, r1, r2    // fp[A] <- value
   .else
   ldr   r0, [r3, r0]
   dmb     ish
   .if 0
   cmp     rMR, #0
   bne     .Lop_iget_wide_read_barrier
   SET_VREG_OBJECT r0, r2              // fp[A] <- value
   .else
   SET_VREG r0, r2                     // fp[A] <- value
   .endif
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip

    NAME_END nterp_op_iget_wide_helper
    NAME_START nterp_op_invoke_interface_helper
   mov r0, rSELF
   ldr r1, [sp]
   mov r2, rPC
   bl nterp_get_method
   mov r4, r0
   b .Lop_invoke_interface_resume

    NAME_END nterp_op_invoke_interface_helper
    NAME_START nterp_op_invoke_interface_range_helper
   mov r0, rSELF
   ldr r1, [sp]
   mov r2, rPC
   bl nterp_get_method
   mov r4, r0
   b .Lop_invoke_interface_range_resume

    NAME_END nterp_op_invoke_interface_range_helper
    NAME_START nterp_op_iput_helper_str00
   mov     r0, rSELF
   ldr     r1, [sp]
   mov     r2, rPC
   .if 0
   mov     r3, r4
   .else
   mov     r3, #0
   .endif
   EXPORT_PC
   bl      nterp_get_instance_field_offset
   .if 0
   // Reload the value as it may have moved.
   ubfx    r4, rINST, #8, #4           // r4<- A
   GET_VREG r4, r4                     // r4 <- v[A]
   .endif
   cmp     r0, #0
   bge     .Lop_iput_resume
   CLEAR_INSTANCE_VOLATILE_MARKER r0
   .if 0
   lsr     r4, rINST, #12              // r4<- B
   ubfx    r1, rINST, #8, #4           // r1<- A
   GET_VREG r4, r4                     // vB (object we're operating on)
   cmp     r4, #0
   beq     common_errNullObject
   VREG_INDEX_TO_ADDR r1, r1
   GET_VREG_WIDE_BY_ADDR r2, r3, r1
   add     ip, r4, r0
   dmb     ish
   ATOMIC_STORE64 ip, r2, r3, r0, r1, .Lop_iput_slow_path_atomic_store
   dmb     ish
   .else
   lsr     r1, rINST, #12              // r4<- B
   GET_VREG r1, r1                     // vB (object we're operating on)
   cmp     r1, #0
   beq     common_errNullObject
   dmb     ish
   str  r4, [r1, r0]
   dmb     ish
   WRITE_BARRIER_IF_OBJECT 0, r4, r1, .Lop_iput_slow_path_skip_write_barrier, r0
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip

    NAME_END nterp_op_iput_helper_str00
    NAME_START nterp_op_iput_helper_str01
   mov     r0, rSELF
   ldr     r1, [sp]
   mov     r2, rPC
   .if 1
   mov     r3, r4
   .else
   mov     r3, #0
   .endif
   EXPORT_PC
   bl      nterp_get_instance_field_offset
   .if 1
   // Reload the value as it may have moved.
   ubfx    r4, rINST, #8, #4           // r4<- A
   GET_VREG r4, r4                     // r4 <- v[A]
   .endif
   cmp     r0, #0
   bge     .Lop_iput_object_resume
   CLEAR_INSTANCE_VOLATILE_MARKER r0
   .if 0
   lsr     r4, rINST, #12              // r4<- B
   ubfx    r1, rINST, #8, #4           // r1<- A
   GET_VREG r4, r4                     // vB (object we're operating on)
   cmp     r4, #0
   beq     common_errNullObject
   VREG_INDEX_TO_ADDR r1, r1
   GET_VREG_WIDE_BY_ADDR r2, r3, r1
   add     ip, r4, r0
   dmb     ish
   ATOMIC_STORE64 ip, r2, r3, r0, r1, .Lop_iput_object_slow_path_atomic_store
   dmb     ish
   .else
   lsr     r1, rINST, #12              // r4<- B
   GET_VREG r1, r1                     // vB (object we're operating on)
   cmp     r1, #0
   beq     common_errNullObject
   dmb     ish
   str  r4, [r1, r0]
   dmb     ish
   WRITE_BARRIER_IF_OBJECT 1, r4, r1, .Lop_iput_object_slow_path_skip_write_barrier, r0
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip

    NAME_END nterp_op_iput_helper_str01
    NAME_START nterp_op_iput_helper_str10
   mov     r0, rSELF
   ldr     r1, [sp]
   mov     r2, rPC
   .if 0
   mov     r3, r4
   .else
   mov     r3, #0
   .endif
   EXPORT_PC
   bl      nterp_get_instance_field_offset
   .if 0
   // Reload the value as it may have moved.
   ubfx    r4, rINST, #8, #4           // r4<- A
   GET_VREG r4, r4                     // r4 <- v[A]
   .endif
   cmp     r0, #0
   bge     .Lop_iput_wide_resume
   CLEAR_INSTANCE_VOLATILE_MARKER r0
   .if 1
   lsr     r4, rINST, #12              // r4<- B
   ubfx    r1, rINST, #8, #4           // r1<- A
   GET_VREG r4, r4                     // vB (object we're operating on)
   cmp     r4, #0
   beq     common_errNullObject
   VREG_INDEX_TO_ADDR r1, r1
   GET_VREG_WIDE_BY_ADDR r2, r3, r1
   add     ip, r4, r0
   dmb     ish
   ATOMIC_STORE64 ip, r2, r3, r0, r1, .Lop_iput_wide_slow_path_atomic_store
   dmb     ish
   .else
   lsr     r1, rINST, #12              // r4<- B
   GET_VREG r1, r1                     // vB (object we're operating on)
   cmp     r1, #0
   beq     common_errNullObject
   dmb     ish
   str  r4, [r1, r0]
   dmb     ish
   WRITE_BARRIER_IF_OBJECT 0, r4, r1, .Lop_iput_wide_slow_path_skip_write_barrier, r0
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip

    NAME_END nterp_op_iput_helper_str10
    NAME_START nterp_op_iput_helper_strb00
   mov     r0, rSELF
   ldr     r1, [sp]
   mov     r2, rPC
   .if 0
   mov     r3, r4
   .else
   mov     r3, #0
   .endif
   EXPORT_PC
   bl      nterp_get_instance_field_offset
   .if 0
   // Reload the value as it may have moved.
   ubfx    r4, rINST, #8, #4           // r4<- A
   GET_VREG r4, r4                     // r4 <- v[A]
   .endif
   cmp     r0, #0
   bge     .Lop_iput_byte_resume
   CLEAR_INSTANCE_VOLATILE_MARKER r0
   .if 0
   lsr     r4, rINST, #12              // r4<- B
   ubfx    r1, rINST, #8, #4           // r1<- A
   GET_VREG r4, r4                     // vB (object we're operating on)
   cmp     r4, #0
   beq     common_errNullObject
   VREG_INDEX_TO_ADDR r1, r1
   GET_VREG_WIDE_BY_ADDR r2, r3, r1
   add     ip, r4, r0
   dmb     ish
   ATOMIC_STORE64 ip, r2, r3, r0, r1, .Lop_iput_byte_slow_path_atomic_store
   dmb     ish
   .else
   lsr     r1, rINST, #12              // r4<- B
   GET_VREG r1, r1                     // vB (object we're operating on)
   cmp     r1, #0
   beq     common_errNullObject
   dmb     ish
   strb  r4, [r1, r0]
   dmb     ish
   WRITE_BARRIER_IF_OBJECT 0, r4, r1, .Lop_iput_byte_slow_path_skip_write_barrier, r0
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip

    NAME_END nterp_op_iput_helper_strb00
    NAME_START nterp_op_iput_helper_strh00
   mov     r0, rSELF
   ldr     r1, [sp]
   mov     r2, rPC
   .if 0
   mov     r3, r4
   .else
   mov     r3, #0
   .endif
   EXPORT_PC
   bl      nterp_get_instance_field_offset
   .if 0
   // Reload the value as it may have moved.
   ubfx    r4, rINST, #8, #4           // r4<- A
   GET_VREG r4, r4                     // r4 <- v[A]
   .endif
   cmp     r0, #0
   bge     .Lop_iput_short_resume
   CLEAR_INSTANCE_VOLATILE_MARKER r0
   .if 0
   lsr     r4, rINST, #12              // r4<- B
   ubfx    r1, rINST, #8, #4           // r1<- A
   GET_VREG r4, r4                     // vB (object we're operating on)
   cmp     r4, #0
   beq     common_errNullObject
   VREG_INDEX_TO_ADDR r1, r1
   GET_VREG_WIDE_BY_ADDR r2, r3, r1
   add     ip, r4, r0
   dmb     ish
   ATOMIC_STORE64 ip, r2, r3, r0, r1, .Lop_iput_short_slow_path_atomic_store
   dmb     ish
   .else
   lsr     r1, rINST, #12              // r4<- B
   GET_VREG r1, r1                     // vB (object we're operating on)
   cmp     r1, #0
   beq     common_errNullObject
   dmb     ish
   strh  r4, [r1, r0]
   dmb     ish
   WRITE_BARRIER_IF_OBJECT 0, r4, r1, .Lop_iput_short_slow_path_skip_write_barrier, r0
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip

    NAME_END nterp_op_iput_helper_strh00
    NAME_START nterp_op_sget_boolean_helper
   mov     r0, rSELF
   ldr     r1, [sp]
   mov     r2, rPC
   mov     r3, #0
   EXPORT_PC
   bl      nterp_get_static_field
   tst     r0, #1
   beq     .Lop_sget_boolean_resume
   CLEAR_STATIC_VOLATILE_MARKER r0
   ldr     r1, [r0, #ART_FIELD_OFFSET_OFFSET]
   lsr     r2, rINST, #8               // r2 <- A
   ldr     r0, [r0, #ART_FIELD_DECLARING_CLASS_OFFSET]
   cmp     rMR, #0
   bne     .Lop_sget_boolean_slow_path_read_barrier
.Lop_sget_boolean_slow_path_resume_after_read_barrier:
   .if 0
   add     ip, r0, r1
   ATOMIC_LOAD64 ip, r0, r1, r3, .Lop_sget_boolean_slow_path_atomic_load
   dmb     ish
   CLEAR_SHADOW_PAIR r2, ip, lr
   VREG_INDEX_TO_ADDR r2, r2
   SET_VREG_WIDE_BY_ADDR r0, r1, r2    // fp[A] <- value
   .else
   ldrb   r0, [r0, r1]
   dmb     ish
   .if 0
   cmp     rMR, #0
   bne     .Lop_sget_boolean_mark_after_load
   SET_VREG_OBJECT r0, r2              // fp[A] <- value
   .else
   SET_VREG r0, r2                     // fp[A] <- value
   .endif
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.Lop_sget_boolean_slow_path_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_sget_boolean_slow_path_resume_after_read_barrier

    NAME_END nterp_op_sget_boolean_helper
    NAME_START nterp_op_sget_byte_helper
   mov     r0, rSELF
   ldr     r1, [sp]
   mov     r2, rPC
   mov     r3, #0
   EXPORT_PC
   bl      nterp_get_static_field
   tst     r0, #1
   beq     .Lop_sget_byte_resume
   CLEAR_STATIC_VOLATILE_MARKER r0
   ldr     r1, [r0, #ART_FIELD_OFFSET_OFFSET]
   lsr     r2, rINST, #8               // r2 <- A
   ldr     r0, [r0, #ART_FIELD_DECLARING_CLASS_OFFSET]
   cmp     rMR, #0
   bne     .Lop_sget_byte_slow_path_read_barrier
.Lop_sget_byte_slow_path_resume_after_read_barrier:
   .if 0
   add     ip, r0, r1
   ATOMIC_LOAD64 ip, r0, r1, r3, .Lop_sget_byte_slow_path_atomic_load
   dmb     ish
   CLEAR_SHADOW_PAIR r2, ip, lr
   VREG_INDEX_TO_ADDR r2, r2
   SET_VREG_WIDE_BY_ADDR r0, r1, r2    // fp[A] <- value
   .else
   ldrsb   r0, [r0, r1]
   dmb     ish
   .if 0
   cmp     rMR, #0
   bne     .Lop_sget_byte_mark_after_load
   SET_VREG_OBJECT r0, r2              // fp[A] <- value
   .else
   SET_VREG r0, r2                     // fp[A] <- value
   .endif
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.Lop_sget_byte_slow_path_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_sget_byte_slow_path_resume_after_read_barrier

    NAME_END nterp_op_sget_byte_helper
    NAME_START nterp_op_sget_char_helper
   mov     r0, rSELF
   ldr     r1, [sp]
   mov     r2, rPC
   mov     r3, #0
   EXPORT_PC
   bl      nterp_get_static_field
   tst     r0, #1
   beq     .Lop_sget_char_resume
   CLEAR_STATIC_VOLATILE_MARKER r0
   ldr     r1, [r0, #ART_FIELD_OFFSET_OFFSET]
   lsr     r2, rINST, #8               // r2 <- A
   ldr     r0, [r0, #ART_FIELD_DECLARING_CLASS_OFFSET]
   cmp     rMR, #0
   bne     .Lop_sget_char_slow_path_read_barrier
.Lop_sget_char_slow_path_resume_after_read_barrier:
   .if 0
   add     ip, r0, r1
   ATOMIC_LOAD64 ip, r0, r1, r3, .Lop_sget_char_slow_path_atomic_load
   dmb     ish
   CLEAR_SHADOW_PAIR r2, ip, lr
   VREG_INDEX_TO_ADDR r2, r2
   SET_VREG_WIDE_BY_ADDR r0, r1, r2    // fp[A] <- value
   .else
   ldrh   r0, [r0, r1]
   dmb     ish
   .if 0
   cmp     rMR, #0
   bne     .Lop_sget_char_mark_after_load
   SET_VREG_OBJECT r0, r2              // fp[A] <- value
   .else
   SET_VREG r0, r2                     // fp[A] <- value
   .endif
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.Lop_sget_char_slow_path_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_sget_char_slow_path_resume_after_read_barrier

    NAME_END nterp_op_sget_char_helper
    NAME_START nterp_op_sget_helper
   mov     r0, rSELF
   ldr     r1, [sp]
   mov     r2, rPC
   mov     r3, #0
   EXPORT_PC
   bl      nterp_get_static_field
   tst     r0, #1
   beq     .Lop_sget_resume
   CLEAR_STATIC_VOLATILE_MARKER r0
   ldr     r1, [r0, #ART_FIELD_OFFSET_OFFSET]
   lsr     r2, rINST, #8               // r2 <- A
   ldr     r0, [r0, #ART_FIELD_DECLARING_CLASS_OFFSET]
   cmp     rMR, #0
   bne     .Lop_sget_slow_path_read_barrier
.Lop_sget_slow_path_resume_after_read_barrier:
   .if 0
   add     ip, r0, r1
   ATOMIC_LOAD64 ip, r0, r1, r3, .Lop_sget_slow_path_atomic_load
   dmb     ish
   CLEAR_SHADOW_PAIR r2, ip, lr
   VREG_INDEX_TO_ADDR r2, r2
   SET_VREG_WIDE_BY_ADDR r0, r1, r2    // fp[A] <- value
   .else
   ldr   r0, [r0, r1]
   dmb     ish
   .if 0
   cmp     rMR, #0
   bne     .Lop_sget_mark_after_load
   SET_VREG_OBJECT r0, r2              // fp[A] <- value
   .else
   SET_VREG r0, r2                     // fp[A] <- value
   .endif
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.Lop_sget_slow_path_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_sget_slow_path_resume_after_read_barrier

    NAME_END nterp_op_sget_helper
    NAME_START nterp_op_sget_object_helper
   mov     r0, rSELF
   ldr     r1, [sp]
   mov     r2, rPC
   mov     r3, #0
   EXPORT_PC
   bl      nterp_get_static_field
   tst     r0, #1
   beq     .Lop_sget_object_resume
   CLEAR_STATIC_VOLATILE_MARKER r0
   ldr     r1, [r0, #ART_FIELD_OFFSET_OFFSET]
   lsr     r2, rINST, #8               // r2 <- A
   ldr     r0, [r0, #ART_FIELD_DECLARING_CLASS_OFFSET]
   cmp     rMR, #0
   bne     .Lop_sget_object_slow_path_read_barrier
.Lop_sget_object_slow_path_resume_after_read_barrier:
   .if 0
   add     ip, r0, r1
   ATOMIC_LOAD64 ip, r0, r1, r3, .Lop_sget_object_slow_path_atomic_load
   dmb     ish
   CLEAR_SHADOW_PAIR r2, ip, lr
   VREG_INDEX_TO_ADDR r2, r2
   SET_VREG_WIDE_BY_ADDR r0, r1, r2    // fp[A] <- value
   .else
   ldr   r0, [r0, r1]
   dmb     ish
   .if 1
   cmp     rMR, #0
   bne     .Lop_sget_object_mark_after_load
   SET_VREG_OBJECT r0, r2              // fp[A] <- value
   .else
   SET_VREG r0, r2                     // fp[A] <- value
   .endif
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.Lop_sget_object_slow_path_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_sget_object_slow_path_resume_after_read_barrier

    NAME_END nterp_op_sget_object_helper
    NAME_START nterp_op_sget_short_helper
   mov     r0, rSELF
   ldr     r1, [sp]
   mov     r2, rPC
   mov     r3, #0
   EXPORT_PC
   bl      nterp_get_static_field
   tst     r0, #1
   beq     .Lop_sget_short_resume
   CLEAR_STATIC_VOLATILE_MARKER r0
   ldr     r1, [r0, #ART_FIELD_OFFSET_OFFSET]
   lsr     r2, rINST, #8               // r2 <- A
   ldr     r0, [r0, #ART_FIELD_DECLARING_CLASS_OFFSET]
   cmp     rMR, #0
   bne     .Lop_sget_short_slow_path_read_barrier
.Lop_sget_short_slow_path_resume_after_read_barrier:
   .if 0
   add     ip, r0, r1
   ATOMIC_LOAD64 ip, r0, r1, r3, .Lop_sget_short_slow_path_atomic_load
   dmb     ish
   CLEAR_SHADOW_PAIR r2, ip, lr
   VREG_INDEX_TO_ADDR r2, r2
   SET_VREG_WIDE_BY_ADDR r0, r1, r2    // fp[A] <- value
   .else
   ldrsh   r0, [r0, r1]
   dmb     ish
   .if 0
   cmp     rMR, #0
   bne     .Lop_sget_short_mark_after_load
   SET_VREG_OBJECT r0, r2              // fp[A] <- value
   .else
   SET_VREG r0, r2                     // fp[A] <- value
   .endif
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.Lop_sget_short_slow_path_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_sget_short_slow_path_resume_after_read_barrier

    NAME_END nterp_op_sget_short_helper
    NAME_START nterp_op_sget_wide_helper
   mov     r0, rSELF
   ldr     r1, [sp]
   mov     r2, rPC
   mov     r3, #0
   EXPORT_PC
   bl      nterp_get_static_field
   tst     r0, #1
   beq     .Lop_sget_wide_resume
   CLEAR_STATIC_VOLATILE_MARKER r0
   ldr     r1, [r0, #ART_FIELD_OFFSET_OFFSET]
   lsr     r2, rINST, #8               // r2 <- A
   ldr     r0, [r0, #ART_FIELD_DECLARING_CLASS_OFFSET]
   cmp     rMR, #0
   bne     .Lop_sget_wide_slow_path_read_barrier
.Lop_sget_wide_slow_path_resume_after_read_barrier:
   .if 1
   add     ip, r0, r1
   ATOMIC_LOAD64 ip, r0, r1, r3, .Lop_sget_wide_slow_path_atomic_load
   dmb     ish
   CLEAR_SHADOW_PAIR r2, ip, lr
   VREG_INDEX_TO_ADDR r2, r2
   SET_VREG_WIDE_BY_ADDR r0, r1, r2    // fp[A] <- value
   .else
   ldr   r0, [r0, r1]
   dmb     ish
   .if 0
   cmp     rMR, #0
   bne     .Lop_sget_wide_mark_after_load
   SET_VREG_OBJECT r0, r2              // fp[A] <- value
   .else
   SET_VREG r0, r2                     // fp[A] <- value
   .endif
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.Lop_sget_wide_slow_path_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_sget_wide_slow_path_resume_after_read_barrier

    NAME_END nterp_op_sget_wide_helper
    NAME_START nterp_op_sput_helper_str00
   mov     r0, rSELF
   ldr     r1, [sp]
   mov     r2, rPC
   .if 0
   mov     r3, r4
   .else
   mov     r3, #0
   .endif
   EXPORT_PC
   bl      nterp_get_static_field
   .if 0
   // Reload the value as it may have moved.
   lsr     r4, rINST, #8               // r4 <- A
   GET_VREG r4, r4                     // r4 <- v[A]
   .endif
   tst     r0, #1
   beq     .Lop_sput_resume
   CLEAR_STATIC_VOLATILE_MARKER r0
   ldr     r1, [r0, #ART_FIELD_OFFSET_OFFSET]
   ldr     r0, [r0, #ART_FIELD_DECLARING_CLASS_OFFSET]
   cmp     rMR, #0
   bne     .Lop_sput_slow_path_read_barrier
.Lop_sput_slow_path_resume_after_read_barrier:
   .if 0
   lsr     r2, rINST, #8               // r2 <- A
   VREG_INDEX_TO_ADDR r2, r2
   GET_VREG_WIDE_BY_ADDR r2, r3, r2
   add     ip, r0, r1
   dmb     ish
   ATOMIC_STORE64 ip, r2, r3, r0, r1, .Lop_sput_slow_path_atomic_store
   dmb     ish
   .else
   dmb     ish
   str  r4, [r0, r1]
   dmb     ish
   WRITE_BARRIER_IF_OBJECT 0, r4, r0, .Lop_sput_slow_path_skip_write_barrier, r1
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.Lop_sput_slow_path_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_sput_slow_path_resume_after_read_barrier

    NAME_END nterp_op_sput_helper_str00
    NAME_START nterp_op_sput_helper_str01
   mov     r0, rSELF
   ldr     r1, [sp]
   mov     r2, rPC
   .if 1
   mov     r3, r4
   .else
   mov     r3, #0
   .endif
   EXPORT_PC
   bl      nterp_get_static_field
   .if 1
   // Reload the value as it may have moved.
   lsr     r4, rINST, #8               // r4 <- A
   GET_VREG r4, r4                     // r4 <- v[A]
   .endif
   tst     r0, #1
   beq     .Lop_sput_object_resume
   CLEAR_STATIC_VOLATILE_MARKER r0
   ldr     r1, [r0, #ART_FIELD_OFFSET_OFFSET]
   ldr     r0, [r0, #ART_FIELD_DECLARING_CLASS_OFFSET]
   cmp     rMR, #0
   bne     .Lop_sput_object_slow_path_read_barrier
.Lop_sput_object_slow_path_resume_after_read_barrier:
   .if 0
   lsr     r2, rINST, #8               // r2 <- A
   VREG_INDEX_TO_ADDR r2, r2
   GET_VREG_WIDE_BY_ADDR r2, r3, r2
   add     ip, r0, r1
   dmb     ish
   ATOMIC_STORE64 ip, r2, r3, r0, r1, .Lop_sput_object_slow_path_atomic_store
   dmb     ish
   .else
   dmb     ish
   str  r4, [r0, r1]
   dmb     ish
   WRITE_BARRIER_IF_OBJECT 1, r4, r0, .Lop_sput_object_slow_path_skip_write_barrier, r1
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.Lop_sput_object_slow_path_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_sput_object_slow_path_resume_after_read_barrier

    NAME_END nterp_op_sput_helper_str01
    NAME_START nterp_op_sput_helper_str10
   mov     r0, rSELF
   ldr     r1, [sp]
   mov     r2, rPC
   .if 0
   mov     r3, r4
   .else
   mov     r3, #0
   .endif
   EXPORT_PC
   bl      nterp_get_static_field
   .if 0
   // Reload the value as it may have moved.
   lsr     r4, rINST, #8               // r4 <- A
   GET_VREG r4, r4                     // r4 <- v[A]
   .endif
   tst     r0, #1
   beq     .Lop_sput_wide_resume
   CLEAR_STATIC_VOLATILE_MARKER r0
   ldr     r1, [r0, #ART_FIELD_OFFSET_OFFSET]
   ldr     r0, [r0, #ART_FIELD_DECLARING_CLASS_OFFSET]
   cmp     rMR, #0
   bne     .Lop_sput_wide_slow_path_read_barrier
.Lop_sput_wide_slow_path_resume_after_read_barrier:
   .if 1
   lsr     r2, rINST, #8               // r2 <- A
   VREG_INDEX_TO_ADDR r2, r2
   GET_VREG_WIDE_BY_ADDR r2, r3, r2
   add     ip, r0, r1
   dmb     ish
   ATOMIC_STORE64 ip, r2, r3, r0, r1, .Lop_sput_wide_slow_path_atomic_store
   dmb     ish
   .else
   dmb     ish
   str  r4, [r0, r1]
   dmb     ish
   WRITE_BARRIER_IF_OBJECT 0, r4, r0, .Lop_sput_wide_slow_path_skip_write_barrier, r1
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.Lop_sput_wide_slow_path_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_sput_wide_slow_path_resume_after_read_barrier

    NAME_END nterp_op_sput_helper_str10
    NAME_START nterp_op_sput_helper_strb00
   mov     r0, rSELF
   ldr     r1, [sp]
   mov     r2, rPC
   .if 0
   mov     r3, r4
   .else
   mov     r3, #0
   .endif
   EXPORT_PC
   bl      nterp_get_static_field
   .if 0
   // Reload the value as it may have moved.
   lsr     r4, rINST, #8               // r4 <- A
   GET_VREG r4, r4                     // r4 <- v[A]
   .endif
   tst     r0, #1
   beq     .Lop_sput_byte_resume
   CLEAR_STATIC_VOLATILE_MARKER r0
   ldr     r1, [r0, #ART_FIELD_OFFSET_OFFSET]
   ldr     r0, [r0, #ART_FIELD_DECLARING_CLASS_OFFSET]
   cmp     rMR, #0
   bne     .Lop_sput_byte_slow_path_read_barrier
.Lop_sput_byte_slow_path_resume_after_read_barrier:
   .if 0
   lsr     r2, rINST, #8               // r2 <- A
   VREG_INDEX_TO_ADDR r2, r2
   GET_VREG_WIDE_BY_ADDR r2, r3, r2
   add     ip, r0, r1
   dmb     ish
   ATOMIC_STORE64 ip, r2, r3, r0, r1, .Lop_sput_byte_slow_path_atomic_store
   dmb     ish
   .else
   dmb     ish
   strb  r4, [r0, r1]
   dmb     ish
   WRITE_BARRIER_IF_OBJECT 0, r4, r0, .Lop_sput_byte_slow_path_skip_write_barrier, r1
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.Lop_sput_byte_slow_path_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_sput_byte_slow_path_resume_after_read_barrier

    NAME_END nterp_op_sput_helper_strb00
    NAME_START nterp_op_sput_helper_strh00
   mov     r0, rSELF
   ldr     r1, [sp]
   mov     r2, rPC
   .if 0
   mov     r3, r4
   .else
   mov     r3, #0
   .endif
   EXPORT_PC
   bl      nterp_get_static_field
   .if 0
   // Reload the value as it may have moved.
   lsr     r4, rINST, #8               // r4 <- A
   GET_VREG r4, r4                     // r4 <- v[A]
   .endif
   tst     r0, #1
   beq     .Lop_sput_short_resume
   CLEAR_STATIC_VOLATILE_MARKER r0
   ldr     r1, [r0, #ART_FIELD_OFFSET_OFFSET]
   ldr     r0, [r0, #ART_FIELD_DECLARING_CLASS_OFFSET]
   cmp     rMR, #0
   bne     .Lop_sput_short_slow_path_read_barrier
.Lop_sput_short_slow_path_resume_after_read_barrier:
   .if 0
   lsr     r2, rINST, #8               // r2 <- A
   VREG_INDEX_TO_ADDR r2, r2
   GET_VREG_WIDE_BY_ADDR r2, r3, r2
   add     ip, r0, r1
   dmb     ish
   ATOMIC_STORE64 ip, r2, r3, r0, r1, .Lop_sput_short_slow_path_atomic_store
   dmb     ish
   .else
   dmb     ish
   strh  r4, [r0, r1]
   dmb     ish
   WRITE_BARRIER_IF_OBJECT 0, r4, r0, .Lop_sput_short_slow_path_skip_write_barrier, r1
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.Lop_sput_short_slow_path_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_sput_short_slow_path_resume_after_read_barrier

    NAME_END nterp_op_sput_helper_strh00

/*
 * ===========================================================================
 *  Common subroutines and data
 * ===========================================================================
 */

    .text
    .align  2

// Enclose all code below in a symbol (which gets printed in backtraces).
NAME_START nterp_helper

// Note: mterp also uses the common_* names below for helpers, but that's OK
// as the assembler compiled each interpreter separately.
common_errDivideByZero:
    EXPORT_PC
    bl art_quick_throw_div_zero

// Expect index in r1, length in r3
common_errArrayIndex:
    EXPORT_PC
    mov r0, r1
    mov r1, r3
    bl art_quick_throw_array_bounds

common_errNullObject:
    EXPORT_PC
    bl art_quick_throw_null_pointer_exception

NterpCommonInvokeStatic:
    COMMON_INVOKE_NON_RANGE is_static=1, suffix="invokeStatic"

NterpCommonInvokeStaticRange:
    COMMON_INVOKE_RANGE is_static=1, suffix="invokeStatic"

NterpCommonInvokeInstance:
    COMMON_INVOKE_NON_RANGE suffix="invokeInstance"

NterpCommonInvokeInstanceRange:
    COMMON_INVOKE_RANGE suffix="invokeInstance"

NterpCommonInvokeInterface:
    COMMON_INVOKE_NON_RANGE is_interface=1, suffix="invokeInterface"

NterpCommonInvokeInterfaceRange:
    COMMON_INVOKE_RANGE is_interface=1, suffix="invokeInterface"

NterpCommonInvokePolymorphic:
    COMMON_INVOKE_NON_RANGE is_polymorphic=1, suffix="invokePolymorphic"

NterpCommonInvokePolymorphicRange:
    COMMON_INVOKE_RANGE is_polymorphic=1, suffix="invokePolymorphic"

NterpCommonInvokeCustom:
    COMMON_INVOKE_NON_RANGE is_static=1, is_custom=1, suffix="invokeCustom"

NterpCommonInvokeCustomRange:
    COMMON_INVOKE_RANGE is_static=1, is_custom=1, suffix="invokeCustom"

NterpHandleStringInit:
   COMMON_INVOKE_NON_RANGE is_string_init=1, suffix="stringInit"

NterpHandleStringInitRange:
   COMMON_INVOKE_RANGE is_string_init=1, suffix="stringInit"

NterpNewArray:
   /* new-array vA, vB, class@CCCC */
   EXPORT_PC
   // Fast-path which gets the class from thread-local cache.
   FETCH_FROM_THREAD_CACHE r0, 2f
   cmp rMR, #0
   bne 3f
1:
   lsr     r1, rINST, #12              // r1<- B
   GET_VREG r1, r1                     // r1<- vB (array length)
   ldr lr, [rSELF, #THREAD_ALLOC_ARRAY_ENTRYPOINT_OFFSET]
   blx lr
   ubfx    r1, rINST, #8, #4           // r1<- A
   SET_VREG_OBJECT r0, r1
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
2:
   mov r0, rSELF
   ldr r1, [sp]
   mov r2, rPC
   bl nterp_get_class_or_allocate_object
   b 1b
3:
   bl art_quick_read_barrier_mark_reg00
   b 1b

NterpHandleHotnessOverflow:
    add r1, rPC, rINST, lsl #1
    mov r2, rFP
    bl nterp_hot_method
    cmp r0, #0
    bne 1f
    add r2, rINST, rINST                // w2<- byte offset
    FETCH_ADVANCE_INST_RB r2            // update rPC, load rINST
    GET_INST_OPCODE ip                  // extract opcode from rINST
    GOTO_OPCODE ip                      // jump to next instruction
1:
    // Drop the current frame.
    ldr ip, [rREFS, #-4]
    mov sp, ip
    .cfi_def_cfa sp, CALLEE_SAVES_SIZE

    // The transition frame of type SaveAllCalleeSaves saves r4, r8, and r9,
    // but not managed ABI. So we need to restore callee-saves of the nterp frame,
    // and save managed ABI callee saves, which will be restored by the callee upon
    // return.

    RESTORE_ALL_CALLEE_SAVES
    push {r5-r7, r10-r11, lr}
   .cfi_adjust_cfa_offset 24
   .cfi_rel_offset r5, 0
   .cfi_rel_offset r6, 4
   .cfi_rel_offset r7, 8
   .cfi_rel_offset r10, 12
   .cfi_rel_offset r11, 16
   .cfi_rel_offset lr, 20
    vpush {s16-s31}
    .cfi_adjust_cfa_offset 64

    // Setup the new frame
    ldr r1, [r0, #OSR_DATA_FRAME_SIZE]
    // Given stack size contains all callee saved registers, remove them.
    sub r1, r1, #(CALLEE_SAVES_SIZE - 12)

    // We know r1 cannot be 0, as it at least contains the ArtMethod.

    // Remember CFA in a callee-save register.
    mov rINST, sp
    .cfi_def_cfa_register rINST

    sub sp, sp, r1

    add r2, r0, #OSR_DATA_MEMORY
2:
    sub r1, r1, #4
    ldr ip, [r2, r1]
    str ip, [sp, r1]
    cmp r1, #0
    bne 2b

    // Fetch the native PC to jump to and save it in a callee-save register.
    ldr rFP, [r0, #OSR_DATA_NATIVE_PC]

    // Free the memory holding OSR Data.
    bl free

    // Jump to the compiled code.
    bx rFP
// This is the logical end of ExecuteNterpImpl, where the frame info applies.
// EndExecuteNterpImpl includes the methods below as we want the runtime to
// see them as part of the Nterp PCs.
.cfi_endproc

nterp_to_nterp_static_non_range:
    .cfi_startproc
    SETUP_STACK_FOR_INVOKE
    SETUP_NON_RANGE_ARGUMENTS_AND_EXECUTE is_static=1, is_string_init=0
    .cfi_endproc

nterp_to_nterp_string_init_non_range:
    .cfi_startproc
    SETUP_STACK_FOR_INVOKE
    SETUP_NON_RANGE_ARGUMENTS_AND_EXECUTE is_static=0, is_string_init=1
    .cfi_endproc

nterp_to_nterp_instance_non_range:
    .cfi_startproc
    SETUP_STACK_FOR_INVOKE
    SETUP_NON_RANGE_ARGUMENTS_AND_EXECUTE is_static=0, is_string_init=0
    .cfi_endproc

nterp_to_nterp_static_range:
    .cfi_startproc
    SETUP_STACK_FOR_INVOKE
    SETUP_RANGE_ARGUMENTS_AND_EXECUTE is_static=1, is_string_init=0
    .cfi_endproc

nterp_to_nterp_string_init_range:
    .cfi_startproc
    SETUP_STACK_FOR_INVOKE
    SETUP_RANGE_ARGUMENTS_AND_EXECUTE is_static=0, is_string_init=1
    .cfi_endproc

nterp_to_nterp_instance_range:
    .cfi_startproc
    SETUP_STACK_FOR_INVOKE
    SETUP_RANGE_ARGUMENTS_AND_EXECUTE is_static=0, is_string_init=0
    .cfi_endproc

NAME_END nterp_helper

// This is the end of PCs contained by the OatQuickMethodHeader created for the interpreter
// entry point.
    .type EndExecuteNterpImpl, #function
    .hidden EndExecuteNterpImpl
    .global EndExecuteNterpImpl
EndExecuteNterpImpl:

/*
 * Convert the double in r0/r1 to a long in r0/r1.
 *
 * We have to clip values to long min/max per the specification.  The
 * expected common case is a "reasonable" value that converts directly
 * to modest integer.  The EABI convert function isn't doing this for us.
 */
nterp_d2l_doconv:
    ubfx    r2, r1, #20, #11            @ grab the exponent
    movw    r3, #0x43e
    cmp     r2, r3                      @ MINLONG < x > MAXLONG?
    bhs     d2l_special_cases
    b       __aeabi_d2lz                @ tail call to convert double to long
d2l_special_cases:
    movw    r3, #0x7ff
    cmp     r2, r3
    beq     d2l_maybeNaN                @ NaN?
d2l_notNaN:
    adds    r1, r1, r1                  @ sign bit to carry
    mov     r0, #0xffffffff             @ assume maxlong for lsw
    mov     r1, #0x7fffffff             @ assume maxlong for msw
    adc     r0, r0, #0
    adc     r1, r1, #0                  @ convert maxlong to minlong if exp negative
    bx      lr                          @ return
d2l_maybeNaN:
    orrs    r3, r0, r1, lsl #12
    beq     d2l_notNaN                  @ if fraction is non-zero, it's a NaN
    mov     r0, #0
    mov     r1, #0
    bx      lr                          @ return 0 for NaN

/*
 * Convert the float in r0 to a long in r0/r1.
 *
 * We have to clip values to long min/max per the specification.  The
 * expected common case is a "reasonable" value that converts directly
 * to modest integer.  The EABI convert function isn't doing this for us.
 */
nterp_f2l_doconv:
    ubfx    r2, r0, #23, #8             @ grab the exponent
    cmp     r2, #0xbe                   @ MININT < x > MAXINT?
    bhs     f2l_special_cases
    b       __aeabi_f2lz                @ tail call to convert float to long
f2l_special_cases:
    cmp     r2, #0xff                   @ NaN or infinity?
    beq     f2l_maybeNaN
f2l_notNaN:
    adds    r0, r0, r0                  @ sign bit to carry
    mov     r0, #0xffffffff             @ assume maxlong for lsw
    mov     r1, #0x7fffffff             @ assume maxlong for msw
    adc     r0, r0, #0
    adc     r1, r1, #0                  @ convert maxlong to minlong if exp negative
    bx      lr                          @ return
f2l_maybeNaN:
    lsls    r3, r0, #9
    beq     f2l_notNaN                  @ if fraction is non-zero, it's a NaN
    mov     r0, #0
    mov     r1, #0
    bx      lr                          @ return 0 for NaN

// Entrypoints into runtime.
NTERP_TRAMPOLINE nterp_get_static_field, NterpGetStaticField
NTERP_TRAMPOLINE nterp_get_instance_field_offset, NterpGetInstanceFieldOffset
NTERP_TRAMPOLINE nterp_filled_new_array, NterpFilledNewArray
NTERP_TRAMPOLINE nterp_filled_new_array_range, NterpFilledNewArrayRange
NTERP_TRAMPOLINE nterp_get_class_or_allocate_object, NterpGetClassOrAllocateObject
NTERP_TRAMPOLINE nterp_get_method, NterpGetMethod
NTERP_TRAMPOLINE nterp_hot_method, NterpHotMethod
NTERP_TRAMPOLINE nterp_load_object, NterpLoadObject

// gen_mterp.py will inline the following definitions
// within [ExecuteNterpImpl, EndExecuteNterpImpl).
