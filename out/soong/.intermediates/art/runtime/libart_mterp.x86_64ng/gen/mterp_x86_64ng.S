/* DO NOT EDIT: This file was generated by gen-mterp.py. */
/*
 * Copyright (C) 2019 The Android Open Source Project
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/*
 * This is a #include, not a %include, because we want the C pre-processor
 * to expand the macros into assembler assignment statements.
 */
#include "asm_support.h"
#include "arch/x86_64/asm_support_x86_64.S"

/**
 * x86_64 ABI general notes:
 *
 * Caller save set:
 *    rax, rdx, rcx, rsi, rdi, r8-r11, st(0)-st(7)
 * Callee save set:
 *    rbx, rbp, r12-r15
 * Return regs:
 *    32-bit in eax
 *    64-bit in rax
 *    fp on xmm0
 *
 * First 8 fp parameters came in xmm0-xmm7.
 * First 6 non-fp parameters came in rdi, rsi, rdx, rcx, r8, r9.
 * Other parameters passed on stack, pushed right-to-left.  On entry to target, first
 * param is at 8(%esp).
 *
 * Stack must be 16-byte aligned to support SSE in native code.
 */

#define IN_ARG3        %rcx
#define IN_ARG2        %rdx
#define IN_ARG1        %rsi
#define IN_ARG0        %rdi
/* Out Args  */
#define OUT_ARG3       %rcx
#define OUT_ARG2       %rdx
#define OUT_ARG1       %rsi
#define OUT_ARG0       %rdi
#define OUT_32_ARG3    %ecx
#define OUT_32_ARG2    %edx
#define OUT_32_ARG1    %esi
#define OUT_32_ARG0    %edi
#define OUT_FP_ARG1    %xmm1
#define OUT_FP_ARG0    %xmm0

/*
 * single-purpose registers, given names for clarity
 */
#define rSELF    %gs
#define rPC      %r12
#define CFI_DEX  12 // DWARF register number of the register holding dex-pc (rPC).
#define CFI_TMP  5  // DWARF register number of the first argument register (rdi).
#define rFP      %r13
#define rINST    %ebx
#define rINSTq   %rbx
#define rINSTw   %bx
#define rINSTbh  %bh
#define rINSTbl  %bl
#define rIBASE   %r14
#define rREFS    %r15
#define rREFS32  %r15d
#define CFI_REFS 15 // DWARF register number of the reference array (r15).

// Temporary registers while setting up a frame.
#define rNEW_FP   %r8
#define rNEW_REFS %r9
#define rNEW_REFS32 %r9d
#define CFI_NEW_REFS 9

/*
 * Get/set the 32-bit value from a Dalvik register.
 */
#define VREG_ADDRESS(_vreg) (rFP,_vreg,4)
#define VREG_HIGH_ADDRESS(_vreg) 4(rFP,_vreg,4)
#define VREG_REF_ADDRESS(_vreg) (rREFS,_vreg,4)
#define VREG_REF_HIGH_ADDRESS(_vreg) 4(rREFS,_vreg,4)

// Includes the return address implictly pushed on stack by 'call'.
#define CALLEE_SAVES_SIZE (6 * 8 + 4 * 8 + 1 * 8)

// +8 for the ArtMethod of the caller.
#define OFFSET_TO_FIRST_ARGUMENT_IN_STACK (CALLEE_SAVES_SIZE + 8)

/*
 * Refresh rINST.
 * At enter to handler rINST does not contain the opcode number.
 * However some utilities require the full value, so this macro
 * restores the opcode number.
 */
.macro REFRESH_INST _opnum
    movb    rINSTbl, rINSTbh
    movb    $\_opnum, rINSTbl
.endm

/*
 * Fetch the next instruction from rPC into rINSTw.  Does not advance rPC.
 */
.macro FETCH_INST
    movzwq  (rPC), rINSTq
.endm

/*
 * Remove opcode from rINST, compute the address of handler and jump to it.
 */
.macro GOTO_NEXT
    movzx   rINSTbl,%ecx
    movzbl  rINSTbh,rINST
    shll    MACRO_LITERAL(MTERP_HANDLER_SIZE_LOG2), %ecx
    addq    rIBASE, %rcx
    jmp     *%rcx
.endm

/*
 * Advance rPC by instruction count.
 */
.macro ADVANCE_PC _count
    leaq    2*\_count(rPC), rPC
.endm

/*
 * Advance rPC by instruction count, fetch instruction and jump to handler.
 */
.macro ADVANCE_PC_FETCH_AND_GOTO_NEXT _count
    ADVANCE_PC \_count
    FETCH_INST
    GOTO_NEXT
.endm

.macro GET_VREG _reg _vreg
    movl    VREG_ADDRESS(\_vreg), \_reg
.endm

.macro GET_VREG_OBJECT _reg _vreg
    movl    VREG_REF_ADDRESS(\_vreg), \_reg
.endm

/* Read wide value. */
.macro GET_WIDE_VREG _reg _vreg
    movq    VREG_ADDRESS(\_vreg), \_reg
.endm

.macro SET_VREG _reg _vreg
    movl    \_reg, VREG_ADDRESS(\_vreg)
    movl    MACRO_LITERAL(0), VREG_REF_ADDRESS(\_vreg)
.endm

/* Write wide value. reg is clobbered. */
.macro SET_WIDE_VREG _reg _vreg
    movq    \_reg, VREG_ADDRESS(\_vreg)
    xorq    \_reg, \_reg
    movq    \_reg, VREG_REF_ADDRESS(\_vreg)
.endm

.macro SET_VREG_OBJECT _reg _vreg
    movl    \_reg, VREG_ADDRESS(\_vreg)
    movl    \_reg, VREG_REF_ADDRESS(\_vreg)
.endm

.macro GET_VREG_HIGH _reg _vreg
    movl    VREG_HIGH_ADDRESS(\_vreg), \_reg
.endm

.macro SET_VREG_HIGH _reg _vreg
    movl    \_reg, VREG_HIGH_ADDRESS(\_vreg)
    movl    MACRO_LITERAL(0), VREG_REF_HIGH_ADDRESS(\_vreg)
.endm

.macro CLEAR_REF _vreg
    movl    MACRO_LITERAL(0), VREG_REF_ADDRESS(\_vreg)
.endm

.macro CLEAR_WIDE_REF _vreg
    movl    MACRO_LITERAL(0), VREG_REF_ADDRESS(\_vreg)
    movl    MACRO_LITERAL(0), VREG_REF_HIGH_ADDRESS(\_vreg)
.endm

.macro GET_VREG_XMMs _xmmreg _vreg
    movss VREG_ADDRESS(\_vreg), \_xmmreg
.endm
.macro GET_VREG_XMMd _xmmreg _vreg
    movsd VREG_ADDRESS(\_vreg), \_xmmreg
.endm
.macro SET_VREG_XMMs _xmmreg _vreg
    movss \_xmmreg, VREG_ADDRESS(\_vreg)
.endm
.macro SET_VREG_XMMd _xmmreg _vreg
    movsd \_xmmreg, VREG_ADDRESS(\_vreg)
.endm

// An assembly entry that has a OatQuickMethodHeader prefix.
.macro OAT_ENTRY name, end
    FUNCTION_TYPE(\name)
    ASM_HIDDEN SYMBOL(\name)
    .global SYMBOL(\name)
    .balign 16
    // Padding of 3 * 8 bytes to get 16 bytes alignment of code entry.
    .long 0
    .long 0
    .long 0
    // OatQuickMethodHeader. Note that the top two bits must be clear.
    .long (SYMBOL(\end) - SYMBOL(\name))
SYMBOL(\name):
.endm

.macro ENTRY name
    .text
    ASM_HIDDEN SYMBOL(\name)
    .global SYMBOL(\name)
    FUNCTION_TYPE(\name)
SYMBOL(\name):
.endm

.macro END name
    SIZE(\name)
.endm

// Macro for defining entrypoints into runtime. We don't need to save registers
// (we're not holding references there), but there is no
// kDontSave runtime method. So just use the kSaveRefsOnly runtime method.
.macro NTERP_TRAMPOLINE name, helper
DEFINE_FUNCTION \name
  SETUP_SAVE_REFS_ONLY_FRAME
  call \helper
  RESTORE_SAVE_REFS_ONLY_FRAME
  RETURN_OR_DELIVER_PENDING_EXCEPTION
END_FUNCTION \name
.endm

.macro CLEAR_VOLATILE_MARKER reg
  andq MACRO_LITERAL(-2), \reg
.endm

.macro EXPORT_PC
    movq    rPC, -16(rREFS)
.endm

.macro BRANCH
    // Update method counter and do a suspend check if the branch is negative.
    testq rINSTq, rINSTq
    js 3f
2:
    leaq    (rPC, rINSTq, 2), rPC
    FETCH_INST
    GOTO_NEXT
3:
    movq (%rsp), %rdi
    addw $1, ART_METHOD_HOTNESS_COUNT_OFFSET(%rdi)
    andw $(NTERP_HOTNESS_MASK), ART_METHOD_HOTNESS_COUNT_OFFSET(%rdi)
    // If the counter overflows, handle this in the runtime.
    jz NterpHandleHotnessOverflow
    // Otherwise, do a suspend check.
    testl   $(THREAD_SUSPEND_OR_CHECKPOINT_REQUEST), rSELF:THREAD_FLAGS_OFFSET
    jz      2b
    EXPORT_PC
    call    SYMBOL(art_quick_test_suspend)
    jmp 2b
.endm

// Expects:
// - r10, and r11 to be available.
// Outputs:
// - \registers contains the dex registers size
// - \outs contains the outs size
// - if load_ins is 1, \ins contains the ins
// - \code_item is replace with a pointer to the instructions
.macro FETCH_CODE_ITEM_INFO code_item, registers, outs, ins, load_ins
    testq MACRO_LITERAL(1), \code_item
    je 5f
    andq $-2, \code_item  // Remove the extra bit that marks it's a compact dex file.
    movzwl COMPACT_CODE_ITEM_FIELDS_OFFSET(\code_item), %r10d
    movl %r10d, \registers
    sarl $COMPACT_CODE_ITEM_REGISTERS_SIZE_SHIFT, \registers
    andl $0xf, \registers
    movl %r10d, \outs
    sarl $COMPACT_CODE_ITEM_OUTS_SIZE_SHIFT, \outs
    andl $0xf, \outs
    .if \load_ins
    movl %r10d, \ins
    sarl $COMPACT_CODE_ITEM_INS_SIZE_SHIFT, \ins
    andl $0xf, \ins
    .else
    movl %r10d, %r11d
    sarl $COMPACT_CODE_ITEM_INS_SIZE_SHIFT, %r11d
    andl $0xf, %r11d
    addl %r11d, \registers
    .endif
    testw $COMPACT_CODE_ITEM_REGISTERS_INS_OUTS_FLAGS, COMPACT_CODE_ITEM_FLAGS_OFFSET(\code_item)
    je 4f
    movq \code_item, %r11
    testw $COMPACT_CODE_ITEM_INSNS_FLAG, COMPACT_CODE_ITEM_FLAGS_OFFSET(\code_item)
    je 1f
    subq $4, %r11
1:
    testw $COMPACT_CODE_ITEM_REGISTERS_FLAG, COMPACT_CODE_ITEM_FLAGS_OFFSET(\code_item)
    je 2f
    subq $2, %r11
    movzwl (%r11), %r10d
    addl %r10d, \registers
2:
    testw $COMPACT_CODE_ITEM_INS_FLAG, COMPACT_CODE_ITEM_FLAGS_OFFSET(\code_item)
    je 3f
    subq $2, %r11
    movzwl (%r11), %r10d
    .if \load_ins
    addl %r10d, \ins
    .else
    addl %r10d, \registers
    .endif
3:
    testw $COMPACT_CODE_ITEM_OUTS_FLAG, COMPACT_CODE_ITEM_FLAGS_OFFSET(\code_item)
    je 4f
    subq $2, %r11
    movzwl (%r11), %r10d
    addl %r10d, \outs
4:
    .if \load_ins
    addl \ins, \registers
    .endif
    addq $COMPACT_CODE_ITEM_INSNS_OFFSET, \code_item
    jmp 6f
5:
    // Fetch dex register size.
    movzwl CODE_ITEM_REGISTERS_SIZE_OFFSET(\code_item), \registers
    // Fetch outs size.
    movzwl CODE_ITEM_OUTS_SIZE_OFFSET(\code_item), \outs
    .if \load_ins
    movzwl CODE_ITEM_INS_SIZE_OFFSET(\code_item), \ins
    .endif
    addq $CODE_ITEM_INSNS_OFFSET, \code_item
6:
.endm

// Setup the stack to start executing the method. Expects:
// - rdi to contain the ArtMethod
// - rbx, r10, r11 to be available.
//
// Outputs
// - rbx contains the dex registers size
// - r11 contains the old stack pointer.
// - \code_item is replace with a pointer to the instructions
// - if load_ins is 1, r14 contains the ins
.macro SETUP_STACK_FRAME code_item, refs, refs32, fp, cfi_refs, load_ins
    FETCH_CODE_ITEM_INFO \code_item, %ebx, \refs32, %r14d, \load_ins

    // Compute required frame size for dex registers: ((2 * ebx) + refs)
    leaq (\refs, %rbx, 2), %r11
    salq $2, %r11

    // Compute new stack pointer in r10: add 24 for saving the previous frame,
    // pc, and method being executed.
    leaq -24(%rsp), %r10
    subq %r11, %r10
    // Alignment
    // Note: There may be two pieces of alignment but there is no need to align
    // out args to `kPointerSize` separately before aligning to kStackAlignment.
    andq $-16, %r10

    // Set reference and dex registers, align to pointer size for previous frame and dex pc.
    leaq 24 + 4(%r10, \refs, 4), \refs
    andq LITERAL(-__SIZEOF_POINTER__), \refs
    leaq (\refs, %rbx, 4), \fp

    // Now setup the stack pointer.
    movq %rsp, %r11
    CFI_DEF_CFA_REGISTER(r11)
    movq %r10, %rsp
    movq %r11, -8(\refs)
    CFI_DEF_CFA_BREG_PLUS_UCONST \cfi_refs, -8, ((6 + 4 + 1) * 8)

    // Put nulls in reference frame.
    testl %ebx, %ebx
    je 2f
    movq \refs, %r10
1:
    movl $0, (%r10)
    addq $4, %r10
    cmpq %r10, \fp
    jne 1b
2:
    // Save the ArtMethod.
    movq %rdi, (%rsp)
.endm

// Puts the next floating point argument into the expected register,
// fetching values based on a non-range invoke.
// Uses rax as temporary.
//
// TODO: We could simplify a lot of code by loading the G argument into
// the "inst" register. Given that we enter the handler with "1(rPC)" in
// the rINST, we can just add rINST<<16 to the args and we don't even
// need to pass "arg_index" around.
.macro LOOP_OVER_SHORTY_LOADING_XMMS xmm_reg, inst, shorty, arg_index, finished
1: // LOOP
    movb (REG_VAR(shorty)), %al             // bl := *shorty
    addq MACRO_LITERAL(1), REG_VAR(shorty)  // shorty++
    cmpb MACRO_LITERAL(0), %al              // if (al == '\0') goto finished
    je VAR(finished)
    cmpb MACRO_LITERAL(68), %al             // if (al == 'D') goto FOUND_DOUBLE
    je 2f
    cmpb MACRO_LITERAL(70), %al             // if (al == 'F') goto FOUND_FLOAT
    je 3f
    shrq MACRO_LITERAL(4), REG_VAR(inst)
    addq MACRO_LITERAL(1), REG_VAR(arg_index)
    //  Handle extra argument in arg array taken by a long.
    cmpb MACRO_LITERAL(74), %al   // if (al != 'J') goto LOOP
    jne 1b
    shrq MACRO_LITERAL(4), REG_VAR(inst)
    addq MACRO_LITERAL(1), REG_VAR(arg_index)
    jmp 1b                        // goto LOOP
2:  // FOUND_DOUBLE
    subq MACRO_LITERAL(8), %rsp
    movq REG_VAR(inst), %rax
    andq MACRO_LITERAL(0xf), %rax
    GET_VREG %eax, %rax
    movl %eax, (%rsp)
    shrq MACRO_LITERAL(4), REG_VAR(inst)
    addq MACRO_LITERAL(1), REG_VAR(arg_index)
    cmpq MACRO_LITERAL(4), REG_VAR(arg_index)
    je 5f
    movq REG_VAR(inst), %rax
    andq MACRO_LITERAL(0xf), %rax
    shrq MACRO_LITERAL(4), REG_VAR(inst)
    addq MACRO_LITERAL(1), REG_VAR(arg_index)
    jmp 6f
5:
    movzbl 1(rPC), %eax
    andq MACRO_LITERAL(0xf), %rax
6:
    GET_VREG %eax, %rax
    movl %eax, 4(%rsp)
    movsd (%rsp), REG_VAR(xmm_reg)
    addq MACRO_LITERAL(8), %rsp
    jmp 4f
3:  // FOUND_FLOAT
    cmpq MACRO_LITERAL(4), REG_VAR(arg_index)
    je 7f
    movq REG_VAR(inst), %rax
    andq MACRO_LITERAL(0xf), %rax
    shrq MACRO_LITERAL(4), REG_VAR(inst)
    addq MACRO_LITERAL(1), REG_VAR(arg_index)
    jmp 8f
7:
    movzbl 1(rPC), %eax
    andq MACRO_LITERAL(0xf), %rax
8:
    GET_VREG_XMMs REG_VAR(xmm_reg), %rax
4:
.endm

// Puts the next int/long/object argument in the expected register,
// fetching values based on a non-range invoke.
// Uses rax as temporary.
.macro LOOP_OVER_SHORTY_LOADING_GPRS gpr_reg64, gpr_reg32, inst, shorty, arg_index, finished
1: // LOOP
    movb (REG_VAR(shorty)), %al   // al := *shorty
    addq MACRO_LITERAL(1), REG_VAR(shorty)  // shorty++
    cmpb MACRO_LITERAL(0), %al    // if (al == '\0') goto finished
    je  VAR(finished)
    cmpb MACRO_LITERAL(74), %al   // if (al == 'J') goto FOUND_LONG
    je 2f
    cmpb MACRO_LITERAL(70), %al   // if (al == 'F') goto SKIP_FLOAT
    je 3f
    cmpb MACRO_LITERAL(68), %al   // if (al == 'D') goto SKIP_DOUBLE
    je 4f
    cmpq MACRO_LITERAL(4), REG_VAR(arg_index)
    je 7f
    movq REG_VAR(inst), %rax
    andq MACRO_LITERAL(0xf), %rax
    shrq MACRO_LITERAL(4), REG_VAR(inst)
    addq MACRO_LITERAL(1), REG_VAR(arg_index)
    jmp 8f
7:
    movzbl 1(rPC), %eax
    andq MACRO_LITERAL(0xf), %rax
8:
    GET_VREG REG_VAR(gpr_reg32), %rax
    jmp 5f
2:  // FOUND_LONG
    subq MACRO_LITERAL(8), %rsp
    movq REG_VAR(inst), %rax
    andq MACRO_LITERAL(0xf), %rax
    GET_VREG %eax, %rax
    movl %eax, (%rsp)
    shrq MACRO_LITERAL(4), REG_VAR(inst)
    addq MACRO_LITERAL(1), REG_VAR(arg_index)
    cmpq MACRO_LITERAL(4), REG_VAR(arg_index)
    je 9f
    movq REG_VAR(inst), %rax
    andq MACRO_LITERAL(0xf), %rax
    shrq MACRO_LITERAL(4), REG_VAR(inst)
    addq MACRO_LITERAL(1), REG_VAR(arg_index)
    jmp 10f
9:
    movzbl 1(rPC), %eax
    andq MACRO_LITERAL(0xf), %rax
10:
    GET_VREG %eax, %rax
    movl %eax, 4(%rsp)
    movq (%rsp), REG_VAR(gpr_reg64)
    addq MACRO_LITERAL(8), %rsp
    jmp 5f
3:  // SKIP_FLOAT
    shrq MACRO_LITERAL(4), REG_VAR(inst)
    addq MACRO_LITERAL(1), REG_VAR(arg_index)
    jmp 1b
4:  // SKIP_DOUBLE
    shrq MACRO_LITERAL(4), REG_VAR(inst)
    addq MACRO_LITERAL(1), REG_VAR(arg_index)
    cmpq MACRO_LITERAL(4), REG_VAR(arg_index)
    je 1b
    shrq MACRO_LITERAL(4), REG_VAR(inst)
    addq MACRO_LITERAL(1), REG_VAR(arg_index)
    jmp 1b
5:
.endm

// Puts the next floating point argument into the expected register,
// fetching values based on a range invoke.
// Uses rax as temporary.
.macro LOOP_RANGE_OVER_SHORTY_LOADING_XMMS xmm_reg, shorty, arg_index, stack_index, finished
1: // LOOP
    movb (REG_VAR(shorty)), %al             // al := *shorty
    addq MACRO_LITERAL(1), REG_VAR(shorty)  // shorty++
    cmpb MACRO_LITERAL(0), %al              // if (al == '\0') goto finished
    je VAR(finished)
    cmpb MACRO_LITERAL(68), %al             // if (al == 'D') goto FOUND_DOUBLE
    je 2f
    cmpb MACRO_LITERAL(70), %al             // if (al == 'F') goto FOUND_FLOAT
    je 3f
    addq MACRO_LITERAL(1), REG_VAR(arg_index)
    addq MACRO_LITERAL(1), REG_VAR(stack_index)
    //  Handle extra argument in arg array taken by a long.
    cmpb MACRO_LITERAL(74), %al   // if (al != 'J') goto LOOP
    jne 1b
    addq MACRO_LITERAL(1), REG_VAR(arg_index)
    addq MACRO_LITERAL(1), REG_VAR(stack_index)
    jmp 1b                        // goto LOOP
2:  // FOUND_DOUBLE
    GET_VREG_XMMd REG_VAR(xmm_reg), REG_VAR(arg_index)
    addq MACRO_LITERAL(2), REG_VAR(arg_index)
    addq MACRO_LITERAL(2), REG_VAR(stack_index)
    jmp 4f
3:  // FOUND_FLOAT
    GET_VREG_XMMs REG_VAR(xmm_reg), REG_VAR(arg_index)
    addq MACRO_LITERAL(1), REG_VAR(arg_index)
    addq MACRO_LITERAL(1), REG_VAR(stack_index)
4:
.endm

// Puts the next floating point argument into the expected stack slot,
// fetching values based on a range invoke.
// Uses rax as temporary.
//
// TODO: We could just copy all the vregs to the stack slots in a simple loop
// (or REP MOVSD) without looking at the shorty at all. (We could also drop
// the "stack_index" from the macros for loading registers.) We could also do
// that conditionally if argument word count > 6; otherwise we know that all
// args fit into registers.
.macro LOOP_RANGE_OVER_FPs shorty, arg_index, stack_index, finished
1: // LOOP
    movb (REG_VAR(shorty)), %al             // bl := *shorty
    addq MACRO_LITERAL(1), REG_VAR(shorty)  // shorty++
    cmpb MACRO_LITERAL(0), %al              // if (al == '\0') goto finished
    je VAR(finished)
    cmpb MACRO_LITERAL(68), %al             // if (al == 'D') goto FOUND_DOUBLE
    je 2f
    cmpb MACRO_LITERAL(70), %al             // if (al == 'F') goto FOUND_FLOAT
    je 3f
    addq MACRO_LITERAL(1), REG_VAR(arg_index)
    addq MACRO_LITERAL(1), REG_VAR(stack_index)
    //  Handle extra argument in arg array taken by a long.
    cmpb MACRO_LITERAL(74), %al   // if (al != 'J') goto LOOP
    jne 1b
    addq MACRO_LITERAL(1), REG_VAR(arg_index)
    addq MACRO_LITERAL(1), REG_VAR(stack_index)
    jmp 1b                        // goto LOOP
2:  // FOUND_DOUBLE
    movq (rFP, REG_VAR(arg_index), 4), %rax
    movq %rax, 8(%rsp, REG_VAR(stack_index), 4)
    addq MACRO_LITERAL(2), REG_VAR(arg_index)
    addq MACRO_LITERAL(2), REG_VAR(stack_index)
    jmp 1b
3:  // FOUND_FLOAT
    movl (rFP, REG_VAR(arg_index), 4), %eax
    movl %eax, 8(%rsp, REG_VAR(stack_index), 4)
    addq MACRO_LITERAL(1), REG_VAR(arg_index)
    addq MACRO_LITERAL(1), REG_VAR(stack_index)
    jmp 1b
.endm

// Puts the next int/long/object argument in the expected register,
// fetching values based on a range invoke.
// Uses rax as temporary.
.macro LOOP_RANGE_OVER_SHORTY_LOADING_GPRS gpr_reg64, gpr_reg32, shorty, arg_index, stack_index, finished
1: // LOOP
    movb (REG_VAR(shorty)), %al             // al := *shorty
    addq MACRO_LITERAL(1), REG_VAR(shorty)  // shorty++
    cmpb MACRO_LITERAL(0), %al    // if (al == '\0') goto finished
    je  VAR(finished)
    cmpb MACRO_LITERAL(74), %al   // if (al == 'J') goto FOUND_LONG
    je 2f
    cmpb MACRO_LITERAL(70), %al   // if (al == 'F') goto SKIP_FLOAT
    je 3f
    cmpb MACRO_LITERAL(68), %al   // if (al == 'D') goto SKIP_DOUBLE
    je 4f
    movl       (rFP, REG_VAR(arg_index), 4), REG_VAR(gpr_reg32)
    addq MACRO_LITERAL(1), REG_VAR(arg_index)
    addq MACRO_LITERAL(1), REG_VAR(stack_index)
    jmp 5f
2:  // FOUND_LONG
    movq (rFP, REG_VAR(arg_index), 4), REG_VAR(gpr_reg64)
    addq MACRO_LITERAL(2), REG_VAR(arg_index)
    addq MACRO_LITERAL(2), REG_VAR(stack_index)
    jmp 5f
3:  // SKIP_FLOAT
    addq MACRO_LITERAL(1), REG_VAR(arg_index)
    addq MACRO_LITERAL(1), REG_VAR(stack_index)
    jmp 1b
4:  // SKIP_DOUBLE
    addq MACRO_LITERAL(2), REG_VAR(arg_index)
    addq MACRO_LITERAL(2), REG_VAR(stack_index)
    jmp 1b
5:
.endm

// Puts the next int/long/object argument in the expected stack slot,
// fetching values based on a range invoke.
// Uses rax as temporary.
.macro LOOP_RANGE_OVER_INTs shorty, arg_index, stack_index, finished
1: // LOOP
    movb (REG_VAR(shorty)), %al             // al := *shorty
    addq MACRO_LITERAL(1), REG_VAR(shorty)  // shorty++
    cmpb MACRO_LITERAL(0), %al    // if (al == '\0') goto finished
    je  VAR(finished)
    cmpb MACRO_LITERAL(74), %al   // if (al == 'J') goto FOUND_LONG
    je 2f
    cmpb MACRO_LITERAL(70), %al   // if (al == 'F') goto SKIP_FLOAT
    je 3f
    cmpb MACRO_LITERAL(68), %al   // if (al == 'D') goto SKIP_DOUBLE
    je 4f
    movl (rFP, REG_VAR(arg_index), 4), %eax
    movl %eax, 8(%rsp, REG_VAR(stack_index), 4)
    addq MACRO_LITERAL(1), REG_VAR(arg_index)
    addq MACRO_LITERAL(1), REG_VAR(stack_index)
    jmp 1b
2:  // FOUND_LONG
    movq (rFP, REG_VAR(arg_index), 4), %rax
    movq %rax, 8(%rsp, REG_VAR(stack_index), 4)
    addq MACRO_LITERAL(2), REG_VAR(arg_index)
    addq MACRO_LITERAL(2), REG_VAR(stack_index)
    jmp 1b
3:  // SKIP_FLOAT
    addq MACRO_LITERAL(1), REG_VAR(arg_index)
    addq MACRO_LITERAL(1), REG_VAR(stack_index)
    jmp 1b
4:  // SKIP_DOUBLE
    addq MACRO_LITERAL(2), REG_VAR(arg_index)
    addq MACRO_LITERAL(2), REG_VAR(stack_index)
    jmp 1b
.endm

// Puts the next floating point parameter passed in physical register
// in the expected dex register array entry.
// Uses rax as temporary.
.macro LOOP_OVER_SHORTY_STORING_XMMS xmm_reg, shorty, arg_index, fp, finished
1: // LOOP
    movb (REG_VAR(shorty)), %al             // al := *shorty
    addq MACRO_LITERAL(1), REG_VAR(shorty)  // shorty++
    cmpb MACRO_LITERAL(0), %al              // if (al == '\0') goto finished
    je VAR(finished)
    cmpb MACRO_LITERAL(68), %al             // if (al == 'D') goto FOUND_DOUBLE
    je 2f
    cmpb MACRO_LITERAL(70), %al             // if (al == 'F') goto FOUND_FLOAT
    je 3f
    addq MACRO_LITERAL(1), REG_VAR(arg_index)
    //  Handle extra argument in arg array taken by a long.
    cmpb MACRO_LITERAL(74), %al   // if (al != 'J') goto LOOP
    jne 1b
    addq MACRO_LITERAL(1), REG_VAR(arg_index)
    jmp 1b                        // goto LOOP
2:  // FOUND_DOUBLE
    movsd REG_VAR(xmm_reg),(REG_VAR(fp), REG_VAR(arg_index), 4)
    addq MACRO_LITERAL(2), REG_VAR(arg_index)
    jmp 4f
3:  // FOUND_FLOAT
    movss REG_VAR(xmm_reg), (REG_VAR(fp), REG_VAR(arg_index), 4)
    addq MACRO_LITERAL(1), REG_VAR(arg_index)
4:
.endm

// Puts the next int/long/object parameter passed in physical register
// in the expected dex register array entry, and in case of object in the
// expected reference array entry.
// Uses rax as temporary.
.macro LOOP_OVER_SHORTY_STORING_GPRS gpr_reg64, gpr_reg32, shorty, arg_index, regs, refs, finished
1: // LOOP
    movb (REG_VAR(shorty)), %al             // al := *shorty
    addq MACRO_LITERAL(1), REG_VAR(shorty)  // shorty++
    cmpb MACRO_LITERAL(0), %al    // if (al == '\0') goto finished
    je  VAR(finished)
    cmpb MACRO_LITERAL(74), %al   // if (al == 'J') goto FOUND_LONG
    je 2f
    cmpb MACRO_LITERAL(70), %al   // if (al == 'F') goto SKIP_FLOAT
    je 3f
    cmpb MACRO_LITERAL(68), %al   // if (al == 'D') goto SKIP_DOUBLE
    je 4f
    movl REG_VAR(gpr_reg32), (REG_VAR(regs), REG_VAR(arg_index), 4)
    cmpb MACRO_LITERAL(76), %al   // if (al != 'L') goto NOT_REFERENCE
    jne 6f
    movl REG_VAR(gpr_reg32), (REG_VAR(refs), REG_VAR(arg_index), 4)
6:  // NOT_REFERENCE
    addq MACRO_LITERAL(1), REG_VAR(arg_index)
    jmp 5f
2:  // FOUND_LONG
    movq REG_VAR(gpr_reg64), (REG_VAR(regs), REG_VAR(arg_index), 4)
    addq MACRO_LITERAL(2), REG_VAR(arg_index)
    jmp 5f
3:  // SKIP_FLOAT
    addq MACRO_LITERAL(1), REG_VAR(arg_index)
    jmp 1b
4:  // SKIP_DOUBLE
    addq MACRO_LITERAL(2), REG_VAR(arg_index)
    jmp 1b
5:
.endm

// Puts the next floating point parameter passed in stack
// in the expected dex register array entry.
// Uses rax as temporary.
//
// TODO: Or we could just spill regs to the reserved slots in the caller's
// frame and copy all regs in a simple loop. This time, however, we would
// need to look at the shorty anyway to look for the references.
// (The trade-off is different for passing arguments and receiving them.)
.macro LOOP_OVER_FPs shorty, arg_index, regs, stack_ptr, finished
1: // LOOP
    movb (REG_VAR(shorty)), %al             // al := *shorty
    addq MACRO_LITERAL(1), REG_VAR(shorty)  // shorty++
    cmpb MACRO_LITERAL(0), %al              // if (al == '\0') goto finished
    je VAR(finished)
    cmpb MACRO_LITERAL(68), %al             // if (al == 'D') goto FOUND_DOUBLE
    je 2f
    cmpb MACRO_LITERAL(70), %al             // if (al == 'F') goto FOUND_FLOAT
    je 3f
    addq MACRO_LITERAL(1), REG_VAR(arg_index)
    //  Handle extra argument in arg array taken by a long.
    cmpb MACRO_LITERAL(74), %al   // if (al != 'J') goto LOOP
    jne 1b
    addq MACRO_LITERAL(1), REG_VAR(arg_index)
    jmp 1b                        // goto LOOP
2:  // FOUND_DOUBLE
    movq OFFSET_TO_FIRST_ARGUMENT_IN_STACK(REG_VAR(stack_ptr), REG_VAR(arg_index), 4), %rax
    movq %rax, (REG_VAR(regs), REG_VAR(arg_index), 4)
    addq MACRO_LITERAL(2), REG_VAR(arg_index)
    jmp 1b
3:  // FOUND_FLOAT
    movl OFFSET_TO_FIRST_ARGUMENT_IN_STACK(REG_VAR(stack_ptr), REG_VAR(arg_index), 4), %eax
    movl %eax, (REG_VAR(regs), REG_VAR(arg_index), 4)
    addq MACRO_LITERAL(1), REG_VAR(arg_index)
    jmp 1b
.endm

// Puts the next int/long/object parameter passed in stack
// in the expected dex register array entry, and in case of object in the
// expected reference array entry.
// Uses rax as temporary.
.macro LOOP_OVER_INTs shorty, arg_index, regs, refs, stack_ptr, finished
1: // LOOP
    movb (REG_VAR(shorty)), %al             // al := *shorty
    addq MACRO_LITERAL(1), REG_VAR(shorty)  // shorty++
    cmpb MACRO_LITERAL(0), %al    // if (al == '\0') goto finished
    je  VAR(finished)
    cmpb MACRO_LITERAL(74), %al   // if (al == 'J') goto FOUND_LONG
    je 2f
    cmpb MACRO_LITERAL(76), %al   // if (al == 'L') goto FOUND_REFERENCE
    je 6f
    cmpb MACRO_LITERAL(70), %al   // if (al == 'F') goto SKIP_FLOAT
    je 3f
    cmpb MACRO_LITERAL(68), %al   // if (al == 'D') goto SKIP_DOUBLE
    je 4f
    movl OFFSET_TO_FIRST_ARGUMENT_IN_STACK(REG_VAR(stack_ptr), REG_VAR(arg_index), 4), %eax
    movl %eax, (REG_VAR(regs), REG_VAR(arg_index), 4)
    addq MACRO_LITERAL(1), REG_VAR(arg_index)
    jmp 1b
6:  // FOUND_REFERENCE
    movl OFFSET_TO_FIRST_ARGUMENT_IN_STACK(REG_VAR(stack_ptr), REG_VAR(arg_index), 4), %eax
    movl %eax, (REG_VAR(regs), REG_VAR(arg_index), 4)
    movl %eax, (REG_VAR(refs), REG_VAR(arg_index), 4)
    addq MACRO_LITERAL(1), REG_VAR(arg_index)
    jmp 1b
2:  // FOUND_LONG
    movq OFFSET_TO_FIRST_ARGUMENT_IN_STACK(REG_VAR(stack_ptr), REG_VAR(arg_index), 4), %rax
    movq %rax, (REG_VAR(regs), REG_VAR(arg_index), 4)
    addq MACRO_LITERAL(2), REG_VAR(arg_index)
    jmp 1b
3:  // SKIP_FLOAT
    addq MACRO_LITERAL(1), REG_VAR(arg_index)
    jmp 1b
4:  // SKIP_DOUBLE
    addq MACRO_LITERAL(2), REG_VAR(arg_index)
    jmp 1b
.endm

// Increase method hotness and do suspend check before starting executing the method.
.macro START_EXECUTING_INSTRUCTIONS
   movq (%rsp), %rdi
   addw $1, ART_METHOD_HOTNESS_COUNT_OFFSET(%rdi)
   andw $(NTERP_HOTNESS_MASK), ART_METHOD_HOTNESS_COUNT_OFFSET(%rdi)
   jz 2f
   testl $(THREAD_SUSPEND_OR_CHECKPOINT_REQUEST), rSELF:THREAD_FLAGS_OFFSET
   jz 1f
   EXPORT_PC
   call SYMBOL(art_quick_test_suspend)
1:
   FETCH_INST
   GOTO_NEXT
2:
   movq $0, %rsi
   movq rFP, %rdx
   call nterp_hot_method
   jmp 1b
.endm

.macro SPILL_ALL_CALLEE_SAVES
    PUSH r15
    PUSH r14
    PUSH r13
    PUSH r12
    PUSH rbp
    PUSH rbx
    SETUP_FP_CALLEE_SAVE_FRAME
.endm

.macro RESTORE_ALL_CALLEE_SAVES
    RESTORE_FP_CALLEE_SAVE_FRAME
    POP rbx
    POP rbp
    POP r12
    POP r13
    POP r14
    POP r15
.endm

// Helper to setup the stack after doing a nterp to nterp call. This will setup:
// - rNEW_FP: the new pointer to dex registers
// - rNEW_REFS: the new pointer to references
// - rPC: the new PC pointer to execute
// - edi: number of arguments
// - ecx: first dex register
//
// This helper expects:
// - rax to contain the code item
.macro SETUP_STACK_FOR_INVOKE
   // We do the same stack overflow check as the compiler. See CanMethodUseNterp
   // in how we limit the maximum nterp frame size.
   testq %rax, -STACK_OVERFLOW_RESERVED_BYTES(%rsp)

   // Spill all callee saves to have a consistent stack frame whether we
   // are called by compiled code or nterp.
   SPILL_ALL_CALLEE_SAVES

   // Setup the frame.
   SETUP_STACK_FRAME %rax, rNEW_REFS, rNEW_REFS32, rNEW_FP, CFI_NEW_REFS, load_ins=0
   // Make r11 point to the top of the dex register array.
   leaq (rNEW_FP, %rbx, 4), %r11

   // Fetch instruction information before replacing rPC.
   movzbl 1(rPC), %edi
   movzwl 4(rPC), %ecx

   // Set the dex pc pointer.
   movq %rax, rPC
   CFI_DEFINE_DEX_PC_WITH_OFFSET(CFI_TMP, CFI_DEX, 0)
.endm

// Setup arguments based on a non-range nterp to nterp call, and start executing
// the method. We expect:
// - rNEW_FP: the new pointer to dex registers
// - rNEW_REFS: the new pointer to references
// - rPC: the new PC pointer to execute
// - edi: number of arguments
// - ecx: first dex register
// - r11: top of dex register array
// - esi: receiver if non-static.
.macro SETUP_NON_RANGE_ARGUMENTS_AND_EXECUTE is_static=0, is_string_init=0
   // Now all temporary registers (except r11 containing top of registers array)
   // are available, copy the parameters.
   // /* op vA, vB, {vC...vG} */
   movl %edi, %eax
   shrl $4, %eax # Number of arguments
   jz 6f  # shl sets the Z flag
   movq MACRO_LITERAL(-1), %r10
   cmpl MACRO_LITERAL(2), %eax
   jl 1f
   je 2f
   cmpl MACRO_LITERAL(4), %eax
   jl 3f
   je 4f

  // We use a decrementing r10 to store references relative
  // to rNEW_FP and dex registers relative to r11.
  //
  // TODO: We could set up r10 as the number of registers (this can be an additional output from
  // SETUP_STACK_FOR_INVOKE) and then just decrement it by one before copying each arg to
  // (rNEW_FP, r10, 4) and (rNEW_REFS, r10, 4).
  // Maybe even introduce macros NEW_VREG_ADDRESS/NEW_VREG_REF_ADDRESS.
5:
   andq        MACRO_LITERAL(15), %rdi
   GET_VREG_OBJECT %edx, %rdi
   movl        %edx, (rNEW_FP, %r10, 4)
   GET_VREG    %edx, %rdi
   movl        %edx, (%r11, %r10, 4)
   subq        MACRO_LITERAL(1), %r10
4:
   movl        %ecx, %eax
   shrl        MACRO_LITERAL(12), %eax
   GET_VREG_OBJECT %edx, %rax
   movl        %edx, (rNEW_FP, %r10, 4)
   GET_VREG    %edx, %rax
   movl        %edx, (%r11, %r10, 4)
   subq        MACRO_LITERAL(1), %r10
3:
   movl        %ecx, %eax
   shrl        MACRO_LITERAL(8), %eax
   andl        MACRO_LITERAL(0xf), %eax
   GET_VREG_OBJECT %edx, %rax
   movl        %edx, (rNEW_FP, %r10, 4)
   GET_VREG    %edx, %rax
   movl        %edx, (%r11, %r10, 4)
   subq        MACRO_LITERAL(1), %r10
2:
   movl        %ecx, %eax
   shrl        MACRO_LITERAL(4), %eax
   andl        MACRO_LITERAL(0xf), %eax
   GET_VREG_OBJECT %edx, %rax
   movl        %edx, (rNEW_FP, %r10, 4)
   GET_VREG    %edx, %rax
   movl        %edx, (%r11, %r10, 4)
   subq        MACRO_LITERAL(1), %r10
1:
   .if \is_string_init
   // Ignore the first argument
   .elseif \is_static
   movl        %ecx, %eax
   andq        MACRO_LITERAL(0x000f), %rax
   GET_VREG_OBJECT %edx, %rax
   movl        %edx, (rNEW_FP, %r10, 4)
   GET_VREG    %edx, %rax
   movl        %edx, (%r11, %r10, 4)
   .else
   movl        %esi, (rNEW_FP, %r10, 4)
   movl        %esi, (%r11, %r10, 4)
   .endif

6:
   // Start executing the method.
   movq rNEW_FP, rFP
   movq rNEW_REFS, rREFS
   CFI_DEF_CFA_BREG_PLUS_UCONST CFI_REFS, -8, ((6 + 4 + 1) * 8)
   START_EXECUTING_INSTRUCTIONS
.endm

// Setup arguments based on a range nterp to nterp call, and start executing
// the method.
.macro SETUP_RANGE_ARGUMENTS_AND_EXECUTE is_static=0, is_string_init=0
   // edi is number of arguments
   // ecx is first register
   movq MACRO_LITERAL(-4), %r10
   .if \is_string_init
   // Ignore the first argument
   subl $1, %edi
   addl $1, %ecx
   .elseif !\is_static
   subl $1, %edi
   addl $1, %ecx
   .endif

   testl %edi, %edi
   je 2f
   leaq  (rREFS, %rcx, 4), %rax  # pointer to first argument in reference array
   leaq  (%rax, %rdi, 4), %rax   # pointer to last argument in reference array
   leaq  (rFP, %rcx, 4), %rcx    # pointer to first argument in register array
   leaq  (%rcx, %rdi, 4), %rdi   # pointer to last argument in register array
   // TODO: Same comment for copying arguments as in SETUP_NON_RANGE_ARGUMENTS_AND_EXECUTE.
1:
   movl  -4(%rax), %edx
   movl  %edx, (rNEW_FP, %r10, 1)
   movl  -4(%rdi), %edx
   movl  %edx, (%r11, %r10, 1)
   subq  MACRO_LITERAL(4), %r10
   subq  MACRO_LITERAL(4), %rax
   subq  MACRO_LITERAL(4), %rdi
   cmpq  %rcx, %rdi
   jne 1b

2:
   .if \is_string_init
   // Ignore first argument
   .elseif !\is_static
   movl        %esi, (rNEW_FP, %r10, 1)
   movl        %esi, (%r11, %r10, 1)
   .endif
   movq rNEW_FP, rFP
   movq rNEW_REFS, rREFS
   CFI_DEF_CFA_BREG_PLUS_UCONST CFI_REFS, -8, ((6 + 4 + 1) * 8)
   START_EXECUTING_INSTRUCTIONS
.endm

.macro GET_SHORTY dest, is_interface, is_polymorphic, is_custom
   push %rdi
   push %rsi
   .if \is_polymorphic
   movq 16(%rsp), %rdi
   movq rPC, %rsi
   call SYMBOL(NterpGetShortyFromInvokePolymorphic)
   .elseif \is_custom
   movq 16(%rsp), %rdi
   movq rPC, %rsi
   call SYMBOL(NterpGetShortyFromInvokeCustom)
   .elseif \is_interface
   movq 16(%rsp), %rdi
   movzwl 2(rPC), %esi
   call SYMBOL(NterpGetShortyFromMethodId)
   .else
   call SYMBOL(NterpGetShorty)
   .endif
   pop %rsi
   pop %rdi
   movq %rax, \dest
.endm

.macro GET_SHORTY_SLOW_PATH dest, is_interface
   // Save all registers that can hold arguments in the fast path.
   push %rdi
   push %rsi
   push %rdx
   subq MACRO_LITERAL(8), %rsp
   mov %xmm0, (%rsp)
   .if \is_interface
   movq 32(%rsp), %rdi
   movzwl 2(rPC), %esi
   call SYMBOL(NterpGetShortyFromMethodId)
   .else
   call SYMBOL(NterpGetShorty)
   .endif
   mov (%rsp), %xmm0
   addq MACRO_LITERAL(8), %rsp
   pop %rdx
   pop %rsi
   pop %rdi
   movq %rax, \dest
.endm

// Uses r9 as temporary.
.macro DO_ENTRY_POINT_CHECK call_compiled_code
   // On entry, the method is %rdi, the instance is %rsi
   leaq ExecuteNterpImpl(%rip), %r9
   cmpq %r9, ART_METHOD_QUICK_CODE_OFFSET_64(%rdi)
   jne  VAR(call_compiled_code)

   movq ART_METHOD_DATA_OFFSET_64(%rdi), %rax
.endm

// Uses r9 and r10 as temporary
.macro UPDATE_REGISTERS_FOR_STRING_INIT old_value, new_value
   movq rREFS, %r9
   movq rFP, %r10
1:
   cmpl (%r9), \old_value
   jne 2f
   movl \new_value, (%r9)
   movl \new_value, (%r10)
2:
   addq $4, %r9
   addq $4, %r10
   cmpq %r9, rFP
   jne 1b
.endm

.macro COMMON_INVOKE_NON_RANGE is_static=0, is_interface=0, suffix="", is_string_init=0, is_polymorphic=0, is_custom=0
   .if \is_polymorphic
   // We always go to compiled code for polymorphic calls.
   .elseif \is_custom
   // We always go to compiled code for custom calls.
   .else
     DO_ENTRY_POINT_CHECK .Lcall_compiled_code_\suffix
     .if \is_string_init
     call nterp_to_nterp_string_init_non_range
     .elseif \is_static
     call nterp_to_nterp_static_non_range
     .else
     call nterp_to_nterp_instance_non_range
     .endif
     jmp .Ldone_return_\suffix
   .endif

.Lcall_compiled_code_\suffix:
   .if \is_polymorphic
   // No fast path for polymorphic calls.
   .elseif \is_custom
   // No fast path for custom calls.
   .elseif \is_string_init
   // No fast path for string.init.
   .else
     testl $ART_METHOD_NTERP_INVOKE_FAST_PATH_FLAG, ART_METHOD_ACCESS_FLAGS_OFFSET(%rdi)
     je .Lfast_path_with_few_args_\suffix
     movzbl 1(rPC), %r9d
     movl %r9d, %ebp
     shrl MACRO_LITERAL(4), %ebp # Number of arguments
     .if \is_static
     jz .Linvoke_fast_path_\suffix  # shl sets the Z flag
     .else
     cmpl MACRO_LITERAL(1), %ebp
     je .Linvoke_fast_path_\suffix
     .endif
     movzwl 4(rPC), %r11d
     cmpl MACRO_LITERAL(2), %ebp
     .if \is_static
     jl .Lone_arg_fast_path_\suffix
     .endif
     je .Ltwo_args_fast_path_\suffix
     cmpl MACRO_LITERAL(4), %ebp
     jl .Lthree_args_fast_path_\suffix
     je .Lfour_args_fast_path_\suffix

     andl        MACRO_LITERAL(0xf), %r9d
     GET_VREG    %r9d, %r9
.Lfour_args_fast_path_\suffix:
     movl        %r11d, %r8d
     shrl        MACRO_LITERAL(12), %r8d
     GET_VREG    %r8d, %r8
.Lthree_args_fast_path_\suffix:
     movl        %r11d, %ecx
     shrl        MACRO_LITERAL(8), %ecx
     andl        MACRO_LITERAL(0xf), %ecx
     GET_VREG    %ecx, %rcx
.Ltwo_args_fast_path_\suffix:
     movl        %r11d, %edx
     shrl        MACRO_LITERAL(4), %edx
     andl        MACRO_LITERAL(0xf), %edx
     GET_VREG    %edx, %rdx
.Lone_arg_fast_path_\suffix:
     .if \is_static
     andl        MACRO_LITERAL(0xf), %r11d
     GET_VREG    %esi, %r11
     .else
     // First argument already in %esi.
     .endif
.Linvoke_fast_path_\suffix:
     call *ART_METHOD_QUICK_CODE_OFFSET_64(%rdi) // Call the method.
     ADVANCE_PC_FETCH_AND_GOTO_NEXT 3

.Lfast_path_with_few_args_\suffix:
     // Fast path when we have zero or one argument (modulo 'this'). If there
     // is one argument, we can put it in both floating point and core register.
     movzbl 1(rPC), %r9d
     shrl MACRO_LITERAL(4), %r9d # Number of arguments
     .if \is_static
     cmpl MACRO_LITERAL(1), %r9d
     jl .Linvoke_with_few_args_\suffix
     jne .Lget_shorty_\suffix
     movzwl 4(rPC), %r9d
     andl MACRO_LITERAL(0xf), %r9d  // dex register of first argument
     GET_VREG %esi, %r9
     movd %esi, %xmm0
     .else
     cmpl MACRO_LITERAL(2), %r9d
     jl .Linvoke_with_few_args_\suffix
     jne .Lget_shorty_\suffix
     movzwl 4(rPC), %r9d
     shrl MACRO_LITERAL(4), %r9d
     andl MACRO_LITERAL(0xf), %r9d  // dex register of second argument
     GET_VREG %edx, %r9
     movd %edx, %xmm0
     .endif
.Linvoke_with_few_args_\suffix:
     // Check if the next instruction is move-result or move-result-wide.
     // If it is, we fetch the shorty and jump to the regular invocation.
     movzwq  6(rPC), %r9
     andl MACRO_LITERAL(0xfe), %r9d
     cmpl MACRO_LITERAL(0x0a), %r9d
     je .Lget_shorty_and_invoke_\suffix
     call *ART_METHOD_QUICK_CODE_OFFSET_64(%rdi) // Call the method.
     ADVANCE_PC_FETCH_AND_GOTO_NEXT 3
.Lget_shorty_and_invoke_\suffix:
     .if \is_interface
     // Save interface method, used for conflict resolution, in a callee-save register.
     movq %rax, %xmm12
     .endif
     GET_SHORTY_SLOW_PATH rINSTq, \is_interface
     jmp .Lgpr_setup_finished_\suffix
   .endif

.Lget_shorty_\suffix:
   .if \is_interface
   // Save interface method, used for conflict resolution, in a callee-save register.
   movq %rax, %xmm12
   .endif
   GET_SHORTY rINSTq, \is_interface, \is_polymorphic, \is_custom
   // From this point:
   // - rISNTq contains shorty (in callee-save to switch over return value after call).
   // - rdi contains method
   // - rsi contains 'this' pointer for instance method.
   leaq 1(rINSTq), %r9  // shorty + 1  ; ie skip return arg character
   movzwl 4(rPC), %r11d // arguments
   .if \is_string_init
   shrq MACRO_LITERAL(4), %r11
   movq $1, %r10       // ignore first argument
   .elseif \is_static
   movq $0, %r10       // arg_index
   .else
   shrq MACRO_LITERAL(4), %r11
   movq $1, %r10       // arg_index
   .endif
   LOOP_OVER_SHORTY_LOADING_XMMS xmm0, r11, r9, r10, .Lxmm_setup_finished_\suffix
   LOOP_OVER_SHORTY_LOADING_XMMS xmm1, r11, r9, r10, .Lxmm_setup_finished_\suffix
   LOOP_OVER_SHORTY_LOADING_XMMS xmm2, r11, r9, r10, .Lxmm_setup_finished_\suffix
   LOOP_OVER_SHORTY_LOADING_XMMS xmm3, r11, r9, r10, .Lxmm_setup_finished_\suffix
   LOOP_OVER_SHORTY_LOADING_XMMS xmm4, r11, r9, r10, .Lxmm_setup_finished_\suffix
.Lxmm_setup_finished_\suffix:
   leaq 1(rINSTq), %r9  // shorty + 1  ; ie skip return arg character
   movzwl 4(rPC), %r11d // arguments
   .if \is_string_init
   movq $1, %r10       // ignore first argument
   shrq MACRO_LITERAL(4), %r11
   LOOP_OVER_SHORTY_LOADING_GPRS rsi, esi, r11, r9, r10, .Lgpr_setup_finished_\suffix
   .elseif \is_static
   movq $0, %r10       // arg_index
   LOOP_OVER_SHORTY_LOADING_GPRS rsi, esi, r11, r9, r10, .Lgpr_setup_finished_\suffix
   .else
   shrq MACRO_LITERAL(4), %r11
   movq $1, %r10       // arg_index
   .endif
   LOOP_OVER_SHORTY_LOADING_GPRS rdx, edx, r11, r9, r10, .Lgpr_setup_finished_\suffix
   LOOP_OVER_SHORTY_LOADING_GPRS rcx, ecx, r11, r9, r10, .Lgpr_setup_finished_\suffix
   LOOP_OVER_SHORTY_LOADING_GPRS r8, r8d, r11, r9, r10, .Lgpr_setup_finished_\suffix
   LOOP_OVER_SHORTY_LOADING_GPRS r9, r9d, r11, r9, r10, .Lgpr_setup_finished_\suffix
.Lgpr_setup_finished_\suffix:
   .if \is_polymorphic
   call SYMBOL(art_quick_invoke_polymorphic)
   .elseif \is_custom
   call SYMBOL(art_quick_invoke_custom)
   .else
      .if \is_interface
      movq %xmm12, %rax
      .endif
      call *ART_METHOD_QUICK_CODE_OFFSET_64(%rdi) // Call the method.
   .endif
   cmpb LITERAL(68), (rINSTq)       // Test if result type char == 'D'.
   je .Lreturn_double_\suffix
   cmpb LITERAL(70), (rINSTq)       // Test if result type char == 'F'.
   jne .Ldone_return_\suffix
.Lreturn_float_\suffix:
   movd %xmm0, %eax
   jmp .Ldone_return_\suffix
.Lreturn_double_\suffix:
   movq %xmm0, %rax
.Ldone_return_\suffix:
   /* resume execution of caller */
   .if \is_string_init
   movzwl 4(rPC), %r11d // arguments
   andq $0xf, %r11
   GET_VREG %esi, %r11
   UPDATE_REGISTERS_FOR_STRING_INIT %esi, %eax
   .endif

   .if \is_polymorphic
   ADVANCE_PC_FETCH_AND_GOTO_NEXT 4
   .else
   ADVANCE_PC_FETCH_AND_GOTO_NEXT 3
   .endif
.endm

.macro COMMON_INVOKE_RANGE is_static=0, is_interface=0, suffix="", is_string_init=0, is_polymorphic=0, is_custom=0
   .if \is_polymorphic
   // We always go to compiled code for polymorphic calls.
   .elseif \is_custom
   // We always go to compiled code for custom calls.
   .else
     DO_ENTRY_POINT_CHECK .Lcall_compiled_code_range_\suffix
     .if \is_string_init
     call nterp_to_nterp_string_init_range
     .elseif \is_static
     call nterp_to_nterp_static_range
     .else
     call nterp_to_nterp_instance_range
     .endif
     jmp .Ldone_return_range_\suffix
   .endif

.Lcall_compiled_code_range_\suffix:
   .if \is_polymorphic
   // No fast path for polymorphic calls.
   .elseif \is_custom
   // No fast path for custom calls.
   .elseif \is_string_init
   // No fast path for string.init.
   .else
     testl $ART_METHOD_NTERP_INVOKE_FAST_PATH_FLAG, ART_METHOD_ACCESS_FLAGS_OFFSET(%rdi)
     je .Lfast_path_with_few_args_range_\suffix
     movzbl 1(rPC), %r9d  // number of arguments
     .if \is_static
     testl %r9d, %r9d
     je .Linvoke_fast_path_range_\suffix
     .else
     cmpl MACRO_LITERAL(1), %r9d
     je .Linvoke_fast_path_range_\suffix
     .endif
     movzwl 4(rPC), %r11d  // dex register of first argument
     leaq (rFP, %r11, 4), %r11  // location of first dex register value
     cmpl MACRO_LITERAL(2), %r9d
     .if \is_static
     jl .Lone_arg_fast_path_range_\suffix
     .endif
     je .Ltwo_args_fast_path_range_\suffix
     cmp MACRO_LITERAL(4), %r9d
     jl .Lthree_args_fast_path_range_\suffix
     je .Lfour_args_fast_path_range_\suffix
     cmp MACRO_LITERAL(5), %r9d
     je .Lfive_args_fast_path_range_\suffix

.Lloop_over_fast_path_range_\suffix:
     subl MACRO_LITERAL(1), %r9d
     movl (%r11, %r9, 4), %r8d
     movl %r8d, 8(%rsp, %r9, 4)  // Add 8 for the ArtMethod
     cmpl MACRO_LITERAL(5), %r9d
     jne .Lloop_over_fast_path_range_\suffix

.Lfive_args_fast_path_range_\suffix:
     movl 16(%r11), %r9d
.Lfour_args_fast_path_range_\suffix:
     movl 12(%r11), %r8d
.Lthree_args_fast_path_range_\suffix:
     movl 8(%r11), %ecx
.Ltwo_args_fast_path_range_\suffix:
     movl 4(%r11), %edx
.Lone_arg_fast_path_range_\suffix:
     .if \is_static
     movl 0(%r11), %esi
     .else
     // First argument already in %esi.
     .endif
.Linvoke_fast_path_range_\suffix:
     call *ART_METHOD_QUICK_CODE_OFFSET_64(%rdi) // Call the method.
     ADVANCE_PC_FETCH_AND_GOTO_NEXT 3

.Lfast_path_with_few_args_range_\suffix:
     // Fast path when we have zero or one argument (modulo 'this'). If there
     // is one argument, we can put it in both floating point and core register.
     movzbl 1(rPC), %r9d # Number of arguments
     .if \is_static
     cmpl MACRO_LITERAL(1), %r9d
     jl .Linvoke_with_few_args_range_\suffix
     jne .Lget_shorty_range_\suffix
     movzwl 4(rPC), %r9d  // Dex register of first argument
     GET_VREG %esi, %r9
     movd %esi, %xmm0
     .else
     cmpl MACRO_LITERAL(2), %r9d
     jl .Linvoke_with_few_args_range_\suffix
     jne .Lget_shorty_range_\suffix
     movzwl 4(rPC), %r9d
     addl MACRO_LITERAL(1), %r9d  // dex register of second argument
     GET_VREG %edx, %r9
     movd %edx, %xmm0
     .endif
.Linvoke_with_few_args_range_\suffix:
     // Check if the next instruction is move-result or move-result-wide.
     // If it is, we fetch the shorty and jump to the regular invocation.
     movzwq  6(rPC), %r9
     and MACRO_LITERAL(0xfe), %r9d
     cmpl MACRO_LITERAL(0x0a), %r9d
     je .Lget_shorty_and_invoke_range_\suffix
     call *ART_METHOD_QUICK_CODE_OFFSET_64(%rdi) // Call the method.
     ADVANCE_PC_FETCH_AND_GOTO_NEXT 3
.Lget_shorty_and_invoke_range_\suffix:
     .if \is_interface
     // Save interface method, used for conflict resolution, in a callee-save register.
     movq %rax, %xmm12
     .endif
     GET_SHORTY_SLOW_PATH rINSTq, \is_interface
     jmp .Lgpr_setup_finished_range_\suffix
   .endif

.Lget_shorty_range_\suffix:
   .if \is_interface
   // Save interface method, used for conflict resolution, in a callee-saved register.
   movq %rax, %xmm12
   .endif
   GET_SHORTY rINSTq, \is_interface, \is_polymorphic, \is_custom
   // From this point:
   // - rINSTq contains shorty (in callee-save to switch over return value after call).
   // - rdi contains method
   // - rsi contains 'this' pointer for instance method.
   leaq 1(rINSTq), %r9  // shorty + 1  ; ie skip return arg character
   movzwl 4(rPC), %r10d // arg start index
   .if \is_string_init
   addq $1, %r10       // arg start index
   movq $1, %rbp       // index in stack
   .elseif \is_static
   movq $0, %rbp       // index in stack
   .else
   addq $1, %r10       // arg start index
   movq $1, %rbp       // index in stack
   .endif
   LOOP_RANGE_OVER_SHORTY_LOADING_XMMS xmm0, r9, r10, rbp, .Lxmm_setup_finished_range_\suffix
   LOOP_RANGE_OVER_SHORTY_LOADING_XMMS xmm1, r9, r10, rbp, .Lxmm_setup_finished_range_\suffix
   LOOP_RANGE_OVER_SHORTY_LOADING_XMMS xmm2, r9, r10, rbp, .Lxmm_setup_finished_range_\suffix
   LOOP_RANGE_OVER_SHORTY_LOADING_XMMS xmm3, r9, r10, rbp, .Lxmm_setup_finished_range_\suffix
   LOOP_RANGE_OVER_SHORTY_LOADING_XMMS xmm4, r9, r10, rbp, .Lxmm_setup_finished_range_\suffix
   LOOP_RANGE_OVER_SHORTY_LOADING_XMMS xmm5, r9, r10, rbp, .Lxmm_setup_finished_range_\suffix
   LOOP_RANGE_OVER_SHORTY_LOADING_XMMS xmm6, r9, r10, rbp, .Lxmm_setup_finished_range_\suffix
   LOOP_RANGE_OVER_SHORTY_LOADING_XMMS xmm7, r9, r10, rbp, .Lxmm_setup_finished_range_\suffix
   LOOP_RANGE_OVER_FPs r9, r10, rbp, .Lxmm_setup_finished_range_\suffix
.Lxmm_setup_finished_range_\suffix:
   leaq 1(%rbx), %r11  // shorty + 1  ; ie skip return arg character
   movzwl 4(rPC), %r10d // arg start index
   .if \is_string_init
   addq $1, %r10       // arg start index
   movq $1, %rbp       // index in stack
   LOOP_RANGE_OVER_SHORTY_LOADING_GPRS rsi, esi, r11, r10, rbp, .Lgpr_setup_finished_range_\suffix
   .elseif \is_static
   movq $0, %rbp // index in stack
   LOOP_RANGE_OVER_SHORTY_LOADING_GPRS rsi, esi, r11, r10, rbp, .Lgpr_setup_finished_range_\suffix
   .else
   addq $1, %r10       // arg start index
   movq $1, %rbp // index in stack
   .endif
   LOOP_RANGE_OVER_SHORTY_LOADING_GPRS rdx, edx, r11, r10, rbp, .Lgpr_setup_finished_range_\suffix
   LOOP_RANGE_OVER_SHORTY_LOADING_GPRS rcx, ecx, r11, r10, rbp, .Lgpr_setup_finished_range_\suffix
   LOOP_RANGE_OVER_SHORTY_LOADING_GPRS r8, r8d, r11, r10, rbp, .Lgpr_setup_finished_range_\suffix
   LOOP_RANGE_OVER_SHORTY_LOADING_GPRS r9, r9d, r11, r10, rbp, .Lgpr_setup_finished_range_\suffix
   LOOP_RANGE_OVER_INTs r11, r10, rbp, .Lgpr_setup_finished_range_\suffix

.Lgpr_setup_finished_range_\suffix:
   .if \is_polymorphic
   call SYMBOL(art_quick_invoke_polymorphic)
   .elseif \is_custom
   call SYMBOL(art_quick_invoke_custom)
   .else
     .if \is_interface
     // Set the hidden argument for conflict resolution.
     movq %xmm12, %rax
     .endif
     call *ART_METHOD_QUICK_CODE_OFFSET_64(%rdi) // Call the method.
   .endif
   cmpb LITERAL(68), (%rbx)       // Test if result type char == 'D'.
   je .Lreturn_range_double_\suffix
   cmpb LITERAL(70), (%rbx)       // Test if result type char == 'F'.
   je .Lreturn_range_float_\suffix
   /* resume execution of caller */
.Ldone_return_range_\suffix:
   .if \is_string_init
   movzwl 4(rPC), %r11d // arguments
   GET_VREG %esi, %r11
   UPDATE_REGISTERS_FOR_STRING_INIT %esi, %eax
   .endif

   .if \is_polymorphic
   ADVANCE_PC_FETCH_AND_GOTO_NEXT 4
   .else
   ADVANCE_PC_FETCH_AND_GOTO_NEXT 3
   .endif
.Lreturn_range_double_\suffix:
    movq %xmm0, %rax
    jmp .Ldone_return_range_\suffix
.Lreturn_range_float_\suffix:
    movd %xmm0, %eax
    jmp .Ldone_return_range_\suffix
.endm

// Fetch some information from the thread cache.
// Uses rax, rdx, rcx as temporaries.
.macro FETCH_FROM_THREAD_CACHE dest_reg, slow_path
   movq rSELF:THREAD_SELF_OFFSET, %rax
   movq rPC, %rdx
   salq MACRO_LITERAL(THREAD_INTERPRETER_CACHE_SIZE_SHIFT), %rdx
   andq MACRO_LITERAL(THREAD_INTERPRETER_CACHE_SIZE_MASK), %rdx
   cmpq THREAD_INTERPRETER_CACHE_OFFSET(%rax, %rdx, 1), rPC
   jne \slow_path
   movq __SIZEOF_POINTER__+THREAD_INTERPRETER_CACHE_OFFSET(%rax, %rdx, 1), \dest_reg
.endm

// Helper for static field get.
.macro OP_SGET load="movl", wide="0"
   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE %rax, 2f
1:
   movl ART_FIELD_OFFSET_OFFSET(%rax), %edx
   movl ART_FIELD_DECLARING_CLASS_OFFSET(%rax), %eax
   cmpq $0, rSELF:THREAD_READ_BARRIER_MARK_REG00_OFFSET
   jne 3f
4:
   .if \wide
   movq (%eax,%edx,1), %rax
   SET_WIDE_VREG %rax, rINSTq              # fp[A] <- value
   .else
   \load (%eax, %edx, 1), %eax
   SET_VREG %eax, rINSTq            # fp[A] <- value
   .endif
   ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
2:
   EXPORT_PC
   movq rSELF:THREAD_SELF_OFFSET, %rdi
   movq 0(%rsp), %rsi
   movq rPC, %rdx
   movq $0, %rcx
   call nterp_get_static_field
   // Clear the marker that we put for volatile fields. The x86 memory
   // model doesn't require a barrier.
   andq $-2, %rax
   jmp 1b
3:
   call art_quick_read_barrier_mark_reg00
   jmp 4b
.endm

// Helper for static field put.
.macro OP_SPUT rINST_reg="rINST", store="movl", wide="0":
   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE %rax, 2f
1:
   movl ART_FIELD_OFFSET_OFFSET(%rax), %edx
   movl ART_FIELD_DECLARING_CLASS_OFFSET(%rax), %eax
   cmpq $0, rSELF:THREAD_READ_BARRIER_MARK_REG00_OFFSET
   jne 3f
4:
   .if \wide
   GET_WIDE_VREG rINSTq, rINSTq           # rINST <- v[A]
   .else
   GET_VREG rINST, rINSTq                  # rINST <- v[A]
   .endif
   \store    \rINST_reg, (%rax,%rdx,1)
   ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
2:
   EXPORT_PC
   movq rSELF:THREAD_SELF_OFFSET, %rdi
   movq 0(%rsp), %rsi
   movq rPC, %rdx
   movq $0, %rcx
   call nterp_get_static_field
   testq MACRO_LITERAL(1), %rax
   je 1b
   // Clear the marker that we put for volatile fields. The x86 memory
   // model doesn't require a barrier.
   CLEAR_VOLATILE_MARKER %rax
   movl ART_FIELD_OFFSET_OFFSET(%rax), %edx
   movl ART_FIELD_DECLARING_CLASS_OFFSET(%rax), %eax
   cmpq $0, rSELF:THREAD_READ_BARRIER_MARK_REG00_OFFSET
   jne 6f
5:
   .if \wide
   GET_WIDE_VREG rINSTq, rINSTq           # rINST <- v[A]
   .else
   GET_VREG rINST, rINSTq                  # rINST <- v[A]
   .endif
   \store    \rINST_reg, (%rax,%rdx,1)
   lock addl $0, (%rsp)
   ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
3:
   call art_quick_read_barrier_mark_reg00
   jmp 4b
6:
   call art_quick_read_barrier_mark_reg00
   jmp 5b
.endm

.macro OP_IPUT_INTERNAL rINST_reg="rINST", store="movl", wide="0":
   movzbq  rINSTbl, %rcx                   # rcx <- BA
   sarl    $4, %ecx                       # ecx <- B
   GET_VREG %ecx, %rcx                     # vB (object we're operating on)
   testl   %ecx, %ecx                      # is object null?
   je      common_errNullObject
   andb    $0xf, rINSTbl                  # rINST <- A
   .if \wide
   GET_WIDE_VREG rINSTq, rINSTq              # rax<- fp[A]/fp[A+1]
   .else
   GET_VREG rINST, rINSTq                  # rINST <- v[A]
   .endif
   \store \rINST_reg, (%rcx,%rax,1)
.endm

// Helper for instance field put.
.macro OP_IPUT rINST_reg="rINST", store="movl", wide="0":
   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE %rax, 2f
1:
   OP_IPUT_INTERNAL \rINST_reg, \store, \wide
   ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
2:
   EXPORT_PC
   movq rSELF:THREAD_SELF_OFFSET, %rdi
   movq 0(%rsp), %rsi
   movq rPC, %rdx
   movq $0, %rcx
   call nterp_get_instance_field_offset
   testl %eax, %eax
   jns 1b
   negl %eax
   OP_IPUT_INTERNAL \rINST_reg, \store, \wide
   lock addl $0, (%rsp)
   ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
.endm

// Helper for instance field get.
.macro OP_IGET load="movl", wide="0"
   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE %rax, 2f
1:
   movl    rINST, %ecx                     # rcx <- BA
   sarl    $4, %ecx                       # ecx <- B
   GET_VREG %ecx, %rcx                     # vB (object we're operating on)
   testl   %ecx, %ecx                      # is object null?
   je      common_errNullObject
   andb    $0xf,rINSTbl                   # rINST <- A
   .if \wide
   movq (%rcx,%rax,1), %rax
   SET_WIDE_VREG %rax, rINSTq              # fp[A] <- value
   .else
   \load (%rcx,%rax,1), %eax
   SET_VREG %eax, rINSTq                   # fp[A] <- value
   .endif
   ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
2:
   EXPORT_PC
   movq rSELF:THREAD_SELF_OFFSET, %rdi
   movq 0(%rsp), %rsi
   movq rPC, %rdx
   movq $0, %rcx
   call nterp_get_instance_field_offset
   testl %eax, %eax
   jns 1b
   negl %eax
   jmp 1b
.endm

.macro SETUP_REFERENCE_PARAMETER_IN_GPR gpr32, regs, refs, ins, arg_offset, finished
    movl REG_VAR(gpr32), (REG_VAR(regs), REG_VAR(arg_offset))
    movl REG_VAR(gpr32), (REG_VAR(refs), REG_VAR(arg_offset))
    addq MACRO_LITERAL(4), REG_VAR(arg_offset)
    subl MACRO_LITERAL(1), REG_VAR(ins)
    je \finished
.endm

// Uses eax as temporary
.macro SETUP_REFERENCE_PARAMETERS_IN_STACK regs, refs, ins, stack_ptr, arg_offset
1:
    movl OFFSET_TO_FIRST_ARGUMENT_IN_STACK(REG_VAR(stack_ptr), REG_VAR(arg_offset)), %eax
    movl %eax, (REG_VAR(regs), REG_VAR(arg_offset))
    movl %eax, (REG_VAR(refs), REG_VAR(arg_offset))
    addq MACRO_LITERAL(4), REG_VAR(arg_offset)
    subl MACRO_LITERAL(1), REG_VAR(ins)
    jne 1b
.endm

/*
 * ArtMethod entry point.
 *
 * On entry:
 *  rdi   ArtMethod* callee
 *  rest  method parameters
 */

OAT_ENTRY ExecuteNterpImpl, EndExecuteNterpImpl
    .cfi_startproc
    .cfi_def_cfa rsp, 8
    testq %rax, -STACK_OVERFLOW_RESERVED_BYTES(%rsp)
    /* Spill callee save regs */
    SPILL_ALL_CALLEE_SAVES

    movq ART_METHOD_DATA_OFFSET_64(%rdi), rPC

    // Setup the stack for executing the method.
    SETUP_STACK_FRAME rPC, rREFS, rREFS32, rFP, CFI_REFS, load_ins=1

    // Setup the parameters
    testl %r14d, %r14d
    je .Lxmm_setup_finished

    subq %r14, %rbx
    salq $2, %rbx // rbx is now the offset for inputs into the registers array.

    testl $ART_METHOD_NTERP_ENTRY_POINT_FAST_PATH_FLAG, ART_METHOD_ACCESS_FLAGS_OFFSET(%rdi)
    je .Lsetup_slow_path
    leaq (rFP, %rbx, 1), %rdi
    leaq (rREFS, %rbx, 1), %rbx
    movq $0, %r10

    SETUP_REFERENCE_PARAMETER_IN_GPR esi, rdi, rbx, r14d, r10, .Lxmm_setup_finished
    SETUP_REFERENCE_PARAMETER_IN_GPR edx, rdi, rbx, r14d, r10, .Lxmm_setup_finished
    SETUP_REFERENCE_PARAMETER_IN_GPR ecx, rdi, rbx, r14d, r10, .Lxmm_setup_finished
    SETUP_REFERENCE_PARAMETER_IN_GPR r8d, rdi, rbx, r14d, r10, .Lxmm_setup_finished
    SETUP_REFERENCE_PARAMETER_IN_GPR r9d, rdi, rbx, r14d, r10, .Lxmm_setup_finished
    SETUP_REFERENCE_PARAMETERS_IN_STACK rdi, rbx, r14d, r11, r10
    jmp .Lxmm_setup_finished

.Lsetup_slow_path:
    // If the method is not static and there is one argument ('this'), we don't need to fetch the
    // shorty.
    testl $ART_METHOD_IS_STATIC_FLAG, ART_METHOD_ACCESS_FLAGS_OFFSET(%rdi)
    jne .Lsetup_with_shorty

    movl %esi, (rFP, %rbx)
    movl %esi, (rREFS, %rbx)

    cmpl $1, %r14d
    je .Lxmm_setup_finished

.Lsetup_with_shorty:
    // TODO: Get shorty in a better way and remove below
    push %rdi
    push %rsi
    push %rdx
    push %rcx
    push %r8
    push %r9

    // Save xmm registers + alignment.
    subq MACRO_LITERAL(8 * 8 + 8), %rsp
    movq %xmm0, 0(%rsp)
    movq %xmm1, 8(%rsp)
    movq %xmm2, 16(%rsp)
    movq %xmm3, 24(%rsp)
    movq %xmm4, 32(%rsp)
    movq %xmm5, 40(%rsp)
    movq %xmm6, 48(%rsp)
    movq %xmm7, 56(%rsp)

    call SYMBOL(NterpGetShorty)
    // Save shorty in callee-save rbp.
    movq %rax, %rbp

    // Restore xmm registers + alignment.
    movq 0(%rsp), %xmm0
    movq 8(%rsp), %xmm1
    movq 16(%rsp), %xmm2
    movq 24(%rsp), %xmm3
    movq 32(%rsp), %xmm4
    movq 40(%rsp), %xmm5
    movq 48(%rsp), %xmm6
    movq 56(%rsp), %xmm7
    addq MACRO_LITERAL(8 * 8 + 8), %rsp

    pop %r9
    pop %r8
    pop %rcx
    pop %rdx
    pop %rsi
    pop %rdi
    // Reload the old stack pointer, which used to be stored in %r11, which is not callee-saved.
    movq -8(rREFS), %r11
    // TODO: Get shorty in a better way and remove above

    movq $0, %r14
    testl $ART_METHOD_IS_STATIC_FLAG, ART_METHOD_ACCESS_FLAGS_OFFSET(%rdi)

    // Available: rdi, r10
    // Note the leaq below don't change the flags.
    leaq 1(%rbp), %r10  // shorty + 1  ; ie skip return arg character
    leaq (rFP, %rbx, 1), %rdi
    leaq (rREFS, %rbx, 1), %rbx
    jne .Lhandle_static_method
    addq $4, %rdi
    addq $4, %rbx
    addq $4, %r11
    jmp .Lcontinue_setup_gprs
.Lhandle_static_method:
    LOOP_OVER_SHORTY_STORING_GPRS rsi, esi, r10, r14, rdi, rbx, .Lgpr_setup_finished
.Lcontinue_setup_gprs:
    LOOP_OVER_SHORTY_STORING_GPRS rdx, edx, r10, r14, rdi, rbx, .Lgpr_setup_finished
    LOOP_OVER_SHORTY_STORING_GPRS rcx, ecx, r10, r14, rdi, rbx, .Lgpr_setup_finished
    LOOP_OVER_SHORTY_STORING_GPRS r8, r8d, r10, r14, rdi, rbx, .Lgpr_setup_finished
    LOOP_OVER_SHORTY_STORING_GPRS r9, r9d, r10, r14, rdi, rbx, .Lgpr_setup_finished
    LOOP_OVER_INTs r10, r14, rdi, rbx, r11, .Lgpr_setup_finished
.Lgpr_setup_finished:
    leaq 1(%rbp), %r10  // shorty + 1  ; ie skip return arg character
    movq $0, %r14 // reset counter
    LOOP_OVER_SHORTY_STORING_XMMS xmm0, r10, r14, rdi, .Lxmm_setup_finished
    LOOP_OVER_SHORTY_STORING_XMMS xmm1, r10, r14, rdi, .Lxmm_setup_finished
    LOOP_OVER_SHORTY_STORING_XMMS xmm2, r10, r14, rdi, .Lxmm_setup_finished
    LOOP_OVER_SHORTY_STORING_XMMS xmm3, r10, r14, rdi, .Lxmm_setup_finished
    LOOP_OVER_SHORTY_STORING_XMMS xmm4, r10, r14, rdi, .Lxmm_setup_finished
    LOOP_OVER_SHORTY_STORING_XMMS xmm5, r10, r14, rdi, .Lxmm_setup_finished
    LOOP_OVER_SHORTY_STORING_XMMS xmm6, r10, r14, rdi, .Lxmm_setup_finished
    LOOP_OVER_SHORTY_STORING_XMMS xmm7, r10, r14, rdi, .Lxmm_setup_finished
    LOOP_OVER_FPs r10, r14, rdi, r11, .Lxmm_setup_finished
.Lxmm_setup_finished:
    CFI_DEFINE_DEX_PC_WITH_OFFSET(CFI_TMP, CFI_DEX, 0)

    // Set rIBASE
    leaq artNterpAsmInstructionStart(%rip), rIBASE
    /* start executing the instruction at rPC */
    START_EXECUTING_INSTRUCTIONS
    /* NOTE: no fallthrough */
    // cfi info continues, and covers the whole nterp implementation.
    END ExecuteNterpImpl

    FUNCTION_TYPE(artNterpAsmInstructionStart)
    ASM_HIDDEN SYMBOL(artNterpAsmInstructionStart)
    .global SYMBOL(artNterpAsmInstructionStart)
SYMBOL(artNterpAsmInstructionStart) = .L_op_nop
    .text

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_nop: /* 0x00 */
    ENTRY nterp_op_nop

    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_nop

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_move: /* 0x01 */
    ENTRY nterp_op_move

    /* for move, move-object, long-to-int */
    /* op vA, vB */
    movl    rINST, %eax                     # eax <- BA
    andb    $0xf, %al                      # eax <- A
    shrl    $4, rINST                      # rINST <- B
    GET_VREG %edx, rINSTq
    .if 0
    SET_VREG_OBJECT %edx, %rax              # fp[A] <- fp[B]
    .else
    SET_VREG %edx, %rax                     # fp[A] <- fp[B]
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_move

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_move_from16: /* 0x02 */
    ENTRY nterp_op_move_from16

    /* for: move/from16, move-object/from16 */
    /* op vAA, vBBBB */
    movzwq  2(rPC), %rax                    # eax <- BBBB
    GET_VREG %edx, %rax                     # edx <- fp[BBBB]
    .if 0
    SET_VREG_OBJECT %edx, rINSTq            # fp[A] <- fp[B]
    .else
    SET_VREG %edx, rINSTq                   # fp[A] <- fp[B]
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_move_from16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_move_16: /* 0x03 */
    ENTRY nterp_op_move_16

    /* for: move/16, move-object/16 */
    /* op vAAAA, vBBBB */
    movzwq  4(rPC), %rcx                    # ecx <- BBBB
    movzwq  2(rPC), %rax                    # eax <- AAAA
    GET_VREG %edx, %rcx
    .if 0
    SET_VREG_OBJECT %edx, %rax              # fp[A] <- fp[B]
    .else
    SET_VREG %edx, %rax                     # fp[A] <- fp[B]
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 3

    END nterp_op_move_16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_move_wide: /* 0x04 */
    ENTRY nterp_op_move_wide

    /* move-wide vA, vB */
    /* NOTE: regs can overlap, e.g. "move v6,v7" or "move v7,v6" */
    movl    rINST, %ecx                     # ecx <- BA
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # ecx <- A
    GET_WIDE_VREG %rdx, rINSTq              # rdx <- v[B]
    SET_WIDE_VREG %rdx, %rcx                # v[A] <- rdx
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_move_wide

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_move_wide_from16: /* 0x05 */
    ENTRY nterp_op_move_wide_from16

    /* move-wide/from16 vAA, vBBBB */
    /* NOTE: regs can overlap, e.g. "move v6,v7" or "move v7,v6" */
    movzwl  2(rPC), %ecx                    # ecx <- BBBB
    GET_WIDE_VREG %rdx, %rcx                # rdx <- v[B]
    SET_WIDE_VREG %rdx, rINSTq              # v[A] <- rdx
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_move_wide_from16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_move_wide_16: /* 0x06 */
    ENTRY nterp_op_move_wide_16

    /* move-wide/16 vAAAA, vBBBB */
    /* NOTE: regs can overlap, e.g. "move v6,v7" or "move v7,v6" */
    movzwq  4(rPC), %rcx                    # ecx<- BBBB
    movzwq  2(rPC), %rax                    # eax<- AAAA
    GET_WIDE_VREG %rdx, %rcx                # rdx <- v[B]
    SET_WIDE_VREG %rdx, %rax                # v[A] <- rdx
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 3

    END nterp_op_move_wide_16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_move_object: /* 0x07 */
    ENTRY nterp_op_move_object

    /* for move, move-object, long-to-int */
    /* op vA, vB */
    movl    rINST, %eax                     # eax <- BA
    andb    $0xf, %al                      # eax <- A
    shrl    $4, rINST                      # rINST <- B
    GET_VREG %edx, rINSTq
    .if 1
    SET_VREG_OBJECT %edx, %rax              # fp[A] <- fp[B]
    .else
    SET_VREG %edx, %rax                     # fp[A] <- fp[B]
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_move_object

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_move_object_from16: /* 0x08 */
    ENTRY nterp_op_move_object_from16

    /* for: move/from16, move-object/from16 */
    /* op vAA, vBBBB */
    movzwq  2(rPC), %rax                    # eax <- BBBB
    GET_VREG %edx, %rax                     # edx <- fp[BBBB]
    .if 1
    SET_VREG_OBJECT %edx, rINSTq            # fp[A] <- fp[B]
    .else
    SET_VREG %edx, rINSTq                   # fp[A] <- fp[B]
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_move_object_from16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_move_object_16: /* 0x09 */
    ENTRY nterp_op_move_object_16

    /* for: move/16, move-object/16 */
    /* op vAAAA, vBBBB */
    movzwq  4(rPC), %rcx                    # ecx <- BBBB
    movzwq  2(rPC), %rax                    # eax <- AAAA
    GET_VREG %edx, %rcx
    .if 1
    SET_VREG_OBJECT %edx, %rax              # fp[A] <- fp[B]
    .else
    SET_VREG %edx, %rax                     # fp[A] <- fp[B]
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 3

    END nterp_op_move_object_16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_move_result: /* 0x0a */
    ENTRY nterp_op_move_result

    /* for: move-result, move-result-object */
    /* op vAA */
    .if 0
    SET_VREG_OBJECT %eax, rINSTq            # fp[A] <- fp[B]
    .else
    SET_VREG %eax, rINSTq                   # fp[A] <- fp[B]
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_move_result

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_move_result_wide: /* 0x0b */
    ENTRY nterp_op_move_result_wide

    /* move-result-wide vAA */
    SET_WIDE_VREG %rax, rINSTq                   # v[AA] <- rdx
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_move_result_wide

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_move_result_object: /* 0x0c */
    ENTRY nterp_op_move_result_object

    /* for: move-result, move-result-object */
    /* op vAA */
    .if 1
    SET_VREG_OBJECT %eax, rINSTq            # fp[A] <- fp[B]
    .else
    SET_VREG %eax, rINSTq                   # fp[A] <- fp[B]
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_move_result_object

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_move_exception: /* 0x0d */
    ENTRY nterp_op_move_exception

    /* move-exception vAA */
    movl    rSELF:THREAD_EXCEPTION_OFFSET, %eax
    SET_VREG_OBJECT %eax, rINSTq            # fp[AA] <- exception object
    movl    $0, rSELF:THREAD_EXCEPTION_OFFSET
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_move_exception

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_return_void: /* 0x0e */
    ENTRY nterp_op_return_void

    // Thread fence for constructor is a no-op on x86_64.
    CFI_REMEMBER_STATE
    movq -8(rREFS), %rsp
    CFI_DEF_CFA(rsp, CALLEE_SAVES_SIZE)
    RESTORE_ALL_CALLEE_SAVES
    ret
    CFI_RESTORE_STATE

    END nterp_op_return_void

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_return: /* 0x0f */
    ENTRY nterp_op_return

    GET_VREG %eax, rINSTq                   # eax <- vAA
    .if !0
    // In case we're going back to compiled code, put the
    // result also in a xmm register.
    movd %eax, %xmm0
    .endif
    CFI_REMEMBER_STATE
    movq -8(rREFS), %rsp
    CFI_DEF_CFA(rsp, CALLEE_SAVES_SIZE)
    RESTORE_ALL_CALLEE_SAVES
    ret
    CFI_RESTORE_STATE

    END nterp_op_return

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_return_wide: /* 0x10 */
    ENTRY nterp_op_return_wide

    GET_WIDE_VREG %rax, rINSTq   # eax <- vAA
    // In case we're going back to compiled code, put the
    // result also in a xmm register.
    movq    %rax, %xmm0
    CFI_REMEMBER_STATE
    movq    -8(rREFS), %rsp
    CFI_DEF_CFA(rsp, CALLEE_SAVES_SIZE)
    RESTORE_ALL_CALLEE_SAVES
    ret
    CFI_RESTORE_STATE

    END nterp_op_return_wide

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_return_object: /* 0x11 */
    ENTRY nterp_op_return_object

    GET_VREG %eax, rINSTq                   # eax <- vAA
    .if !1
    // In case we're going back to compiled code, put the
    // result also in a xmm register.
    movd %eax, %xmm0
    .endif
    CFI_REMEMBER_STATE
    movq -8(rREFS), %rsp
    CFI_DEF_CFA(rsp, CALLEE_SAVES_SIZE)
    RESTORE_ALL_CALLEE_SAVES
    ret
    CFI_RESTORE_STATE

    END nterp_op_return_object

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_const_4: /* 0x12 */
    ENTRY nterp_op_const_4

    /* const/4 vA, #+B */
    movsbl  rINSTbl, %eax                   # eax <-ssssssBx
    andl    MACRO_LITERAL(0xf), rINST       # rINST <- A
    sarl    MACRO_LITERAL(4), %eax
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_const_4

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_const_16: /* 0x13 */
    ENTRY nterp_op_const_16

    /* const/16 vAA, #+BBBB */
    movswl  2(rPC), %ecx                    # ecx <- ssssBBBB
    SET_VREG %ecx, rINSTq                   # vAA <- ssssBBBB
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_const_16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_const: /* 0x14 */
    ENTRY nterp_op_const

    /* const vAA, #+BBBBbbbb */
    movl    2(rPC), %eax                    # grab all 32 bits at once
    SET_VREG %eax, rINSTq                   # vAA<- eax
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 3

    END nterp_op_const

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_const_high16: /* 0x15 */
    ENTRY nterp_op_const_high16

    /* const/high16 vAA, #+BBBB0000 */
    movzwl  2(rPC), %eax                    # eax <- 0000BBBB
    sall    MACRO_LITERAL(16), %eax         # eax <- BBBB0000
    SET_VREG %eax, rINSTq                   # vAA <- eax
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_const_high16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_const_wide_16: /* 0x16 */
    ENTRY nterp_op_const_wide_16

    /* const-wide/16 vAA, #+BBBB */
    movswq  2(rPC), %rax                    # rax <- ssssssssssssBBBB
    SET_WIDE_VREG %rax, rINSTq              # store
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_const_wide_16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_const_wide_32: /* 0x17 */
    ENTRY nterp_op_const_wide_32

    /* const-wide/32 vAA, #+BBBBbbbb */
    movslq   2(rPC), %rax                   # eax <- ssssssssBBBBbbbb
    SET_WIDE_VREG %rax, rINSTq              # store
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 3

    END nterp_op_const_wide_32

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_const_wide: /* 0x18 */
    ENTRY nterp_op_const_wide

    /* const-wide vAA, #+HHHHhhhhBBBBbbbb */
    movq    2(rPC), %rax                    # rax <- HHHHhhhhBBBBbbbb
    SET_WIDE_VREG %rax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 5

    END nterp_op_const_wide

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_const_wide_high16: /* 0x19 */
    ENTRY nterp_op_const_wide_high16

    /* const-wide/high16 vAA, #+BBBB000000000000 */
    movzwq  2(rPC), %rax                    # eax <- 000000000000BBBB
    salq    $48, %rax                      # eax <- 00000000BBBB0000
    SET_WIDE_VREG %rax, rINSTq              # v[AA+0] <- eax
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_const_wide_high16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_const_string: /* 0x1a */
    ENTRY nterp_op_const_string

   /* const/string vAA, String@BBBB */
   // Fast-path which gets the object from thread-local cache.
   FETCH_FROM_THREAD_CACHE %rax, 2f
   cmpq MACRO_LITERAL(0), rSELF:THREAD_READ_BARRIER_MARK_REG00_OFFSET
   jne 3f
1:
   SET_VREG_OBJECT %eax, rINSTq            # vAA <- value
   .if 0
   ADVANCE_PC_FETCH_AND_GOTO_NEXT 3
   .else
   ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
   .endif
2:
   EXPORT_PC
   movq rSELF:THREAD_SELF_OFFSET, %rdi
   movq 0(%rsp), %rsi
   movq rPC, %rdx
   call SYMBOL(nterp_load_object)
   jmp 1b
3:
   // 00 is %rax
   call art_quick_read_barrier_mark_reg00
   jmp 1b

    END nterp_op_const_string

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_const_string_jumbo: /* 0x1b */
    ENTRY nterp_op_const_string_jumbo

   /* const/string vAA, String@BBBBBBBB */
   // Fast-path which gets the object from thread-local cache.
   FETCH_FROM_THREAD_CACHE %rax, 2f
   cmpq MACRO_LITERAL(0), rSELF:THREAD_READ_BARRIER_MARK_REG00_OFFSET
   jne 3f
1:
   SET_VREG_OBJECT %eax, rINSTq            # vAA <- value
   .if 1
   ADVANCE_PC_FETCH_AND_GOTO_NEXT 3
   .else
   ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
   .endif
2:
   EXPORT_PC
   movq rSELF:THREAD_SELF_OFFSET, %rdi
   movq 0(%rsp), %rsi
   movq rPC, %rdx
   call SYMBOL(nterp_load_object)
   jmp 1b
3:
   // 00 is %rax
   call art_quick_read_barrier_mark_reg00
   jmp 1b

    END nterp_op_const_string_jumbo

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_const_class: /* 0x1c */
    ENTRY nterp_op_const_class

   // Fast-path which gets the object from thread-local cache.
   FETCH_FROM_THREAD_CACHE %rax, 2f
   cmpq MACRO_LITERAL(0), rSELF:THREAD_READ_BARRIER_MARK_REG00_OFFSET
   jne 3f
1:
   SET_VREG_OBJECT %eax, rINSTq            # vAA <- value
   .if 0
   ADVANCE_PC_FETCH_AND_GOTO_NEXT 3
   .else
   ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
   .endif
2:
   EXPORT_PC
   movq rSELF:THREAD_SELF_OFFSET, %rdi
   movq 0(%rsp), %rsi
   movq rPC, %rdx
   call SYMBOL(nterp_get_class_or_allocate_object)
   jmp 1b
3:
   // 00 is %rax
   call art_quick_read_barrier_mark_reg00
   jmp 1b

    END nterp_op_const_class

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_monitor_enter: /* 0x1d */
    ENTRY nterp_op_monitor_enter

/*
 * Synchronize on an object.
 */
    /* monitor-enter vAA */
    EXPORT_PC
    GET_VREG %edi, rINSTq
    call art_quick_lock_object
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_monitor_enter

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_monitor_exit: /* 0x1e */
    ENTRY nterp_op_monitor_exit

/*
 * Unlock an object.
 *
 * Exceptions that occur when unlocking a monitor need to appear as
 * if they happened at the following instruction.  See the Dalvik
 * instruction spec.
 */
    /* monitor-exit vAA */
    EXPORT_PC
    GET_VREG %edi, rINSTq
    call art_quick_unlock_object
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_monitor_exit

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_check_cast: /* 0x1f */
    ENTRY nterp_op_check_cast

  jmp NterpCheckCast

    END nterp_op_check_cast

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_instance_of: /* 0x20 */
    ENTRY nterp_op_instance_of

   jmp NterpInstanceOf

    END nterp_op_instance_of

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_array_length: /* 0x21 */
    ENTRY nterp_op_array_length

/*
 * Return the length of an array.
 */
    movl    rINST, %eax                     # eax <- BA
    sarl    $4, rINST                      # rINST <- B
    GET_VREG %ecx, rINSTq                   # ecx <- vB (object ref)
    testl   %ecx, %ecx                      # is null?
    je      common_errNullObject
    andb    $0xf, %al                      # eax <- A
    movl    MIRROR_ARRAY_LENGTH_OFFSET(%rcx), rINST
    SET_VREG rINST, %rax
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_array_length

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_new_instance: /* 0x22 */
    ENTRY nterp_op_new_instance

   // The routine is too big to fit in a handler, so jump to it.
   jmp NterpNewInstance
    END nterp_op_new_instance

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_new_array: /* 0x23 */
    ENTRY nterp_op_new_array

  jmp NterpNewArray
    END nterp_op_new_array

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_filled_new_array: /* 0x24 */
    ENTRY nterp_op_filled_new_array

/*
 * Create a new array with elements filled from registers.
 *
 * for: filled-new-array, filled-new-array/range
 */
    /* op vB, {vD, vE, vF, vG, vA}, class@CCCC */
    /* op {vCCCC..v(CCCC+AA-1)}, type@BBBB */
    EXPORT_PC
    movq    rSELF:THREAD_SELF_OFFSET, OUT_ARG0
    movq    (%rsp), OUT_ARG1
    movq    rFP, OUT_ARG2
    movq    rPC, OUT_ARG3
    call    SYMBOL(nterp_filled_new_array)
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 3

    END nterp_op_filled_new_array

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_filled_new_array_range: /* 0x25 */
    ENTRY nterp_op_filled_new_array_range

/*
 * Create a new array with elements filled from registers.
 *
 * for: filled-new-array, filled-new-array/range
 */
    /* op vB, {vD, vE, vF, vG, vA}, class@CCCC */
    /* op {vCCCC..v(CCCC+AA-1)}, type@BBBB */
    EXPORT_PC
    movq    rSELF:THREAD_SELF_OFFSET, OUT_ARG0
    movq    (%rsp), OUT_ARG1
    movq    rFP, OUT_ARG2
    movq    rPC, OUT_ARG3
    call    SYMBOL(nterp_filled_new_array_range)
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 3

    END nterp_op_filled_new_array_range

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_fill_array_data: /* 0x26 */
    ENTRY nterp_op_fill_array_data

    /* fill-array-data vAA, +BBBBBBBB */
    EXPORT_PC
    movslq  2(rPC), %rcx                    # rcx <- ssssssssBBBBbbbb
    leaq    (rPC,%rcx,2), OUT_ARG0          # OUT_ARG0 <- PC + ssssssssBBBBbbbb*2
    GET_VREG OUT_32_ARG1, rINSTq            # OUT_ARG1 <- vAA (array object)
    call    art_quick_handle_fill_data
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 3

    END nterp_op_fill_array_data

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_throw: /* 0x27 */
    ENTRY nterp_op_throw

  EXPORT_PC
  GET_VREG %edi, rINSTq                   # edi<- vAA (exception object)
  movq rSELF:THREAD_SELF_OFFSET, %rsi
  call SYMBOL(art_quick_deliver_exception)
  int3
    END nterp_op_throw

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_goto: /* 0x28 */
    ENTRY nterp_op_goto

/*
 * Unconditional branch, 8-bit offset.
 *
 * The branch distance is a signed code-unit offset, which we need to
 * double to get a byte offset.
 */
    /* goto +AA */
    movsbq  rINSTbl, rINSTq                 # rINSTq <- ssssssAA
    BRANCH

    END nterp_op_goto

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_goto_16: /* 0x29 */
    ENTRY nterp_op_goto_16

/*
 * Unconditional branch, 16-bit offset.
 *
 * The branch distance is a signed code-unit offset, which we need to
 * double to get a byte offset.
 */
    /* goto/16 +AAAA */
    movswq  2(rPC), rINSTq                  # rINSTq <- ssssAAAA
    BRANCH

    END nterp_op_goto_16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_goto_32: /* 0x2a */
    ENTRY nterp_op_goto_32

/*
 * Unconditional branch, 32-bit offset.
 *
 * The branch distance is a signed code-unit offset, which we need to
 * double to get a byte offset.
 *
 * Because we need the SF bit set, we'll use an adds
 * to convert from Dalvik offset to byte offset.
 */
    /* goto/32 +AAAAAAAA */
    movslq  2(rPC), rINSTq                  # rINSTq <- AAAAAAAA
    BRANCH

    END nterp_op_goto_32

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_packed_switch: /* 0x2b */
    ENTRY nterp_op_packed_switch

/*
 * Handle a packed-switch or sparse-switch instruction.  In both cases
 * we decode it and hand it off to a helper function.
 *
 * We don't really expect backward branches in a switch statement, but
 * they're perfectly legal, so we check for them here.
 *
 * for: packed-switch, sparse-switch
 */
    /* op vAA, +BBBB */
    movslq  2(rPC), OUT_ARG0                # rcx <- ssssssssBBBBbbbb
    leaq    (rPC,OUT_ARG0,2), OUT_ARG0      # rcx <- PC + ssssssssBBBBbbbb*2
    GET_VREG OUT_32_ARG1, rINSTq            # eax <- vAA
    call    SYMBOL(NterpDoPackedSwitch)
    movslq  %eax, rINSTq
    BRANCH

/*
 * Return a 32-bit value.
 */
    END nterp_op_packed_switch

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sparse_switch: /* 0x2c */
    ENTRY nterp_op_sparse_switch

/*
 * Handle a packed-switch or sparse-switch instruction.  In both cases
 * we decode it and hand it off to a helper function.
 *
 * We don't really expect backward branches in a switch statement, but
 * they're perfectly legal, so we check for them here.
 *
 * for: packed-switch, sparse-switch
 */
    /* op vAA, +BBBB */
    movslq  2(rPC), OUT_ARG0                # rcx <- ssssssssBBBBbbbb
    leaq    (rPC,OUT_ARG0,2), OUT_ARG0      # rcx <- PC + ssssssssBBBBbbbb*2
    GET_VREG OUT_32_ARG1, rINSTq            # eax <- vAA
    call    SYMBOL(NterpDoSparseSwitch)
    movslq  %eax, rINSTq
    BRANCH

/*
 * Return a 32-bit value.
 */

    END nterp_op_sparse_switch

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_cmpl_float: /* 0x2d */
    ENTRY nterp_op_cmpl_float

/*
 * Compare two floating-point values.  Puts 0, 1, or -1 into the
 * destination register based on the results of the comparison.
 *
 * int compare(x, y) {
 *     if (x == y) {
 *         return 0;
 *     } else if (x < y) {
 *         return -1;
 *     } else if (x > y) {
 *         return 1;
 *     } else {
 *         return nanval ? 1 : -1;
 *     }
 * }
 */
    /* op vAA, vBB, vCC */
    movzbq  3(rPC), %rcx                    # ecx<- CC
    movzbq  2(rPC), %rax                    # eax<- BB
    GET_VREG_XMMs %xmm0, %rax
    xor     %eax, %eax
    ucomiss VREG_ADDRESS(%rcx), %xmm0
    jp      .Lop_cmpl_float_nan_is_neg
    je      .Lop_cmpl_float_finish
    jb      .Lop_cmpl_float_less
.Lop_cmpl_float_nan_is_pos:
    addb    $1, %al
    jmp     .Lop_cmpl_float_finish
.Lop_cmpl_float_nan_is_neg:
.Lop_cmpl_float_less:
    movl    $-1, %eax
.Lop_cmpl_float_finish:
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_cmpl_float

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_cmpg_float: /* 0x2e */
    ENTRY nterp_op_cmpg_float

/*
 * Compare two floating-point values.  Puts 0, 1, or -1 into the
 * destination register based on the results of the comparison.
 *
 * int compare(x, y) {
 *     if (x == y) {
 *         return 0;
 *     } else if (x < y) {
 *         return -1;
 *     } else if (x > y) {
 *         return 1;
 *     } else {
 *         return nanval ? 1 : -1;
 *     }
 * }
 */
    /* op vAA, vBB, vCC */
    movzbq  3(rPC), %rcx                    # ecx<- CC
    movzbq  2(rPC), %rax                    # eax<- BB
    GET_VREG_XMMs %xmm0, %rax
    xor     %eax, %eax
    ucomiss VREG_ADDRESS(%rcx), %xmm0
    jp      .Lop_cmpg_float_nan_is_pos
    je      .Lop_cmpg_float_finish
    jb      .Lop_cmpg_float_less
.Lop_cmpg_float_nan_is_pos:
    addb    $1, %al
    jmp     .Lop_cmpg_float_finish
.Lop_cmpg_float_nan_is_neg:
.Lop_cmpg_float_less:
    movl    $-1, %eax
.Lop_cmpg_float_finish:
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_cmpg_float

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_cmpl_double: /* 0x2f */
    ENTRY nterp_op_cmpl_double

/*
 * Compare two floating-point values.  Puts 0, 1, or -1 into the
 * destination register based on the results of the comparison.
 *
 * int compare(x, y) {
 *     if (x == y) {
 *         return 0;
 *     } else if (x < y) {
 *         return -1;
 *     } else if (x > y) {
 *         return 1;
 *     } else {
 *         return nanval ? 1 : -1;
 *     }
 * }
 */
    /* op vAA, vBB, vCC */
    movzbq  3(rPC), %rcx                    # ecx<- CC
    movzbq  2(rPC), %rax                    # eax<- BB
    GET_VREG_XMMd %xmm0, %rax
    xor     %eax, %eax
    ucomisd VREG_ADDRESS(%rcx), %xmm0
    jp      .Lop_cmpl_double_nan_is_neg
    je      .Lop_cmpl_double_finish
    jb      .Lop_cmpl_double_less
.Lop_cmpl_double_nan_is_pos:
    addb    $1, %al
    jmp     .Lop_cmpl_double_finish
.Lop_cmpl_double_nan_is_neg:
.Lop_cmpl_double_less:
    movl    $-1, %eax
.Lop_cmpl_double_finish:
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_cmpl_double

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_cmpg_double: /* 0x30 */
    ENTRY nterp_op_cmpg_double

/*
 * Compare two floating-point values.  Puts 0, 1, or -1 into the
 * destination register based on the results of the comparison.
 *
 * int compare(x, y) {
 *     if (x == y) {
 *         return 0;
 *     } else if (x < y) {
 *         return -1;
 *     } else if (x > y) {
 *         return 1;
 *     } else {
 *         return nanval ? 1 : -1;
 *     }
 * }
 */
    /* op vAA, vBB, vCC */
    movzbq  3(rPC), %rcx                    # ecx<- CC
    movzbq  2(rPC), %rax                    # eax<- BB
    GET_VREG_XMMd %xmm0, %rax
    xor     %eax, %eax
    ucomisd VREG_ADDRESS(%rcx), %xmm0
    jp      .Lop_cmpg_double_nan_is_pos
    je      .Lop_cmpg_double_finish
    jb      .Lop_cmpg_double_less
.Lop_cmpg_double_nan_is_pos:
    addb    $1, %al
    jmp     .Lop_cmpg_double_finish
.Lop_cmpg_double_nan_is_neg:
.Lop_cmpg_double_less:
    movl    $-1, %eax
.Lop_cmpg_double_finish:
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_cmpg_double

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_cmp_long: /* 0x31 */
    ENTRY nterp_op_cmp_long

/*
 * Compare two 64-bit values.  Puts 0, 1, or -1 into the destination
 * register based on the results of the comparison.
 */
    /* cmp-long vAA, vBB, vCC */
    movzbq  2(rPC), %rdx                    # edx <- BB
    movzbq  3(rPC), %rcx                    # ecx <- CC
    GET_WIDE_VREG %rdx, %rdx                # rdx <- v[BB]
    xorl    %eax, %eax
    xorl    %edi, %edi
    addb    $1, %al
    movl    $-1, %esi
    cmpq    VREG_ADDRESS(%rcx), %rdx
    cmovl   %esi, %edi
    cmovg   %eax, %edi
    SET_VREG %edi, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_cmp_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_if_eq: /* 0x32 */
    ENTRY nterp_op_if_eq

/*
 * Generic two-operand compare-and-branch operation.  Provide a "revcmp"
 * fragment that specifies the *reverse* comparison to perform, e.g.
 * for "if-le" you would use "gt".
 *
 * For: if-eq, if-ne, if-lt, if-ge, if-gt, if-le
 */
    /* if-cmp vA, vB, +CCCC */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # rcx <- A
    GET_VREG %eax, %rcx                     # eax <- vA
    cmpl    VREG_ADDRESS(rINSTq), %eax      # compare (vA, vB)
    jne   1f
    movswq  2(rPC), rINSTq                  # Get signed branch offset
    BRANCH
1:
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_if_eq

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_if_ne: /* 0x33 */
    ENTRY nterp_op_if_ne

/*
 * Generic two-operand compare-and-branch operation.  Provide a "revcmp"
 * fragment that specifies the *reverse* comparison to perform, e.g.
 * for "if-le" you would use "gt".
 *
 * For: if-eq, if-ne, if-lt, if-ge, if-gt, if-le
 */
    /* if-cmp vA, vB, +CCCC */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # rcx <- A
    GET_VREG %eax, %rcx                     # eax <- vA
    cmpl    VREG_ADDRESS(rINSTq), %eax      # compare (vA, vB)
    je   1f
    movswq  2(rPC), rINSTq                  # Get signed branch offset
    BRANCH
1:
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_if_ne

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_if_lt: /* 0x34 */
    ENTRY nterp_op_if_lt

/*
 * Generic two-operand compare-and-branch operation.  Provide a "revcmp"
 * fragment that specifies the *reverse* comparison to perform, e.g.
 * for "if-le" you would use "gt".
 *
 * For: if-eq, if-ne, if-lt, if-ge, if-gt, if-le
 */
    /* if-cmp vA, vB, +CCCC */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # rcx <- A
    GET_VREG %eax, %rcx                     # eax <- vA
    cmpl    VREG_ADDRESS(rINSTq), %eax      # compare (vA, vB)
    jge   1f
    movswq  2(rPC), rINSTq                  # Get signed branch offset
    BRANCH
1:
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_if_lt

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_if_ge: /* 0x35 */
    ENTRY nterp_op_if_ge

/*
 * Generic two-operand compare-and-branch operation.  Provide a "revcmp"
 * fragment that specifies the *reverse* comparison to perform, e.g.
 * for "if-le" you would use "gt".
 *
 * For: if-eq, if-ne, if-lt, if-ge, if-gt, if-le
 */
    /* if-cmp vA, vB, +CCCC */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # rcx <- A
    GET_VREG %eax, %rcx                     # eax <- vA
    cmpl    VREG_ADDRESS(rINSTq), %eax      # compare (vA, vB)
    jl   1f
    movswq  2(rPC), rINSTq                  # Get signed branch offset
    BRANCH
1:
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_if_ge

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_if_gt: /* 0x36 */
    ENTRY nterp_op_if_gt

/*
 * Generic two-operand compare-and-branch operation.  Provide a "revcmp"
 * fragment that specifies the *reverse* comparison to perform, e.g.
 * for "if-le" you would use "gt".
 *
 * For: if-eq, if-ne, if-lt, if-ge, if-gt, if-le
 */
    /* if-cmp vA, vB, +CCCC */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # rcx <- A
    GET_VREG %eax, %rcx                     # eax <- vA
    cmpl    VREG_ADDRESS(rINSTq), %eax      # compare (vA, vB)
    jle   1f
    movswq  2(rPC), rINSTq                  # Get signed branch offset
    BRANCH
1:
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_if_gt

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_if_le: /* 0x37 */
    ENTRY nterp_op_if_le

/*
 * Generic two-operand compare-and-branch operation.  Provide a "revcmp"
 * fragment that specifies the *reverse* comparison to perform, e.g.
 * for "if-le" you would use "gt".
 *
 * For: if-eq, if-ne, if-lt, if-ge, if-gt, if-le
 */
    /* if-cmp vA, vB, +CCCC */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # rcx <- A
    GET_VREG %eax, %rcx                     # eax <- vA
    cmpl    VREG_ADDRESS(rINSTq), %eax      # compare (vA, vB)
    jg   1f
    movswq  2(rPC), rINSTq                  # Get signed branch offset
    BRANCH
1:
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_if_le

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_if_eqz: /* 0x38 */
    ENTRY nterp_op_if_eqz

/*
 * Generic one-operand compare-and-branch operation.  Provide a "revcmp"
 * fragment that specifies the *reverse* comparison to perform, e.g.
 * for "if-le" you would use "gt".
 *
 * for: if-eqz, if-nez, if-ltz, if-gez, if-gtz, if-lez
 */
    /* if-cmp vAA, +BBBB */
    cmpl    $0, VREG_ADDRESS(rINSTq)       # compare (vA, 0)
    jne   1f
    movswq  2(rPC), rINSTq                  # fetch signed displacement
    BRANCH
1:
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_if_eqz

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_if_nez: /* 0x39 */
    ENTRY nterp_op_if_nez

/*
 * Generic one-operand compare-and-branch operation.  Provide a "revcmp"
 * fragment that specifies the *reverse* comparison to perform, e.g.
 * for "if-le" you would use "gt".
 *
 * for: if-eqz, if-nez, if-ltz, if-gez, if-gtz, if-lez
 */
    /* if-cmp vAA, +BBBB */
    cmpl    $0, VREG_ADDRESS(rINSTq)       # compare (vA, 0)
    je   1f
    movswq  2(rPC), rINSTq                  # fetch signed displacement
    BRANCH
1:
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_if_nez

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_if_ltz: /* 0x3a */
    ENTRY nterp_op_if_ltz

/*
 * Generic one-operand compare-and-branch operation.  Provide a "revcmp"
 * fragment that specifies the *reverse* comparison to perform, e.g.
 * for "if-le" you would use "gt".
 *
 * for: if-eqz, if-nez, if-ltz, if-gez, if-gtz, if-lez
 */
    /* if-cmp vAA, +BBBB */
    cmpl    $0, VREG_ADDRESS(rINSTq)       # compare (vA, 0)
    jge   1f
    movswq  2(rPC), rINSTq                  # fetch signed displacement
    BRANCH
1:
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_if_ltz

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_if_gez: /* 0x3b */
    ENTRY nterp_op_if_gez

/*
 * Generic one-operand compare-and-branch operation.  Provide a "revcmp"
 * fragment that specifies the *reverse* comparison to perform, e.g.
 * for "if-le" you would use "gt".
 *
 * for: if-eqz, if-nez, if-ltz, if-gez, if-gtz, if-lez
 */
    /* if-cmp vAA, +BBBB */
    cmpl    $0, VREG_ADDRESS(rINSTq)       # compare (vA, 0)
    jl   1f
    movswq  2(rPC), rINSTq                  # fetch signed displacement
    BRANCH
1:
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_if_gez

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_if_gtz: /* 0x3c */
    ENTRY nterp_op_if_gtz

/*
 * Generic one-operand compare-and-branch operation.  Provide a "revcmp"
 * fragment that specifies the *reverse* comparison to perform, e.g.
 * for "if-le" you would use "gt".
 *
 * for: if-eqz, if-nez, if-ltz, if-gez, if-gtz, if-lez
 */
    /* if-cmp vAA, +BBBB */
    cmpl    $0, VREG_ADDRESS(rINSTq)       # compare (vA, 0)
    jle   1f
    movswq  2(rPC), rINSTq                  # fetch signed displacement
    BRANCH
1:
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_if_gtz

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_if_lez: /* 0x3d */
    ENTRY nterp_op_if_lez

/*
 * Generic one-operand compare-and-branch operation.  Provide a "revcmp"
 * fragment that specifies the *reverse* comparison to perform, e.g.
 * for "if-le" you would use "gt".
 *
 * for: if-eqz, if-nez, if-ltz, if-gez, if-gtz, if-lez
 */
    /* if-cmp vAA, +BBBB */
    cmpl    $0, VREG_ADDRESS(rINSTq)       # compare (vA, 0)
    jg   1f
    movswq  2(rPC), rINSTq                  # fetch signed displacement
    BRANCH
1:
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_if_lez

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_3e: /* 0x3e */
    ENTRY nterp_op_unused_3e

    int3

    END nterp_op_unused_3e

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_3f: /* 0x3f */
    ENTRY nterp_op_unused_3f

    int3

    END nterp_op_unused_3f

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_40: /* 0x40 */
    ENTRY nterp_op_unused_40

    int3

    END nterp_op_unused_40

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_41: /* 0x41 */
    ENTRY nterp_op_unused_41

    int3

    END nterp_op_unused_41

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_42: /* 0x42 */
    ENTRY nterp_op_unused_42

    int3

    END nterp_op_unused_42

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_43: /* 0x43 */
    ENTRY nterp_op_unused_43

    int3

    END nterp_op_unused_43

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aget: /* 0x44 */
    ENTRY nterp_op_aget

/*
 * Array get.  vAA <- vBB[vCC].
 *
 * for: aget, aget-boolean, aget-byte, aget-char, aget-short, aget-wide, aget-object
 *
 */
    /* op vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # eax <- BB
    movzbq  3(rPC), %rcx                    # ecx <- CC
    GET_VREG %edi, %rax                     # eax <- vBB (array object)
    GET_VREG %esi, %rcx                     # ecx <- vCC (requested index)
    testl   %edi, %edi                      # null array object?
    je      common_errNullObject            # bail if so
    cmpl    MIRROR_ARRAY_LENGTH_OFFSET(%edi), %esi
    jae     common_errArrayIndex            # index >= length, bail.
    .if 0
    movq    MIRROR_INT_ARRAY_DATA_OFFSET(%rdi,%rsi,8), %rax
    SET_WIDE_VREG %rax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
    .elseif 0
    testb $READ_BARRIER_TEST_VALUE, GRAY_BYTE_OFFSET(%edi)
    movl   MIRROR_INT_ARRAY_DATA_OFFSET(%rdi,%rsi,4), %eax
    jnz 2f
1:
    SET_VREG_OBJECT %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
2:
    // reg00 is eax
    call art_quick_read_barrier_mark_reg00
    jmp 1b
    .else
    movl   MIRROR_INT_ARRAY_DATA_OFFSET(%rdi,%rsi,4), %eax
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
    .endif

    END nterp_op_aget

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aget_wide: /* 0x45 */
    ENTRY nterp_op_aget_wide

/*
 * Array get.  vAA <- vBB[vCC].
 *
 * for: aget, aget-boolean, aget-byte, aget-char, aget-short, aget-wide, aget-object
 *
 */
    /* op vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # eax <- BB
    movzbq  3(rPC), %rcx                    # ecx <- CC
    GET_VREG %edi, %rax                     # eax <- vBB (array object)
    GET_VREG %esi, %rcx                     # ecx <- vCC (requested index)
    testl   %edi, %edi                      # null array object?
    je      common_errNullObject            # bail if so
    cmpl    MIRROR_ARRAY_LENGTH_OFFSET(%edi), %esi
    jae     common_errArrayIndex            # index >= length, bail.
    .if 1
    movq    MIRROR_WIDE_ARRAY_DATA_OFFSET(%rdi,%rsi,8), %rax
    SET_WIDE_VREG %rax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
    .elseif 0
    testb $READ_BARRIER_TEST_VALUE, GRAY_BYTE_OFFSET(%edi)
    movq   MIRROR_WIDE_ARRAY_DATA_OFFSET(%rdi,%rsi,8), %eax
    jnz 2f
1:
    SET_VREG_OBJECT %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
2:
    // reg00 is eax
    call art_quick_read_barrier_mark_reg00
    jmp 1b
    .else
    movq   MIRROR_WIDE_ARRAY_DATA_OFFSET(%rdi,%rsi,8), %eax
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
    .endif

    END nterp_op_aget_wide

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aget_object: /* 0x46 */
    ENTRY nterp_op_aget_object

/*
 * Array get.  vAA <- vBB[vCC].
 *
 * for: aget, aget-boolean, aget-byte, aget-char, aget-short, aget-wide, aget-object
 *
 */
    /* op vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # eax <- BB
    movzbq  3(rPC), %rcx                    # ecx <- CC
    GET_VREG %edi, %rax                     # eax <- vBB (array object)
    GET_VREG %esi, %rcx                     # ecx <- vCC (requested index)
    testl   %edi, %edi                      # null array object?
    je      common_errNullObject            # bail if so
    cmpl    MIRROR_ARRAY_LENGTH_OFFSET(%edi), %esi
    jae     common_errArrayIndex            # index >= length, bail.
    .if 0
    movq    MIRROR_OBJECT_ARRAY_DATA_OFFSET(%rdi,%rsi,8), %rax
    SET_WIDE_VREG %rax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
    .elseif 1
    testb $READ_BARRIER_TEST_VALUE, GRAY_BYTE_OFFSET(%edi)
    movl   MIRROR_OBJECT_ARRAY_DATA_OFFSET(%rdi,%rsi,4), %eax
    jnz 2f
1:
    SET_VREG_OBJECT %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
2:
    // reg00 is eax
    call art_quick_read_barrier_mark_reg00
    jmp 1b
    .else
    movl   MIRROR_OBJECT_ARRAY_DATA_OFFSET(%rdi,%rsi,4), %eax
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
    .endif

    END nterp_op_aget_object

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aget_boolean: /* 0x47 */
    ENTRY nterp_op_aget_boolean

/*
 * Array get.  vAA <- vBB[vCC].
 *
 * for: aget, aget-boolean, aget-byte, aget-char, aget-short, aget-wide, aget-object
 *
 */
    /* op vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # eax <- BB
    movzbq  3(rPC), %rcx                    # ecx <- CC
    GET_VREG %edi, %rax                     # eax <- vBB (array object)
    GET_VREG %esi, %rcx                     # ecx <- vCC (requested index)
    testl   %edi, %edi                      # null array object?
    je      common_errNullObject            # bail if so
    cmpl    MIRROR_ARRAY_LENGTH_OFFSET(%edi), %esi
    jae     common_errArrayIndex            # index >= length, bail.
    .if 0
    movq    MIRROR_BOOLEAN_ARRAY_DATA_OFFSET(%rdi,%rsi,8), %rax
    SET_WIDE_VREG %rax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
    .elseif 0
    testb $READ_BARRIER_TEST_VALUE, GRAY_BYTE_OFFSET(%edi)
    movzbl   MIRROR_BOOLEAN_ARRAY_DATA_OFFSET(%rdi,%rsi,1), %eax
    jnz 2f
1:
    SET_VREG_OBJECT %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
2:
    // reg00 is eax
    call art_quick_read_barrier_mark_reg00
    jmp 1b
    .else
    movzbl   MIRROR_BOOLEAN_ARRAY_DATA_OFFSET(%rdi,%rsi,1), %eax
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
    .endif

    END nterp_op_aget_boolean

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aget_byte: /* 0x48 */
    ENTRY nterp_op_aget_byte

/*
 * Array get.  vAA <- vBB[vCC].
 *
 * for: aget, aget-boolean, aget-byte, aget-char, aget-short, aget-wide, aget-object
 *
 */
    /* op vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # eax <- BB
    movzbq  3(rPC), %rcx                    # ecx <- CC
    GET_VREG %edi, %rax                     # eax <- vBB (array object)
    GET_VREG %esi, %rcx                     # ecx <- vCC (requested index)
    testl   %edi, %edi                      # null array object?
    je      common_errNullObject            # bail if so
    cmpl    MIRROR_ARRAY_LENGTH_OFFSET(%edi), %esi
    jae     common_errArrayIndex            # index >= length, bail.
    .if 0
    movq    MIRROR_BYTE_ARRAY_DATA_OFFSET(%rdi,%rsi,8), %rax
    SET_WIDE_VREG %rax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
    .elseif 0
    testb $READ_BARRIER_TEST_VALUE, GRAY_BYTE_OFFSET(%edi)
    movsbl   MIRROR_BYTE_ARRAY_DATA_OFFSET(%rdi,%rsi,1), %eax
    jnz 2f
1:
    SET_VREG_OBJECT %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
2:
    // reg00 is eax
    call art_quick_read_barrier_mark_reg00
    jmp 1b
    .else
    movsbl   MIRROR_BYTE_ARRAY_DATA_OFFSET(%rdi,%rsi,1), %eax
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
    .endif

    END nterp_op_aget_byte

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aget_char: /* 0x49 */
    ENTRY nterp_op_aget_char

/*
 * Array get.  vAA <- vBB[vCC].
 *
 * for: aget, aget-boolean, aget-byte, aget-char, aget-short, aget-wide, aget-object
 *
 */
    /* op vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # eax <- BB
    movzbq  3(rPC), %rcx                    # ecx <- CC
    GET_VREG %edi, %rax                     # eax <- vBB (array object)
    GET_VREG %esi, %rcx                     # ecx <- vCC (requested index)
    testl   %edi, %edi                      # null array object?
    je      common_errNullObject            # bail if so
    cmpl    MIRROR_ARRAY_LENGTH_OFFSET(%edi), %esi
    jae     common_errArrayIndex            # index >= length, bail.
    .if 0
    movq    MIRROR_CHAR_ARRAY_DATA_OFFSET(%rdi,%rsi,8), %rax
    SET_WIDE_VREG %rax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
    .elseif 0
    testb $READ_BARRIER_TEST_VALUE, GRAY_BYTE_OFFSET(%edi)
    movzwl   MIRROR_CHAR_ARRAY_DATA_OFFSET(%rdi,%rsi,2), %eax
    jnz 2f
1:
    SET_VREG_OBJECT %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
2:
    // reg00 is eax
    call art_quick_read_barrier_mark_reg00
    jmp 1b
    .else
    movzwl   MIRROR_CHAR_ARRAY_DATA_OFFSET(%rdi,%rsi,2), %eax
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
    .endif

    END nterp_op_aget_char

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aget_short: /* 0x4a */
    ENTRY nterp_op_aget_short

/*
 * Array get.  vAA <- vBB[vCC].
 *
 * for: aget, aget-boolean, aget-byte, aget-char, aget-short, aget-wide, aget-object
 *
 */
    /* op vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # eax <- BB
    movzbq  3(rPC), %rcx                    # ecx <- CC
    GET_VREG %edi, %rax                     # eax <- vBB (array object)
    GET_VREG %esi, %rcx                     # ecx <- vCC (requested index)
    testl   %edi, %edi                      # null array object?
    je      common_errNullObject            # bail if so
    cmpl    MIRROR_ARRAY_LENGTH_OFFSET(%edi), %esi
    jae     common_errArrayIndex            # index >= length, bail.
    .if 0
    movq    MIRROR_SHORT_ARRAY_DATA_OFFSET(%rdi,%rsi,8), %rax
    SET_WIDE_VREG %rax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
    .elseif 0
    testb $READ_BARRIER_TEST_VALUE, GRAY_BYTE_OFFSET(%edi)
    movswl   MIRROR_SHORT_ARRAY_DATA_OFFSET(%rdi,%rsi,2), %eax
    jnz 2f
1:
    SET_VREG_OBJECT %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
2:
    // reg00 is eax
    call art_quick_read_barrier_mark_reg00
    jmp 1b
    .else
    movswl   MIRROR_SHORT_ARRAY_DATA_OFFSET(%rdi,%rsi,2), %eax
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
    .endif

    END nterp_op_aget_short

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aput: /* 0x4b */
    ENTRY nterp_op_aput

/*
 * Array put.  vBB[vCC] <- vAA.
 *
 * for: aput, aput-boolean, aput-byte, aput-char, aput-short, aput-wide
 *
 */
    /* op vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # rax <- BB
    movzbq  3(rPC), %rcx                    # rcx <- CC
    GET_VREG %edi, %rax                     # edi <- vBB (array object)
    GET_VREG %esi, %rcx                     # esi <- vCC (requested index)
    testl   %edi, %edi                      # null array object?
    je      common_errNullObject            # bail if so
    cmpl    MIRROR_ARRAY_LENGTH_OFFSET(%edi), %esi
    jae     common_errArrayIndex            # index >= length, bail.
    .if 0
    GET_WIDE_VREG rINSTq, rINSTq
    .else
    GET_VREG rINST, rINSTq
    .endif
    movl    rINST, MIRROR_INT_ARRAY_DATA_OFFSET(%rdi,%rsi,4)
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_aput

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aput_wide: /* 0x4c */
    ENTRY nterp_op_aput_wide

/*
 * Array put.  vBB[vCC] <- vAA.
 *
 * for: aput, aput-boolean, aput-byte, aput-char, aput-short, aput-wide
 *
 */
    /* op vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # rax <- BB
    movzbq  3(rPC), %rcx                    # rcx <- CC
    GET_VREG %edi, %rax                     # edi <- vBB (array object)
    GET_VREG %esi, %rcx                     # esi <- vCC (requested index)
    testl   %edi, %edi                      # null array object?
    je      common_errNullObject            # bail if so
    cmpl    MIRROR_ARRAY_LENGTH_OFFSET(%edi), %esi
    jae     common_errArrayIndex            # index >= length, bail.
    .if 1
    GET_WIDE_VREG rINSTq, rINSTq
    .else
    GET_VREG rINST, rINSTq
    .endif
    movq    rINSTq, MIRROR_WIDE_ARRAY_DATA_OFFSET(%rdi,%rsi,8)
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_aput_wide

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aput_object: /* 0x4d */
    ENTRY nterp_op_aput_object

    EXPORT_PC                               # for the art_quick_aput_obj call
    movzbq  2(rPC), %rax                    # rax <- BB
    movzbq  3(rPC), %rcx                    # rcx <- CC
    GET_VREG %edi, %rax                     # edi <- vBB (array object)
    GET_VREG %esi, %rcx                     # esi <- vCC (requested index)
    testl   %edi, %edi                      # null array object?
    je      common_errNullObject            # bail if so
    cmpl    MIRROR_ARRAY_LENGTH_OFFSET(%edi), %esi
    jae     common_errArrayIndex            # index >= length, bail.
    GET_VREG %edx, rINSTq
    call art_quick_aput_obj
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_aput_object

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aput_boolean: /* 0x4e */
    ENTRY nterp_op_aput_boolean

/*
 * Array put.  vBB[vCC] <- vAA.
 *
 * for: aput, aput-boolean, aput-byte, aput-char, aput-short, aput-wide
 *
 */
    /* op vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # rax <- BB
    movzbq  3(rPC), %rcx                    # rcx <- CC
    GET_VREG %edi, %rax                     # edi <- vBB (array object)
    GET_VREG %esi, %rcx                     # esi <- vCC (requested index)
    testl   %edi, %edi                      # null array object?
    je      common_errNullObject            # bail if so
    cmpl    MIRROR_ARRAY_LENGTH_OFFSET(%edi), %esi
    jae     common_errArrayIndex            # index >= length, bail.
    .if 0
    GET_WIDE_VREG rINSTq, rINSTq
    .else
    GET_VREG rINST, rINSTq
    .endif
    movb    rINSTbl, MIRROR_BOOLEAN_ARRAY_DATA_OFFSET(%rdi,%rsi,1)
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_aput_boolean

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aput_byte: /* 0x4f */
    ENTRY nterp_op_aput_byte

/*
 * Array put.  vBB[vCC] <- vAA.
 *
 * for: aput, aput-boolean, aput-byte, aput-char, aput-short, aput-wide
 *
 */
    /* op vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # rax <- BB
    movzbq  3(rPC), %rcx                    # rcx <- CC
    GET_VREG %edi, %rax                     # edi <- vBB (array object)
    GET_VREG %esi, %rcx                     # esi <- vCC (requested index)
    testl   %edi, %edi                      # null array object?
    je      common_errNullObject            # bail if so
    cmpl    MIRROR_ARRAY_LENGTH_OFFSET(%edi), %esi
    jae     common_errArrayIndex            # index >= length, bail.
    .if 0
    GET_WIDE_VREG rINSTq, rINSTq
    .else
    GET_VREG rINST, rINSTq
    .endif
    movb    rINSTbl, MIRROR_BYTE_ARRAY_DATA_OFFSET(%rdi,%rsi,1)
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_aput_byte

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aput_char: /* 0x50 */
    ENTRY nterp_op_aput_char

/*
 * Array put.  vBB[vCC] <- vAA.
 *
 * for: aput, aput-boolean, aput-byte, aput-char, aput-short, aput-wide
 *
 */
    /* op vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # rax <- BB
    movzbq  3(rPC), %rcx                    # rcx <- CC
    GET_VREG %edi, %rax                     # edi <- vBB (array object)
    GET_VREG %esi, %rcx                     # esi <- vCC (requested index)
    testl   %edi, %edi                      # null array object?
    je      common_errNullObject            # bail if so
    cmpl    MIRROR_ARRAY_LENGTH_OFFSET(%edi), %esi
    jae     common_errArrayIndex            # index >= length, bail.
    .if 0
    GET_WIDE_VREG rINSTq, rINSTq
    .else
    GET_VREG rINST, rINSTq
    .endif
    movw    rINSTw, MIRROR_CHAR_ARRAY_DATA_OFFSET(%rdi,%rsi,2)
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_aput_char

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aput_short: /* 0x51 */
    ENTRY nterp_op_aput_short

/*
 * Array put.  vBB[vCC] <- vAA.
 *
 * for: aput, aput-boolean, aput-byte, aput-char, aput-short, aput-wide
 *
 */
    /* op vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # rax <- BB
    movzbq  3(rPC), %rcx                    # rcx <- CC
    GET_VREG %edi, %rax                     # edi <- vBB (array object)
    GET_VREG %esi, %rcx                     # esi <- vCC (requested index)
    testl   %edi, %edi                      # null array object?
    je      common_errNullObject            # bail if so
    cmpl    MIRROR_ARRAY_LENGTH_OFFSET(%edi), %esi
    jae     common_errArrayIndex            # index >= length, bail.
    .if 0
    GET_WIDE_VREG rINSTq, rINSTq
    .else
    GET_VREG rINST, rINSTq
    .endif
    movw    rINSTw, MIRROR_SHORT_ARRAY_DATA_OFFSET(%rdi,%rsi,2)
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_aput_short

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iget: /* 0x52 */
    ENTRY nterp_op_iget

   jmp NterpGetInstanceField

    END nterp_op_iget

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iget_wide: /* 0x53 */
    ENTRY nterp_op_iget_wide

   jmp NterpGetWideInstanceField

    END nterp_op_iget_wide

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iget_object: /* 0x54 */
    ENTRY nterp_op_iget_object

    jmp NterpGetObjectInstanceField

    END nterp_op_iget_object

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iget_boolean: /* 0x55 */
    ENTRY nterp_op_iget_boolean

   jmp NterpGetBooleanInstanceField

    END nterp_op_iget_boolean

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iget_byte: /* 0x56 */
    ENTRY nterp_op_iget_byte

   jmp NterpGetByteInstanceField

    END nterp_op_iget_byte

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iget_char: /* 0x57 */
    ENTRY nterp_op_iget_char

   jmp NterpGetCharInstanceField

    END nterp_op_iget_char

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iget_short: /* 0x58 */
    ENTRY nterp_op_iget_short

   jmp NterpGetShortInstanceField

    END nterp_op_iget_short

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iput: /* 0x59 */
    ENTRY nterp_op_iput

   jmp NterpPutInstanceField

    END nterp_op_iput

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iput_wide: /* 0x5a */
    ENTRY nterp_op_iput_wide

   jmp NterpPutWideInstanceField

    END nterp_op_iput_wide

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iput_object: /* 0x5b */
    ENTRY nterp_op_iput_object

    jmp NterpPutObjectInstanceField

    END nterp_op_iput_object

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iput_boolean: /* 0x5c */
    ENTRY nterp_op_iput_boolean

   jmp NterpPutBooleanInstanceField

    END nterp_op_iput_boolean

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iput_byte: /* 0x5d */
    ENTRY nterp_op_iput_byte

   jmp NterpPutByteInstanceField

    END nterp_op_iput_byte

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iput_char: /* 0x5e */
    ENTRY nterp_op_iput_char

   jmp NterpPutCharInstanceField

    END nterp_op_iput_char

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iput_short: /* 0x5f */
    ENTRY nterp_op_iput_short

   jmp NterpPutShortInstanceField

    END nterp_op_iput_short

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sget: /* 0x60 */
    ENTRY nterp_op_sget

   jmp NterpGetIntStaticField

    END nterp_op_sget

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sget_wide: /* 0x61 */
    ENTRY nterp_op_sget_wide

   jmp NterpGetWideStaticField

    END nterp_op_sget_wide

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sget_object: /* 0x62 */
    ENTRY nterp_op_sget_object

   jmp NterpGetObjectStaticField

    END nterp_op_sget_object

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sget_boolean: /* 0x63 */
    ENTRY nterp_op_sget_boolean

   jmp NterpGetBooleanStaticField

    END nterp_op_sget_boolean

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sget_byte: /* 0x64 */
    ENTRY nterp_op_sget_byte

   jmp NterpGetByteStaticField

    END nterp_op_sget_byte

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sget_char: /* 0x65 */
    ENTRY nterp_op_sget_char

   jmp NterpGetCharStaticField

    END nterp_op_sget_char

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sget_short: /* 0x66 */
    ENTRY nterp_op_sget_short

   jmp NterpGetShortStaticField

    END nterp_op_sget_short

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sput: /* 0x67 */
    ENTRY nterp_op_sput

   jmp NterpPutStaticField

    END nterp_op_sput

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sput_wide: /* 0x68 */
    ENTRY nterp_op_sput_wide

   jmp NterpPutWideStaticField

    END nterp_op_sput_wide

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sput_object: /* 0x69 */
    ENTRY nterp_op_sput_object

   jmp NterpPutObjectStaticField

    END nterp_op_sput_object

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sput_boolean: /* 0x6a */
    ENTRY nterp_op_sput_boolean

   jmp NterpPutBooleanStaticField

    END nterp_op_sput_boolean

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sput_byte: /* 0x6b */
    ENTRY nterp_op_sput_byte

   jmp NterpPutByteStaticField

    END nterp_op_sput_byte

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sput_char: /* 0x6c */
    ENTRY nterp_op_sput_char

   jmp NterpPutCharStaticField

    END nterp_op_sput_char

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sput_short: /* 0x6d */
    ENTRY nterp_op_sput_short

   jmp NterpPutShortStaticField

    END nterp_op_sput_short

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_virtual: /* 0x6e */
    ENTRY nterp_op_invoke_virtual

   EXPORT_PC
   // Fast-path which gets the method from thread-local cache.
   FETCH_FROM_THREAD_CACHE %rdi, 2f
1:
   // First argument is the 'this' pointer.
   movzwl 4(rPC), %r11d // arguments
   .if !0
   andq $0xf, %r11
   .endif
   movl (rFP, %r11, 4), %esi
   // Note: if esi is null, this will be handled by our SIGSEGV handler.
   movl MIRROR_OBJECT_CLASS_OFFSET(%esi), %edx
   movq MIRROR_CLASS_VTABLE_OFFSET_64(%edx, %edi, 8), %rdi
   jmp NterpCommonInvokeInstance
2:
   movq rSELF:THREAD_SELF_OFFSET, %rdi
   movq 0(%rsp), %rsi
   movq rPC, %rdx
   call nterp_get_method
   movl %eax, %edi
   jmp 1b

    END nterp_op_invoke_virtual

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_super: /* 0x6f */
    ENTRY nterp_op_invoke_super

   EXPORT_PC
   // Fast-path which gets the method from thread-local cache.
   FETCH_FROM_THREAD_CACHE %rdi, 2f
1:
   // Load the first argument (the 'this' pointer).
   movzwl 4(rPC), %r11d // arguments
   .if !0
   andq $0xf, %r11
   .endif
   movl (rFP, %r11, 4), %esi
   // NullPointerException check.
   movl (%esi), %eax
   jmp NterpCommonInvokeInstance
2:
   movq rSELF:THREAD_SELF_OFFSET, %rdi
   movq 0(%rsp), %rsi
   movq rPC, %rdx
   call nterp_get_method
   movq %rax, %rdi
   .if 1
   jmp 1b
   .else
   testl MACRO_LITERAL(1), %eax
   je 1b
   andq $-2, %rdi  // Remove the extra bit that marks it's a String.<init> method.
   .if 0
   jmp NterpHandleStringInitRange
   .else
   jmp NterpHandleStringInit
   .endif
   .endif

    END nterp_op_invoke_super

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_direct: /* 0x70 */
    ENTRY nterp_op_invoke_direct

   EXPORT_PC
   // Fast-path which gets the method from thread-local cache.
   FETCH_FROM_THREAD_CACHE %rdi, 2f
1:
   // Load the first argument (the 'this' pointer).
   movzwl 4(rPC), %r11d // arguments
   .if !0
   andq $0xf, %r11
   .endif
   movl (rFP, %r11, 4), %esi
   // NullPointerException check.
   movl (%esi), %eax
   jmp NterpCommonInvokeInstance
2:
   movq rSELF:THREAD_SELF_OFFSET, %rdi
   movq 0(%rsp), %rsi
   movq rPC, %rdx
   call nterp_get_method
   movq %rax, %rdi
   .if 0
   jmp 1b
   .else
   testl MACRO_LITERAL(1), %eax
   je 1b
   andq $-2, %rdi  // Remove the extra bit that marks it's a String.<init> method.
   .if 0
   jmp NterpHandleStringInitRange
   .else
   jmp NterpHandleStringInit
   .endif
   .endif

    END nterp_op_invoke_direct

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_static: /* 0x71 */
    ENTRY nterp_op_invoke_static

   EXPORT_PC
   // Fast-path which gets the method from thread-local cache.
   FETCH_FROM_THREAD_CACHE %rdi, 1f
   jmp NterpCommonInvokeStatic
1:
   movq rSELF:THREAD_SELF_OFFSET, %rdi
   movq 0(%rsp), %rsi
   movq rPC, %rdx
   call nterp_get_method
   movq %rax, %rdi
   jmp NterpCommonInvokeStatic

    END nterp_op_invoke_static

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_interface: /* 0x72 */
    ENTRY nterp_op_invoke_interface

   EXPORT_PC
   // Fast-path which gets the interface method from thread-local cache.
   FETCH_FROM_THREAD_CACHE %rax, nterp_op_invoke_interface_helper
.Lop_invoke_interface_resume:
   // First argument is the 'this' pointer.
   movzwl 4(rPC), %r11d // arguments
   .if !0
   andq $0xf, %r11
   .endif
   movl (rFP, %r11, 4), %esi
   movl MIRROR_OBJECT_CLASS_OFFSET(%esi), %edx
   // Test the first two bits of the fetched ArtMethod:
   // - If the first bit is set, this is a method on j.l.Object
   // - If the second bit is set, this is a default method.
   testl $3, %eax
   jne 2f
   movzw ART_METHOD_IMT_INDEX_OFFSET(%rax), %ecx
1:
   movq MIRROR_CLASS_IMT_PTR_OFFSET_64(%edx), %rdx
   movq (%rdx, %rcx, 8), %rdi
   jmp NterpCommonInvokeInterface
2:
   testl $1, %eax
   .if 0
   jne NterpHandleInvokeInterfaceOnObjectMethodRange
   .else
   jne NterpHandleInvokeInterfaceOnObjectMethod
   .endif
   // Default method
   andq $-4, %rax
   movzw ART_METHOD_METHOD_INDEX_OFFSET(%rax), %ecx
   andl $ART_METHOD_IMT_MASK, %ecx
   jmp 1b

    END nterp_op_invoke_interface

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_73: /* 0x73 */
    ENTRY nterp_op_unused_73

    int3

    END nterp_op_unused_73

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_virtual_range: /* 0x74 */
    ENTRY nterp_op_invoke_virtual_range

   EXPORT_PC
   // Fast-path which gets the method from thread-local cache.
   FETCH_FROM_THREAD_CACHE %rdi, 2f
1:
   // First argument is the 'this' pointer.
   movzwl 4(rPC), %r11d // arguments
   .if !1
   andq $0xf, %r11
   .endif
   movl (rFP, %r11, 4), %esi
   // Note: if esi is null, this will be handled by our SIGSEGV handler.
   movl MIRROR_OBJECT_CLASS_OFFSET(%esi), %edx
   movq MIRROR_CLASS_VTABLE_OFFSET_64(%edx, %edi, 8), %rdi
   jmp NterpCommonInvokeInstanceRange
2:
   movq rSELF:THREAD_SELF_OFFSET, %rdi
   movq 0(%rsp), %rsi
   movq rPC, %rdx
   call nterp_get_method
   movl %eax, %edi
   jmp 1b

    END nterp_op_invoke_virtual_range

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_super_range: /* 0x75 */
    ENTRY nterp_op_invoke_super_range

   EXPORT_PC
   // Fast-path which gets the method from thread-local cache.
   FETCH_FROM_THREAD_CACHE %rdi, 2f
1:
   // Load the first argument (the 'this' pointer).
   movzwl 4(rPC), %r11d // arguments
   .if !1
   andq $0xf, %r11
   .endif
   movl (rFP, %r11, 4), %esi
   // NullPointerException check.
   movl (%esi), %eax
   jmp NterpCommonInvokeInstanceRange
2:
   movq rSELF:THREAD_SELF_OFFSET, %rdi
   movq 0(%rsp), %rsi
   movq rPC, %rdx
   call nterp_get_method
   movq %rax, %rdi
   .if 1
   jmp 1b
   .else
   testl MACRO_LITERAL(1), %eax
   je 1b
   andq $-2, %rdi  // Remove the extra bit that marks it's a String.<init> method.
   .if 1
   jmp NterpHandleStringInitRange
   .else
   jmp NterpHandleStringInit
   .endif
   .endif

    END nterp_op_invoke_super_range

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_direct_range: /* 0x76 */
    ENTRY nterp_op_invoke_direct_range

   EXPORT_PC
   // Fast-path which gets the method from thread-local cache.
   FETCH_FROM_THREAD_CACHE %rdi, 2f
1:
   // Load the first argument (the 'this' pointer).
   movzwl 4(rPC), %r11d // arguments
   .if !1
   andq $0xf, %r11
   .endif
   movl (rFP, %r11, 4), %esi
   // NullPointerException check.
   movl (%esi), %eax
   jmp NterpCommonInvokeInstanceRange
2:
   movq rSELF:THREAD_SELF_OFFSET, %rdi
   movq 0(%rsp), %rsi
   movq rPC, %rdx
   call nterp_get_method
   movq %rax, %rdi
   .if 0
   jmp 1b
   .else
   testl MACRO_LITERAL(1), %eax
   je 1b
   andq $-2, %rdi  // Remove the extra bit that marks it's a String.<init> method.
   .if 1
   jmp NterpHandleStringInitRange
   .else
   jmp NterpHandleStringInit
   .endif
   .endif

    END nterp_op_invoke_direct_range

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_static_range: /* 0x77 */
    ENTRY nterp_op_invoke_static_range

   EXPORT_PC
   // Fast-path which gets the method from thread-local cache.
   FETCH_FROM_THREAD_CACHE %rdi, 1f
   jmp NterpCommonInvokeStaticRange
1:
   movq rSELF:THREAD_SELF_OFFSET, %rdi
   movq 0(%rsp), %rsi
   movq rPC, %rdx
   call nterp_get_method
   movq %rax, %rdi
   jmp NterpCommonInvokeStaticRange

    END nterp_op_invoke_static_range

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_interface_range: /* 0x78 */
    ENTRY nterp_op_invoke_interface_range

   EXPORT_PC
   // Fast-path which gets the interface method from thread-local cache.
   FETCH_FROM_THREAD_CACHE %rax, nterp_op_invoke_interface_range_helper
.Lop_invoke_interface_range_resume:
   // First argument is the 'this' pointer.
   movzwl 4(rPC), %r11d // arguments
   .if !1
   andq $0xf, %r11
   .endif
   movl (rFP, %r11, 4), %esi
   movl MIRROR_OBJECT_CLASS_OFFSET(%esi), %edx
   // Test the first two bits of the fetched ArtMethod:
   // - If the first bit is set, this is a method on j.l.Object
   // - If the second bit is set, this is a default method.
   testl $3, %eax
   jne 2f
   movzw ART_METHOD_IMT_INDEX_OFFSET(%rax), %ecx
1:
   movq MIRROR_CLASS_IMT_PTR_OFFSET_64(%edx), %rdx
   movq (%rdx, %rcx, 8), %rdi
   jmp NterpCommonInvokeInterfaceRange
2:
   testl $1, %eax
   .if 1
   jne NterpHandleInvokeInterfaceOnObjectMethodRange
   .else
   jne NterpHandleInvokeInterfaceOnObjectMethod
   .endif
   // Default method
   andq $-4, %rax
   movzw ART_METHOD_METHOD_INDEX_OFFSET(%rax), %ecx
   andl $ART_METHOD_IMT_MASK, %ecx
   jmp 1b

    END nterp_op_invoke_interface_range

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_79: /* 0x79 */
    ENTRY nterp_op_unused_79

    int3

    END nterp_op_unused_79

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_7a: /* 0x7a */
    ENTRY nterp_op_unused_7a

    int3

    END nterp_op_unused_7a

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_neg_int: /* 0x7b */
    ENTRY nterp_op_neg_int

/*
 * Generic 32/64-bit unary operation.  Provide an "instr" line that
 * specifies an instruction that performs "result = op eax".
 */
    /* unop vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4,rINST                       # rINST <- B
    .if 0
    GET_WIDE_VREG %rax, rINSTq              # rax <- vB
    .else
    GET_VREG %eax, rINSTq                   # eax <- vB
    .endif
    andb    $0xf,%cl                       # ecx <- A

    negl    %eax
    .if 0
    SET_WIDE_VREG %rax, %rcx
    .else
    SET_VREG %eax, %rcx
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_neg_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_not_int: /* 0x7c */
    ENTRY nterp_op_not_int

/*
 * Generic 32/64-bit unary operation.  Provide an "instr" line that
 * specifies an instruction that performs "result = op eax".
 */
    /* unop vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4,rINST                       # rINST <- B
    .if 0
    GET_WIDE_VREG %rax, rINSTq              # rax <- vB
    .else
    GET_VREG %eax, rINSTq                   # eax <- vB
    .endif
    andb    $0xf,%cl                       # ecx <- A

    notl    %eax
    .if 0
    SET_WIDE_VREG %rax, %rcx
    .else
    SET_VREG %eax, %rcx
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_not_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_neg_long: /* 0x7d */
    ENTRY nterp_op_neg_long

/*
 * Generic 32/64-bit unary operation.  Provide an "instr" line that
 * specifies an instruction that performs "result = op eax".
 */
    /* unop vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4,rINST                       # rINST <- B
    .if 1
    GET_WIDE_VREG %rax, rINSTq              # rax <- vB
    .else
    GET_VREG %eax, rINSTq                   # eax <- vB
    .endif
    andb    $0xf,%cl                       # ecx <- A

    negq    %rax
    .if 1
    SET_WIDE_VREG %rax, %rcx
    .else
    SET_VREG %eax, %rcx
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_neg_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_not_long: /* 0x7e */
    ENTRY nterp_op_not_long

/*
 * Generic 32/64-bit unary operation.  Provide an "instr" line that
 * specifies an instruction that performs "result = op eax".
 */
    /* unop vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4,rINST                       # rINST <- B
    .if 1
    GET_WIDE_VREG %rax, rINSTq              # rax <- vB
    .else
    GET_VREG %eax, rINSTq                   # eax <- vB
    .endif
    andb    $0xf,%cl                       # ecx <- A

    notq    %rax
    .if 1
    SET_WIDE_VREG %rax, %rcx
    .else
    SET_VREG %eax, %rcx
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_not_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_neg_float: /* 0x7f */
    ENTRY nterp_op_neg_float

/*
 * Generic 32/64-bit unary operation.  Provide an "instr" line that
 * specifies an instruction that performs "result = op eax".
 */
    /* unop vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4,rINST                       # rINST <- B
    .if 0
    GET_WIDE_VREG %rax, rINSTq              # rax <- vB
    .else
    GET_VREG %eax, rINSTq                   # eax <- vB
    .endif
    andb    $0xf,%cl                       # ecx <- A

    xorl    $0x80000000, %eax
    .if 0
    SET_WIDE_VREG %rax, %rcx
    .else
    SET_VREG %eax, %rcx
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_neg_float

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_neg_double: /* 0x80 */
    ENTRY nterp_op_neg_double

/*
 * Generic 32/64-bit unary operation.  Provide an "instr" line that
 * specifies an instruction that performs "result = op eax".
 */
    /* unop vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4,rINST                       # rINST <- B
    .if 1
    GET_WIDE_VREG %rax, rINSTq              # rax <- vB
    .else
    GET_VREG %eax, rINSTq                   # eax <- vB
    .endif
    andb    $0xf,%cl                       # ecx <- A
    movq    $0x8000000000000000, %rsi
    xorq    %rsi, %rax
    .if 1
    SET_WIDE_VREG %rax, %rcx
    .else
    SET_VREG %eax, %rcx
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_neg_double

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_int_to_long: /* 0x81 */
    ENTRY nterp_op_int_to_long

    /* int to long vA, vB */
    movzbq  rINSTbl, %rax                   # rax <- +A
    sarl    $4, %eax                       # eax <- B
    andb    $0xf, rINSTbl                  # rINST <- A
    movslq  VREG_ADDRESS(%rax), %rax
    SET_WIDE_VREG %rax, rINSTq              # v[A] <- %rax
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_int_to_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_int_to_float: /* 0x82 */
    ENTRY nterp_op_int_to_float

/*
 * Generic 32-bit FP conversion operation.
 */
    /* unop vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # ecx <- A
    cvtsi2ssl    VREG_ADDRESS(rINSTq), %xmm0
    .if 0
    SET_VREG_XMMd %xmm0, %rcx
    CLEAR_WIDE_REF %rcx
    .else
    SET_VREG_XMMs %xmm0, %rcx
    CLEAR_REF %rcx
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_int_to_float

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_int_to_double: /* 0x83 */
    ENTRY nterp_op_int_to_double

/*
 * Generic 32-bit FP conversion operation.
 */
    /* unop vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # ecx <- A
    cvtsi2sdl    VREG_ADDRESS(rINSTq), %xmm0
    .if 1
    SET_VREG_XMMd %xmm0, %rcx
    CLEAR_WIDE_REF %rcx
    .else
    SET_VREG_XMMs %xmm0, %rcx
    CLEAR_REF %rcx
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_int_to_double

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_long_to_int: /* 0x84 */
    ENTRY nterp_op_long_to_int

/* we ignore the high word, making this equivalent to a 32-bit reg move */
    /* for move, move-object, long-to-int */
    /* op vA, vB */
    movl    rINST, %eax                     # eax <- BA
    andb    $0xf, %al                      # eax <- A
    shrl    $4, rINST                      # rINST <- B
    GET_VREG %edx, rINSTq
    .if 0
    SET_VREG_OBJECT %edx, %rax              # fp[A] <- fp[B]
    .else
    SET_VREG %edx, %rax                     # fp[A] <- fp[B]
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_long_to_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_long_to_float: /* 0x85 */
    ENTRY nterp_op_long_to_float

/*
 * Generic 32-bit FP conversion operation.
 */
    /* unop vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # ecx <- A
    cvtsi2ssq    VREG_ADDRESS(rINSTq), %xmm0
    .if 0
    SET_VREG_XMMd %xmm0, %rcx
    CLEAR_WIDE_REF %rcx
    .else
    SET_VREG_XMMs %xmm0, %rcx
    CLEAR_REF %rcx
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_long_to_float

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_long_to_double: /* 0x86 */
    ENTRY nterp_op_long_to_double

/*
 * Generic 32-bit FP conversion operation.
 */
    /* unop vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # ecx <- A
    cvtsi2sdq    VREG_ADDRESS(rINSTq), %xmm0
    .if 1
    SET_VREG_XMMd %xmm0, %rcx
    CLEAR_WIDE_REF %rcx
    .else
    SET_VREG_XMMs %xmm0, %rcx
    CLEAR_REF %rcx
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_long_to_double

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_float_to_int: /* 0x87 */
    ENTRY nterp_op_float_to_int

/* On fp to int conversions, Java requires that
 * if the result > maxint, it should be clamped to maxint.  If it is less
 * than minint, it should be clamped to minint.  If it is a nan, the result
 * should be zero.  Further, the rounding mode is to truncate.
 */
    /* float/double to int/long vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # ecx <- A
    GET_VREG_XMMs %xmm0, rINSTq
    movl  $0x7fffffff, %eax
    cvtsi2ssl %eax, %xmm1
    comiss    %xmm1, %xmm0
    jae     1f
    jp      2f
    cvttss2sil  %xmm0, %eax
    jmp     1f
2:
    xorl    %eax, %eax
1:
    .if 0
    SET_WIDE_VREG %eax, %rcx
    .else
    SET_VREG %eax, %rcx
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_float_to_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_float_to_long: /* 0x88 */
    ENTRY nterp_op_float_to_long

/* On fp to int conversions, Java requires that
 * if the result > maxint, it should be clamped to maxint.  If it is less
 * than minint, it should be clamped to minint.  If it is a nan, the result
 * should be zero.  Further, the rounding mode is to truncate.
 */
    /* float/double to int/long vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # ecx <- A
    GET_VREG_XMMs %xmm0, rINSTq
    movq  $0x7fffffffffffffff, %rax
    cvtsi2ssq %rax, %xmm1
    comiss    %xmm1, %xmm0
    jae     1f
    jp      2f
    cvttss2siq  %xmm0, %rax
    jmp     1f
2:
    xorq    %rax, %rax
1:
    .if 1
    SET_WIDE_VREG %rax, %rcx
    .else
    SET_VREG %rax, %rcx
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_float_to_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_float_to_double: /* 0x89 */
    ENTRY nterp_op_float_to_double

/*
 * Generic 32-bit FP conversion operation.
 */
    /* unop vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # ecx <- A
    cvtss2sd    VREG_ADDRESS(rINSTq), %xmm0
    .if 1
    SET_VREG_XMMd %xmm0, %rcx
    CLEAR_WIDE_REF %rcx
    .else
    SET_VREG_XMMs %xmm0, %rcx
    CLEAR_REF %rcx
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_float_to_double

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_double_to_int: /* 0x8a */
    ENTRY nterp_op_double_to_int

/* On fp to int conversions, Java requires that
 * if the result > maxint, it should be clamped to maxint.  If it is less
 * than minint, it should be clamped to minint.  If it is a nan, the result
 * should be zero.  Further, the rounding mode is to truncate.
 */
    /* float/double to int/long vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # ecx <- A
    GET_VREG_XMMd %xmm0, rINSTq
    movl  $0x7fffffff, %eax
    cvtsi2sdl %eax, %xmm1
    comisd    %xmm1, %xmm0
    jae     1f
    jp      2f
    cvttsd2sil  %xmm0, %eax
    jmp     1f
2:
    xorl    %eax, %eax
1:
    .if 0
    SET_WIDE_VREG %eax, %rcx
    .else
    SET_VREG %eax, %rcx
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_double_to_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_double_to_long: /* 0x8b */
    ENTRY nterp_op_double_to_long

/* On fp to int conversions, Java requires that
 * if the result > maxint, it should be clamped to maxint.  If it is less
 * than minint, it should be clamped to minint.  If it is a nan, the result
 * should be zero.  Further, the rounding mode is to truncate.
 */
    /* float/double to int/long vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # ecx <- A
    GET_VREG_XMMd %xmm0, rINSTq
    movq  $0x7fffffffffffffff, %rax
    cvtsi2sdq %rax, %xmm1
    comisd    %xmm1, %xmm0
    jae     1f
    jp      2f
    cvttsd2siq  %xmm0, %rax
    jmp     1f
2:
    xorq    %rax, %rax
1:
    .if 1
    SET_WIDE_VREG %rax, %rcx
    .else
    SET_VREG %rax, %rcx
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_double_to_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_double_to_float: /* 0x8c */
    ENTRY nterp_op_double_to_float

/*
 * Generic 32-bit FP conversion operation.
 */
    /* unop vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # ecx <- A
    cvtsd2ss    VREG_ADDRESS(rINSTq), %xmm0
    .if 0
    SET_VREG_XMMd %xmm0, %rcx
    CLEAR_WIDE_REF %rcx
    .else
    SET_VREG_XMMs %xmm0, %rcx
    CLEAR_REF %rcx
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_double_to_float

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_int_to_byte: /* 0x8d */
    ENTRY nterp_op_int_to_byte

/*
 * Generic 32/64-bit unary operation.  Provide an "instr" line that
 * specifies an instruction that performs "result = op eax".
 */
    /* unop vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4,rINST                       # rINST <- B
    .if 0
    GET_WIDE_VREG %rax, rINSTq              # rax <- vB
    .else
    GET_VREG %eax, rINSTq                   # eax <- vB
    .endif
    andb    $0xf,%cl                       # ecx <- A

movsbl  %al, %eax
    .if 0
    SET_WIDE_VREG %rax, %rcx
    .else
    SET_VREG %eax, %rcx
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_int_to_byte

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_int_to_char: /* 0x8e */
    ENTRY nterp_op_int_to_char

/*
 * Generic 32/64-bit unary operation.  Provide an "instr" line that
 * specifies an instruction that performs "result = op eax".
 */
    /* unop vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4,rINST                       # rINST <- B
    .if 0
    GET_WIDE_VREG %rax, rINSTq              # rax <- vB
    .else
    GET_VREG %eax, rINSTq                   # eax <- vB
    .endif
    andb    $0xf,%cl                       # ecx <- A

movzwl  %ax,%eax
    .if 0
    SET_WIDE_VREG %rax, %rcx
    .else
    SET_VREG %eax, %rcx
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_int_to_char

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_int_to_short: /* 0x8f */
    ENTRY nterp_op_int_to_short

/*
 * Generic 32/64-bit unary operation.  Provide an "instr" line that
 * specifies an instruction that performs "result = op eax".
 */
    /* unop vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4,rINST                       # rINST <- B
    .if 0
    GET_WIDE_VREG %rax, rINSTq              # rax <- vB
    .else
    GET_VREG %eax, rINSTq                   # eax <- vB
    .endif
    andb    $0xf,%cl                       # ecx <- A

movswl %ax, %eax
    .if 0
    SET_WIDE_VREG %rax, %rcx
    .else
    SET_VREG %eax, %rcx
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_int_to_short

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_add_int: /* 0x90 */
    ENTRY nterp_op_add_int

/*
 * Generic 32-bit binary operation.  Provide an "instr" line that
 * specifies an instruction that performs "result = eax op (rFP,%ecx,4)".
 * This could be an x86 instruction or a function call.  (If the result
 * comes back in a register other than eax, you can override "result".)
 *
 * For: add-int, sub-int, and-int, or-int,
 *      xor-int, shl-int, shr-int, ushr-int
 */
    /* binop vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # rax <- BB
    movzbq  3(rPC), %rcx                    # rcx <- CC
    GET_VREG %eax, %rax                     # eax <- vBB
    addl VREG_ADDRESS(%rcx),%eax
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_add_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sub_int: /* 0x91 */
    ENTRY nterp_op_sub_int

/*
 * Generic 32-bit binary operation.  Provide an "instr" line that
 * specifies an instruction that performs "result = eax op (rFP,%ecx,4)".
 * This could be an x86 instruction or a function call.  (If the result
 * comes back in a register other than eax, you can override "result".)
 *
 * For: add-int, sub-int, and-int, or-int,
 *      xor-int, shl-int, shr-int, ushr-int
 */
    /* binop vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # rax <- BB
    movzbq  3(rPC), %rcx                    # rcx <- CC
    GET_VREG %eax, %rax                     # eax <- vBB
    subl VREG_ADDRESS(%rcx),%eax
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_sub_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_mul_int: /* 0x92 */
    ENTRY nterp_op_mul_int

/*
 * Generic 32-bit binary operation.  Provide an "instr" line that
 * specifies an instruction that performs "result = eax op (rFP,%ecx,4)".
 * This could be an x86 instruction or a function call.  (If the result
 * comes back in a register other than eax, you can override "result".)
 *
 * For: add-int, sub-int, and-int, or-int,
 *      xor-int, shl-int, shr-int, ushr-int
 */
    /* binop vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # rax <- BB
    movzbq  3(rPC), %rcx                    # rcx <- CC
    GET_VREG %eax, %rax                     # eax <- vBB
    imull VREG_ADDRESS(%rcx),%eax
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_mul_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_div_int: /* 0x93 */
    ENTRY nterp_op_div_int

/*
 * 32-bit binary div/rem operation.  Handles special case of op1=-1.
 */
    /* div/rem vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # rax <- BB
    movzbq  3(rPC), %rcx                    # rcx <- CC
    .if 0
    GET_WIDE_VREG %rax, %rax                # eax <- vBB
    GET_WIDE_VREG %ecx, %rcx             # ecx <- vCC
    .else
    GET_VREG %eax, %rax                     # eax <- vBB
    GET_VREG %ecx, %rcx                  # ecx <- vCC
    .endif
    testl   %ecx, %ecx
    jz      common_errDivideByZero
    cmpl  $-1, %ecx
    je      2f
    cmpl  $2, %ecx
    je 3f
    cdq                                    # rdx:rax <- sign-extended of rax
    idivl   %ecx
1:
    .if 0
    SET_WIDE_VREG %eax, rINSTq           # eax <- vBB
    .else
    SET_VREG %eax, rINSTq                # eax <- vBB
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
2:
    .if 0
    xorl %eax, %eax
    .else
    negl %eax
    .endif
    jmp     1b
3:
    .if 0
    movl %edx, %eax
    .if 0
    shrl $63, %eax
    .else
    shrl $31, %eax
    .endif
    addl %edx, %eax
    andl $-2, %eax
    subl %eax, %edx
    movl %edx, %eax
    .else
    movl %eax, %edx
    .if 0
    shrl $63, %edx
    .else
    shrl $31, %edx
    .endif
    addl %edx, %eax
    sarl %eax
    .endif
    jmp     1b

    END nterp_op_div_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_rem_int: /* 0x94 */
    ENTRY nterp_op_rem_int

/*
 * 32-bit binary div/rem operation.  Handles special case of op1=-1.
 */
    /* div/rem vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # rax <- BB
    movzbq  3(rPC), %rcx                    # rcx <- CC
    .if 0
    GET_WIDE_VREG %rax, %rax                # eax <- vBB
    GET_WIDE_VREG %ecx, %rcx             # ecx <- vCC
    .else
    GET_VREG %eax, %rax                     # eax <- vBB
    GET_VREG %ecx, %rcx                  # ecx <- vCC
    .endif
    testl   %ecx, %ecx
    jz      common_errDivideByZero
    cmpl  $-1, %ecx
    je      2f
    cmpl  $2, %ecx
    je 3f
    cdq                                    # rdx:rax <- sign-extended of rax
    idivl   %ecx
1:
    .if 0
    SET_WIDE_VREG %edx, rINSTq           # eax <- vBB
    .else
    SET_VREG %edx, rINSTq                # eax <- vBB
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
2:
    .if 1
    xorl %edx, %edx
    .else
    negl %edx
    .endif
    jmp     1b
3:
    .if 1
    movl %eax, %edx
    .if 0
    shrl $63, %edx
    .else
    shrl $31, %edx
    .endif
    addl %eax, %edx
    andl $-2, %edx
    subl %edx, %eax
    movl %eax, %edx
    .else
    movl %edx, %eax
    .if 0
    shrl $63, %eax
    .else
    shrl $31, %eax
    .endif
    addl %eax, %edx
    sarl %edx
    .endif
    jmp     1b

    END nterp_op_rem_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_and_int: /* 0x95 */
    ENTRY nterp_op_and_int

/*
 * Generic 32-bit binary operation.  Provide an "instr" line that
 * specifies an instruction that performs "result = eax op (rFP,%ecx,4)".
 * This could be an x86 instruction or a function call.  (If the result
 * comes back in a register other than eax, you can override "result".)
 *
 * For: add-int, sub-int, and-int, or-int,
 *      xor-int, shl-int, shr-int, ushr-int
 */
    /* binop vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # rax <- BB
    movzbq  3(rPC), %rcx                    # rcx <- CC
    GET_VREG %eax, %rax                     # eax <- vBB
    andl VREG_ADDRESS(%rcx),%eax
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_and_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_or_int: /* 0x96 */
    ENTRY nterp_op_or_int

/*
 * Generic 32-bit binary operation.  Provide an "instr" line that
 * specifies an instruction that performs "result = eax op (rFP,%ecx,4)".
 * This could be an x86 instruction or a function call.  (If the result
 * comes back in a register other than eax, you can override "result".)
 *
 * For: add-int, sub-int, and-int, or-int,
 *      xor-int, shl-int, shr-int, ushr-int
 */
    /* binop vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # rax <- BB
    movzbq  3(rPC), %rcx                    # rcx <- CC
    GET_VREG %eax, %rax                     # eax <- vBB
    orl VREG_ADDRESS(%rcx),%eax
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_or_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_xor_int: /* 0x97 */
    ENTRY nterp_op_xor_int

/*
 * Generic 32-bit binary operation.  Provide an "instr" line that
 * specifies an instruction that performs "result = eax op (rFP,%ecx,4)".
 * This could be an x86 instruction or a function call.  (If the result
 * comes back in a register other than eax, you can override "result".)
 *
 * For: add-int, sub-int, and-int, or-int,
 *      xor-int, shl-int, shr-int, ushr-int
 */
    /* binop vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # rax <- BB
    movzbq  3(rPC), %rcx                    # rcx <- CC
    GET_VREG %eax, %rax                     # eax <- vBB
    xorl VREG_ADDRESS(%rcx),%eax
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_xor_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_shl_int: /* 0x98 */
    ENTRY nterp_op_shl_int

/*
 * Generic 32-bit binary operation in which both operands loaded to
 * registers (op0 in eax, op1 in ecx).
 */
    /* binop vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # eax <- BB
    movzbq  3(rPC), %rcx                    # ecx <- CC
    GET_VREG %ecx, %rcx                     # eax <- vCC
    .if 0
    GET_WIDE_VREG %rax, %rax                # rax <- vBB
    sall    %cl, %eax                                  # ex: addl    %ecx,%eax
    SET_WIDE_VREG %rax, rINSTq
    .else
    GET_VREG %eax, %rax                     # eax <- vBB
    sall    %cl, %eax                                  # ex: addl    %ecx,%eax
    SET_VREG %eax, rINSTq
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_shl_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_shr_int: /* 0x99 */
    ENTRY nterp_op_shr_int

/*
 * Generic 32-bit binary operation in which both operands loaded to
 * registers (op0 in eax, op1 in ecx).
 */
    /* binop vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # eax <- BB
    movzbq  3(rPC), %rcx                    # ecx <- CC
    GET_VREG %ecx, %rcx                     # eax <- vCC
    .if 0
    GET_WIDE_VREG %rax, %rax                # rax <- vBB
    sarl    %cl, %eax                                  # ex: addl    %ecx,%eax
    SET_WIDE_VREG %rax, rINSTq
    .else
    GET_VREG %eax, %rax                     # eax <- vBB
    sarl    %cl, %eax                                  # ex: addl    %ecx,%eax
    SET_VREG %eax, rINSTq
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_shr_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_ushr_int: /* 0x9a */
    ENTRY nterp_op_ushr_int

/*
 * Generic 32-bit binary operation in which both operands loaded to
 * registers (op0 in eax, op1 in ecx).
 */
    /* binop vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # eax <- BB
    movzbq  3(rPC), %rcx                    # ecx <- CC
    GET_VREG %ecx, %rcx                     # eax <- vCC
    .if 0
    GET_WIDE_VREG %rax, %rax                # rax <- vBB
    shrl    %cl, %eax                                  # ex: addl    %ecx,%eax
    SET_WIDE_VREG %rax, rINSTq
    .else
    GET_VREG %eax, %rax                     # eax <- vBB
    shrl    %cl, %eax                                  # ex: addl    %ecx,%eax
    SET_VREG %eax, rINSTq
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_ushr_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_add_long: /* 0x9b */
    ENTRY nterp_op_add_long

/*
 * Generic 64-bit binary operation.
 */
    /* binop vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # eax <- BB
    movzbq  3(rPC), %rcx                    # ecx <- CC
    GET_WIDE_VREG %rax, %rax                # rax <- v[BB]
    addq VREG_ADDRESS(%rcx),%rax
    SET_WIDE_VREG %rax, rINSTq              # v[AA] <- rax
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_add_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sub_long: /* 0x9c */
    ENTRY nterp_op_sub_long

/*
 * Generic 64-bit binary operation.
 */
    /* binop vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # eax <- BB
    movzbq  3(rPC), %rcx                    # ecx <- CC
    GET_WIDE_VREG %rax, %rax                # rax <- v[BB]
    subq VREG_ADDRESS(%rcx),%rax
    SET_WIDE_VREG %rax, rINSTq              # v[AA] <- rax
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_sub_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_mul_long: /* 0x9d */
    ENTRY nterp_op_mul_long

/*
 * Generic 64-bit binary operation.
 */
    /* binop vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # eax <- BB
    movzbq  3(rPC), %rcx                    # ecx <- CC
    GET_WIDE_VREG %rax, %rax                # rax <- v[BB]
    imulq VREG_ADDRESS(%rcx),%rax
    SET_WIDE_VREG %rax, rINSTq              # v[AA] <- rax
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_mul_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_div_long: /* 0x9e */
    ENTRY nterp_op_div_long

/*
 * 32-bit binary div/rem operation.  Handles special case of op1=-1.
 */
    /* div/rem vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # rax <- BB
    movzbq  3(rPC), %rcx                    # rcx <- CC
    .if 1
    GET_WIDE_VREG %rax, %rax                # eax <- vBB
    GET_WIDE_VREG %rcx, %rcx             # ecx <- vCC
    .else
    GET_VREG %eax, %rax                     # eax <- vBB
    GET_VREG %rcx, %rcx                  # ecx <- vCC
    .endif
    testq   %rcx, %rcx
    jz      common_errDivideByZero
    cmpq  $-1, %rcx
    je      2f
    cmpq  $2, %rcx
    je 3f
    cqo                                    # rdx:rax <- sign-extended of rax
    idivq   %rcx
1:
    .if 1
    SET_WIDE_VREG %rax, rINSTq           # eax <- vBB
    .else
    SET_VREG %rax, rINSTq                # eax <- vBB
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
2:
    .if 0
    xorq %rax, %rax
    .else
    negq %rax
    .endif
    jmp     1b
3:
    .if 0
    movq %rdx, %rax
    .if 1
    shrq $63, %rax
    .else
    shrq $31, %rax
    .endif
    addq %rdx, %rax
    andq $-2, %rax
    subq %rax, %rdx
    movq %rdx, %rax
    .else
    movq %rax, %rdx
    .if 1
    shrq $63, %rdx
    .else
    shrq $31, %rdx
    .endif
    addq %rdx, %rax
    sarq %rax
    .endif
    jmp     1b

    END nterp_op_div_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_rem_long: /* 0x9f */
    ENTRY nterp_op_rem_long

/*
 * 32-bit binary div/rem operation.  Handles special case of op1=-1.
 */
    /* div/rem vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # rax <- BB
    movzbq  3(rPC), %rcx                    # rcx <- CC
    .if 1
    GET_WIDE_VREG %rax, %rax                # eax <- vBB
    GET_WIDE_VREG %rcx, %rcx             # ecx <- vCC
    .else
    GET_VREG %eax, %rax                     # eax <- vBB
    GET_VREG %rcx, %rcx                  # ecx <- vCC
    .endif
    testq   %rcx, %rcx
    jz      common_errDivideByZero
    cmpq  $-1, %rcx
    je      2f
    cmpq  $2, %rcx
    je 3f
    cqo                                    # rdx:rax <- sign-extended of rax
    idivq   %rcx
1:
    .if 1
    SET_WIDE_VREG %rdx, rINSTq           # eax <- vBB
    .else
    SET_VREG %rdx, rINSTq                # eax <- vBB
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
2:
    .if 1
    xorq %rdx, %rdx
    .else
    negq %rdx
    .endif
    jmp     1b
3:
    .if 1
    movq %rax, %rdx
    .if 1
    shrq $63, %rdx
    .else
    shrq $31, %rdx
    .endif
    addq %rax, %rdx
    andq $-2, %rdx
    subq %rdx, %rax
    movq %rax, %rdx
    .else
    movq %rdx, %rax
    .if 1
    shrq $63, %rax
    .else
    shrq $31, %rax
    .endif
    addq %rax, %rdx
    sarq %rdx
    .endif
    jmp     1b

    END nterp_op_rem_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_and_long: /* 0xa0 */
    ENTRY nterp_op_and_long

/*
 * Generic 64-bit binary operation.
 */
    /* binop vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # eax <- BB
    movzbq  3(rPC), %rcx                    # ecx <- CC
    GET_WIDE_VREG %rax, %rax                # rax <- v[BB]
    andq VREG_ADDRESS(%rcx),%rax
    SET_WIDE_VREG %rax, rINSTq              # v[AA] <- rax
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_and_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_or_long: /* 0xa1 */
    ENTRY nterp_op_or_long

/*
 * Generic 64-bit binary operation.
 */
    /* binop vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # eax <- BB
    movzbq  3(rPC), %rcx                    # ecx <- CC
    GET_WIDE_VREG %rax, %rax                # rax <- v[BB]
    orq VREG_ADDRESS(%rcx),%rax
    SET_WIDE_VREG %rax, rINSTq              # v[AA] <- rax
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_or_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_xor_long: /* 0xa2 */
    ENTRY nterp_op_xor_long

/*
 * Generic 64-bit binary operation.
 */
    /* binop vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # eax <- BB
    movzbq  3(rPC), %rcx                    # ecx <- CC
    GET_WIDE_VREG %rax, %rax                # rax <- v[BB]
    xorq VREG_ADDRESS(%rcx),%rax
    SET_WIDE_VREG %rax, rINSTq              # v[AA] <- rax
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_xor_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_shl_long: /* 0xa3 */
    ENTRY nterp_op_shl_long

/*
 * Generic 32-bit binary operation in which both operands loaded to
 * registers (op0 in eax, op1 in ecx).
 */
    /* binop vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # eax <- BB
    movzbq  3(rPC), %rcx                    # ecx <- CC
    GET_VREG %ecx, %rcx                     # eax <- vCC
    .if 1
    GET_WIDE_VREG %rax, %rax                # rax <- vBB
    salq    %cl, %rax                                  # ex: addl    %ecx,%eax
    SET_WIDE_VREG %rax, rINSTq
    .else
    GET_VREG %eax, %rax                     # eax <- vBB
    salq    %cl, %rax                                  # ex: addl    %ecx,%eax
    SET_VREG %eax, rINSTq
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_shl_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_shr_long: /* 0xa4 */
    ENTRY nterp_op_shr_long

/*
 * Generic 32-bit binary operation in which both operands loaded to
 * registers (op0 in eax, op1 in ecx).
 */
    /* binop vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # eax <- BB
    movzbq  3(rPC), %rcx                    # ecx <- CC
    GET_VREG %ecx, %rcx                     # eax <- vCC
    .if 1
    GET_WIDE_VREG %rax, %rax                # rax <- vBB
    sarq    %cl, %rax                                  # ex: addl    %ecx,%eax
    SET_WIDE_VREG %rax, rINSTq
    .else
    GET_VREG %eax, %rax                     # eax <- vBB
    sarq    %cl, %rax                                  # ex: addl    %ecx,%eax
    SET_VREG %eax, rINSTq
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_shr_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_ushr_long: /* 0xa5 */
    ENTRY nterp_op_ushr_long

/*
 * Generic 32-bit binary operation in which both operands loaded to
 * registers (op0 in eax, op1 in ecx).
 */
    /* binop vAA, vBB, vCC */
    movzbq  2(rPC), %rax                    # eax <- BB
    movzbq  3(rPC), %rcx                    # ecx <- CC
    GET_VREG %ecx, %rcx                     # eax <- vCC
    .if 1
    GET_WIDE_VREG %rax, %rax                # rax <- vBB
    shrq    %cl, %rax                                  # ex: addl    %ecx,%eax
    SET_WIDE_VREG %rax, rINSTq
    .else
    GET_VREG %eax, %rax                     # eax <- vBB
    shrq    %cl, %rax                                  # ex: addl    %ecx,%eax
    SET_VREG %eax, rINSTq
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_ushr_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_add_float: /* 0xa6 */
    ENTRY nterp_op_add_float

    movzbq  2(rPC), %rcx                    # ecx <- BB
    movzbq  3(rPC), %rax                    # eax <- CC
    GET_VREG_XMMs %xmm0, %rcx         # %xmm0 <- 1st src
#ifdef MTERP_USE_AVX
    vaddss VREG_ADDRESS(%rax), %xmm0, %xmm0
    SET_VREG_XMMs %xmm0, rINSTq       # vAA <- %xmm0
    vpxor    %xmm0, %xmm0, %xmm0
    vmovss   %xmm0, VREG_REF_ADDRESS(rINSTq) # clear ref
#else
    addss VREG_ADDRESS(%rax), %xmm0
    SET_VREG_XMMs %xmm0, rINSTq       # vAA <- %xmm0
    pxor    %xmm0, %xmm0
    movss   %xmm0, VREG_REF_ADDRESS(rINSTq) # clear ref
#endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_add_float

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sub_float: /* 0xa7 */
    ENTRY nterp_op_sub_float

    movzbq  2(rPC), %rcx                    # ecx <- BB
    movzbq  3(rPC), %rax                    # eax <- CC
    GET_VREG_XMMs %xmm0, %rcx         # %xmm0 <- 1st src
#ifdef MTERP_USE_AVX
    vsubss VREG_ADDRESS(%rax), %xmm0, %xmm0
    SET_VREG_XMMs %xmm0, rINSTq       # vAA <- %xmm0
    vpxor    %xmm0, %xmm0, %xmm0
    vmovss   %xmm0, VREG_REF_ADDRESS(rINSTq) # clear ref
#else
    subss VREG_ADDRESS(%rax), %xmm0
    SET_VREG_XMMs %xmm0, rINSTq       # vAA <- %xmm0
    pxor    %xmm0, %xmm0
    movss   %xmm0, VREG_REF_ADDRESS(rINSTq) # clear ref
#endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_sub_float

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_mul_float: /* 0xa8 */
    ENTRY nterp_op_mul_float

    movzbq  2(rPC), %rcx                    # ecx <- BB
    movzbq  3(rPC), %rax                    # eax <- CC
    GET_VREG_XMMs %xmm0, %rcx         # %xmm0 <- 1st src
#ifdef MTERP_USE_AVX
    vmulss VREG_ADDRESS(%rax), %xmm0, %xmm0
    SET_VREG_XMMs %xmm0, rINSTq       # vAA <- %xmm0
    vpxor    %xmm0, %xmm0, %xmm0
    vmovss   %xmm0, VREG_REF_ADDRESS(rINSTq) # clear ref
#else
    mulss VREG_ADDRESS(%rax), %xmm0
    SET_VREG_XMMs %xmm0, rINSTq       # vAA <- %xmm0
    pxor    %xmm0, %xmm0
    movss   %xmm0, VREG_REF_ADDRESS(rINSTq) # clear ref
#endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_mul_float

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_div_float: /* 0xa9 */
    ENTRY nterp_op_div_float

    movzbq  2(rPC), %rcx                    # ecx <- BB
    movzbq  3(rPC), %rax                    # eax <- CC
    GET_VREG_XMMs %xmm0, %rcx         # %xmm0 <- 1st src
#ifdef MTERP_USE_AVX
    vdivss VREG_ADDRESS(%rax), %xmm0, %xmm0
    SET_VREG_XMMs %xmm0, rINSTq       # vAA <- %xmm0
    vpxor    %xmm0, %xmm0, %xmm0
    vmovss   %xmm0, VREG_REF_ADDRESS(rINSTq) # clear ref
#else
    divss VREG_ADDRESS(%rax), %xmm0
    SET_VREG_XMMs %xmm0, rINSTq       # vAA <- %xmm0
    pxor    %xmm0, %xmm0
    movss   %xmm0, VREG_REF_ADDRESS(rINSTq) # clear ref
#endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_div_float

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_rem_float: /* 0xaa */
    ENTRY nterp_op_rem_float

    /* rem_float vAA, vBB, vCC */
    movzbq  3(rPC), %rcx                    # ecx <- BB
    movzbq  2(rPC), %rax                    # eax <- CC
    flds    VREG_ADDRESS(%rcx)              # vBB to fp stack
    flds    VREG_ADDRESS(%rax)              # vCC to fp stack
1:
    fprem
    fstsw   %ax
    sahf
    jp      1b
    fstp    %st(1)
    fstps   VREG_ADDRESS(rINSTq)            # %st to vAA
    CLEAR_REF rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_rem_float

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_add_double: /* 0xab */
    ENTRY nterp_op_add_double

    movzbq  2(rPC), %rcx                    # ecx <- BB
    movzbq  3(rPC), %rax                    # eax <- CC
    GET_VREG_XMMd %xmm0, %rcx         # %xmm0 <- 1st src
#ifdef MTERP_USE_AVX
    vaddsd VREG_ADDRESS(%rax), %xmm0, %xmm0
    SET_VREG_XMMd %xmm0, rINSTq       # vAA <- %xmm0
    vpxor    %xmm0, %xmm0, %xmm0
    vmovsd   %xmm0, VREG_REF_ADDRESS(rINSTq) # clear ref
#else
    addsd VREG_ADDRESS(%rax), %xmm0
    SET_VREG_XMMd %xmm0, rINSTq       # vAA <- %xmm0
    pxor    %xmm0, %xmm0
    movsd   %xmm0, VREG_REF_ADDRESS(rINSTq) # clear ref
#endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_add_double

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sub_double: /* 0xac */
    ENTRY nterp_op_sub_double

    movzbq  2(rPC), %rcx                    # ecx <- BB
    movzbq  3(rPC), %rax                    # eax <- CC
    GET_VREG_XMMd %xmm0, %rcx         # %xmm0 <- 1st src
#ifdef MTERP_USE_AVX
    vsubsd VREG_ADDRESS(%rax), %xmm0, %xmm0
    SET_VREG_XMMd %xmm0, rINSTq       # vAA <- %xmm0
    vpxor    %xmm0, %xmm0, %xmm0
    vmovsd   %xmm0, VREG_REF_ADDRESS(rINSTq) # clear ref
#else
    subsd VREG_ADDRESS(%rax), %xmm0
    SET_VREG_XMMd %xmm0, rINSTq       # vAA <- %xmm0
    pxor    %xmm0, %xmm0
    movsd   %xmm0, VREG_REF_ADDRESS(rINSTq) # clear ref
#endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_sub_double

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_mul_double: /* 0xad */
    ENTRY nterp_op_mul_double

    movzbq  2(rPC), %rcx                    # ecx <- BB
    movzbq  3(rPC), %rax                    # eax <- CC
    GET_VREG_XMMd %xmm0, %rcx         # %xmm0 <- 1st src
#ifdef MTERP_USE_AVX
    vmulsd VREG_ADDRESS(%rax), %xmm0, %xmm0
    SET_VREG_XMMd %xmm0, rINSTq       # vAA <- %xmm0
    vpxor    %xmm0, %xmm0, %xmm0
    vmovsd   %xmm0, VREG_REF_ADDRESS(rINSTq) # clear ref
#else
    mulsd VREG_ADDRESS(%rax), %xmm0
    SET_VREG_XMMd %xmm0, rINSTq       # vAA <- %xmm0
    pxor    %xmm0, %xmm0
    movsd   %xmm0, VREG_REF_ADDRESS(rINSTq) # clear ref
#endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_mul_double

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_div_double: /* 0xae */
    ENTRY nterp_op_div_double

    movzbq  2(rPC), %rcx                    # ecx <- BB
    movzbq  3(rPC), %rax                    # eax <- CC
    GET_VREG_XMMd %xmm0, %rcx         # %xmm0 <- 1st src
#ifdef MTERP_USE_AVX
    vdivsd VREG_ADDRESS(%rax), %xmm0, %xmm0
    SET_VREG_XMMd %xmm0, rINSTq       # vAA <- %xmm0
    vpxor    %xmm0, %xmm0, %xmm0
    vmovsd   %xmm0, VREG_REF_ADDRESS(rINSTq) # clear ref
#else
    divsd VREG_ADDRESS(%rax), %xmm0
    SET_VREG_XMMd %xmm0, rINSTq       # vAA <- %xmm0
    pxor    %xmm0, %xmm0
    movsd   %xmm0, VREG_REF_ADDRESS(rINSTq) # clear ref
#endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_div_double

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_rem_double: /* 0xaf */
    ENTRY nterp_op_rem_double

    /* rem_double vAA, vBB, vCC */
    movzbq  3(rPC), %rcx                    # ecx <- BB
    movzbq  2(rPC), %rax                    # eax <- CC
    fldl    VREG_ADDRESS(%rcx)              # %st1 <- fp[vBB]
    fldl    VREG_ADDRESS(%rax)              # %st0 <- fp[vCC]
1:
    fprem
    fstsw   %ax
    sahf
    jp      1b
    fstp    %st(1)
    fstpl   VREG_ADDRESS(rINSTq)            # fp[vAA] <- %st
    CLEAR_WIDE_REF rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_rem_double

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_add_int_2addr: /* 0xb0 */
    ENTRY nterp_op_add_int_2addr

/*
 * Generic 32-bit "/2addr" binary operation.  Provide an "instr" line
 * that specifies an instruction that performs "result = r0 op r1".
 * This could be an instruction or a function call.
 *
 * For: add-int/2addr, sub-int/2addr, mul-int/2addr, div-int/2addr,
 *      rem-int/2addr, and-int/2addr, or-int/2addr, xor-int/2addr,
 *      shl-int/2addr, shr-int/2addr, ushr-int/2addr, add-float/2addr,
 *      sub-float/2addr, mul-float/2addr, div-float/2addr, rem-float/2addr
 */
    /* binop/2addr vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # ecx <- A
    GET_VREG %eax, rINSTq                   # eax <- vB
    addl %eax, VREG_ADDRESS(%rcx)
    CLEAR_REF %rcx
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_add_int_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sub_int_2addr: /* 0xb1 */
    ENTRY nterp_op_sub_int_2addr

/*
 * Generic 32-bit "/2addr" binary operation.  Provide an "instr" line
 * that specifies an instruction that performs "result = r0 op r1".
 * This could be an instruction or a function call.
 *
 * For: add-int/2addr, sub-int/2addr, mul-int/2addr, div-int/2addr,
 *      rem-int/2addr, and-int/2addr, or-int/2addr, xor-int/2addr,
 *      shl-int/2addr, shr-int/2addr, ushr-int/2addr, add-float/2addr,
 *      sub-float/2addr, mul-float/2addr, div-float/2addr, rem-float/2addr
 */
    /* binop/2addr vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # ecx <- A
    GET_VREG %eax, rINSTq                   # eax <- vB
    subl %eax, VREG_ADDRESS(%rcx)
    CLEAR_REF %rcx
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_sub_int_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_mul_int_2addr: /* 0xb2 */
    ENTRY nterp_op_mul_int_2addr

    /* mul vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # ecx <- A
    GET_VREG %eax, %rcx                     # eax <- vA
    imull   (rFP,rINSTq,4), %eax
    SET_VREG %eax, %rcx
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_mul_int_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_div_int_2addr: /* 0xb3 */
    ENTRY nterp_op_div_int_2addr

/*
 * 32-bit binary div/rem operation.  Handles special case of op1=-1.
 */
    /* div/rem/2addr vA, vB */
    movl    rINST, %ecx                     # rcx <- BA
    sarl    $4, %ecx                       # rcx <- B
    andb    $0xf, rINSTbl                  # rINST <- A
    .if 0
    GET_WIDE_VREG %rax, rINSTq              # eax <- vA
    GET_WIDE_VREG %ecx, %rcx             # ecx <- vB
    .else
    GET_VREG %eax, rINSTq                   # eax <- vA
    GET_VREG %ecx, %rcx                  # ecx <- vB
    .endif
    testl   %ecx, %ecx
    jz      common_errDivideByZero
    cmpl  $-1, %ecx
    je      2f
    cmpl  $2, %ecx
    je      3f
    cdq                                    # rdx:rax <- sign-extended of rax
    idivl   %ecx
1:
    .if 0
    SET_WIDE_VREG %eax, rINSTq           # vA <- result
    .else
    SET_VREG %eax, rINSTq                # vA <- result
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1
2:
    .if 0
    xorl %eax, %eax
    .else
    negl %eax
    .endif
    jmp     1b
3:
    .if 0
    movl %edx, %eax
    .if 0
    shrl $63, %eax
    .else
    shrl $31, %eax
    .endif
    addl %edx, %eax
    andl $-2, %eax
    subl %eax, %edx
    movl %edx, %eax
    .else
    movl %eax, %edx
    .if 0
    shrl $63, %edx
    .else
    shrl $31, %edx
    .endif
    addl %edx, %eax
    sarl %eax
    .endif
    jmp     1b

    END nterp_op_div_int_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_rem_int_2addr: /* 0xb4 */
    ENTRY nterp_op_rem_int_2addr

/*
 * 32-bit binary div/rem operation.  Handles special case of op1=-1.
 */
    /* div/rem/2addr vA, vB */
    movl    rINST, %ecx                     # rcx <- BA
    sarl    $4, %ecx                       # rcx <- B
    andb    $0xf, rINSTbl                  # rINST <- A
    .if 0
    GET_WIDE_VREG %rax, rINSTq              # eax <- vA
    GET_WIDE_VREG %ecx, %rcx             # ecx <- vB
    .else
    GET_VREG %eax, rINSTq                   # eax <- vA
    GET_VREG %ecx, %rcx                  # ecx <- vB
    .endif
    testl   %ecx, %ecx
    jz      common_errDivideByZero
    cmpl  $-1, %ecx
    je      2f
    cmpl  $2, %ecx
    je      3f
    cdq                                    # rdx:rax <- sign-extended of rax
    idivl   %ecx
1:
    .if 0
    SET_WIDE_VREG %edx, rINSTq           # vA <- result
    .else
    SET_VREG %edx, rINSTq                # vA <- result
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1
2:
    .if 1
    xorl %edx, %edx
    .else
    negl %edx
    .endif
    jmp     1b
3:
    .if 1
    movl %eax, %edx
    .if 0
    shrl $63, %edx
    .else
    shrl $31, %edx
    .endif
    addl %eax, %edx
    andl $-2, %edx
    subl %edx, %eax
    movl %eax, %edx
    .else
    movl %edx, %eax
    .if 0
    shrl $63, %eax
    .else
    shrl $31, %eax
    .endif
    addl %eax, %edx
    sarl %edx
    .endif
    jmp     1b

    END nterp_op_rem_int_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_and_int_2addr: /* 0xb5 */
    ENTRY nterp_op_and_int_2addr

/*
 * Generic 32-bit "/2addr" binary operation.  Provide an "instr" line
 * that specifies an instruction that performs "result = r0 op r1".
 * This could be an instruction or a function call.
 *
 * For: add-int/2addr, sub-int/2addr, mul-int/2addr, div-int/2addr,
 *      rem-int/2addr, and-int/2addr, or-int/2addr, xor-int/2addr,
 *      shl-int/2addr, shr-int/2addr, ushr-int/2addr, add-float/2addr,
 *      sub-float/2addr, mul-float/2addr, div-float/2addr, rem-float/2addr
 */
    /* binop/2addr vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # ecx <- A
    GET_VREG %eax, rINSTq                   # eax <- vB
    andl %eax, VREG_ADDRESS(%rcx)
    CLEAR_REF %rcx
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_and_int_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_or_int_2addr: /* 0xb6 */
    ENTRY nterp_op_or_int_2addr

/*
 * Generic 32-bit "/2addr" binary operation.  Provide an "instr" line
 * that specifies an instruction that performs "result = r0 op r1".
 * This could be an instruction or a function call.
 *
 * For: add-int/2addr, sub-int/2addr, mul-int/2addr, div-int/2addr,
 *      rem-int/2addr, and-int/2addr, or-int/2addr, xor-int/2addr,
 *      shl-int/2addr, shr-int/2addr, ushr-int/2addr, add-float/2addr,
 *      sub-float/2addr, mul-float/2addr, div-float/2addr, rem-float/2addr
 */
    /* binop/2addr vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # ecx <- A
    GET_VREG %eax, rINSTq                   # eax <- vB
    orl %eax, VREG_ADDRESS(%rcx)
    CLEAR_REF %rcx
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_or_int_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_xor_int_2addr: /* 0xb7 */
    ENTRY nterp_op_xor_int_2addr

/*
 * Generic 32-bit "/2addr" binary operation.  Provide an "instr" line
 * that specifies an instruction that performs "result = r0 op r1".
 * This could be an instruction or a function call.
 *
 * For: add-int/2addr, sub-int/2addr, mul-int/2addr, div-int/2addr,
 *      rem-int/2addr, and-int/2addr, or-int/2addr, xor-int/2addr,
 *      shl-int/2addr, shr-int/2addr, ushr-int/2addr, add-float/2addr,
 *      sub-float/2addr, mul-float/2addr, div-float/2addr, rem-float/2addr
 */
    /* binop/2addr vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # ecx <- A
    GET_VREG %eax, rINSTq                   # eax <- vB
    xorl %eax, VREG_ADDRESS(%rcx)
    CLEAR_REF %rcx
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_xor_int_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_shl_int_2addr: /* 0xb8 */
    ENTRY nterp_op_shl_int_2addr

/*
 * Generic 32-bit "shift/2addr" operation.
 */
    /* shift/2addr vA, vB */
    movl    rINST, %ecx                     # ecx <- BA
    sarl    $4, %ecx                       # ecx <- B
    GET_VREG %ecx, %rcx                     # ecx <- vBB
    andb    $0xf, rINSTbl                  # rINST <- A
    .if 0
    GET_WIDE_VREG %rax, rINSTq              # rax <- vAA
    sall    %cl, %eax                                  # ex: sarl %cl, %eax
    SET_WIDE_VREG %rax, rINSTq
    .else
    GET_VREG %eax, rINSTq                   # eax <- vAA
    sall    %cl, %eax                                  # ex: sarl %cl, %eax
    SET_VREG %eax, rINSTq
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_shl_int_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_shr_int_2addr: /* 0xb9 */
    ENTRY nterp_op_shr_int_2addr

/*
 * Generic 32-bit "shift/2addr" operation.
 */
    /* shift/2addr vA, vB */
    movl    rINST, %ecx                     # ecx <- BA
    sarl    $4, %ecx                       # ecx <- B
    GET_VREG %ecx, %rcx                     # ecx <- vBB
    andb    $0xf, rINSTbl                  # rINST <- A
    .if 0
    GET_WIDE_VREG %rax, rINSTq              # rax <- vAA
    sarl    %cl, %eax                                  # ex: sarl %cl, %eax
    SET_WIDE_VREG %rax, rINSTq
    .else
    GET_VREG %eax, rINSTq                   # eax <- vAA
    sarl    %cl, %eax                                  # ex: sarl %cl, %eax
    SET_VREG %eax, rINSTq
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_shr_int_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_ushr_int_2addr: /* 0xba */
    ENTRY nterp_op_ushr_int_2addr

/*
 * Generic 32-bit "shift/2addr" operation.
 */
    /* shift/2addr vA, vB */
    movl    rINST, %ecx                     # ecx <- BA
    sarl    $4, %ecx                       # ecx <- B
    GET_VREG %ecx, %rcx                     # ecx <- vBB
    andb    $0xf, rINSTbl                  # rINST <- A
    .if 0
    GET_WIDE_VREG %rax, rINSTq              # rax <- vAA
    shrl    %cl, %eax                                  # ex: sarl %cl, %eax
    SET_WIDE_VREG %rax, rINSTq
    .else
    GET_VREG %eax, rINSTq                   # eax <- vAA
    shrl    %cl, %eax                                  # ex: sarl %cl, %eax
    SET_VREG %eax, rINSTq
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_ushr_int_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_add_long_2addr: /* 0xbb */
    ENTRY nterp_op_add_long_2addr

/*
 * Generic 64-bit binary operation.
 */
    /* binop/2addr vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # ecx <- A
    GET_WIDE_VREG %rax, rINSTq              # rax <- vB
    addq %rax,VREG_ADDRESS(%rcx)
    CLEAR_WIDE_REF %rcx
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_add_long_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sub_long_2addr: /* 0xbc */
    ENTRY nterp_op_sub_long_2addr

/*
 * Generic 64-bit binary operation.
 */
    /* binop/2addr vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # ecx <- A
    GET_WIDE_VREG %rax, rINSTq              # rax <- vB
    subq %rax,VREG_ADDRESS(%rcx)
    CLEAR_WIDE_REF %rcx
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_sub_long_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_mul_long_2addr: /* 0xbd */
    ENTRY nterp_op_mul_long_2addr

    /* mul vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # ecx <- A
    GET_WIDE_VREG %rax, %rcx                # rax <- vA
    imulq   (rFP,rINSTq,4), %rax
    SET_WIDE_VREG %rax, %rcx
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_mul_long_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_div_long_2addr: /* 0xbe */
    ENTRY nterp_op_div_long_2addr

/*
 * 32-bit binary div/rem operation.  Handles special case of op1=-1.
 */
    /* div/rem/2addr vA, vB */
    movl    rINST, %ecx                     # rcx <- BA
    sarl    $4, %ecx                       # rcx <- B
    andb    $0xf, rINSTbl                  # rINST <- A
    .if 1
    GET_WIDE_VREG %rax, rINSTq              # eax <- vA
    GET_WIDE_VREG %rcx, %rcx             # ecx <- vB
    .else
    GET_VREG %eax, rINSTq                   # eax <- vA
    GET_VREG %rcx, %rcx                  # ecx <- vB
    .endif
    testq   %rcx, %rcx
    jz      common_errDivideByZero
    cmpq  $-1, %rcx
    je      2f
    cmpq  $2, %rcx
    je      3f
    cqo                                    # rdx:rax <- sign-extended of rax
    idivq   %rcx
1:
    .if 1
    SET_WIDE_VREG %rax, rINSTq           # vA <- result
    .else
    SET_VREG %rax, rINSTq                # vA <- result
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1
2:
    .if 0
    xorq %rax, %rax
    .else
    negq %rax
    .endif
    jmp     1b
3:
    .if 0
    movq %rdx, %rax
    .if 1
    shrq $63, %rax
    .else
    shrq $31, %rax
    .endif
    addq %rdx, %rax
    andq $-2, %rax
    subq %rax, %rdx
    movq %rdx, %rax
    .else
    movq %rax, %rdx
    .if 1
    shrq $63, %rdx
    .else
    shrq $31, %rdx
    .endif
    addq %rdx, %rax
    sarq %rax
    .endif
    jmp     1b

    END nterp_op_div_long_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_rem_long_2addr: /* 0xbf */
    ENTRY nterp_op_rem_long_2addr

/*
 * 32-bit binary div/rem operation.  Handles special case of op1=-1.
 */
    /* div/rem/2addr vA, vB */
    movl    rINST, %ecx                     # rcx <- BA
    sarl    $4, %ecx                       # rcx <- B
    andb    $0xf, rINSTbl                  # rINST <- A
    .if 1
    GET_WIDE_VREG %rax, rINSTq              # eax <- vA
    GET_WIDE_VREG %rcx, %rcx             # ecx <- vB
    .else
    GET_VREG %eax, rINSTq                   # eax <- vA
    GET_VREG %rcx, %rcx                  # ecx <- vB
    .endif
    testq   %rcx, %rcx
    jz      common_errDivideByZero
    cmpq  $-1, %rcx
    je      2f
    cmpq  $2, %rcx
    je      3f
    cqo                                    # rdx:rax <- sign-extended of rax
    idivq   %rcx
1:
    .if 1
    SET_WIDE_VREG %rdx, rINSTq           # vA <- result
    .else
    SET_VREG %rdx, rINSTq                # vA <- result
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1
2:
    .if 1
    xorq %rdx, %rdx
    .else
    negq %rdx
    .endif
    jmp     1b
3:
    .if 1
    movq %rax, %rdx
    .if 1
    shrq $63, %rdx
    .else
    shrq $31, %rdx
    .endif
    addq %rax, %rdx
    andq $-2, %rdx
    subq %rdx, %rax
    movq %rax, %rdx
    .else
    movq %rdx, %rax
    .if 1
    shrq $63, %rax
    .else
    shrq $31, %rax
    .endif
    addq %rax, %rdx
    sarq %rdx
    .endif
    jmp     1b

    END nterp_op_rem_long_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_and_long_2addr: /* 0xc0 */
    ENTRY nterp_op_and_long_2addr

/*
 * Generic 64-bit binary operation.
 */
    /* binop/2addr vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # ecx <- A
    GET_WIDE_VREG %rax, rINSTq              # rax <- vB
    andq %rax,VREG_ADDRESS(%rcx)
    CLEAR_WIDE_REF %rcx
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_and_long_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_or_long_2addr: /* 0xc1 */
    ENTRY nterp_op_or_long_2addr

/*
 * Generic 64-bit binary operation.
 */
    /* binop/2addr vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # ecx <- A
    GET_WIDE_VREG %rax, rINSTq              # rax <- vB
    orq %rax,VREG_ADDRESS(%rcx)
    CLEAR_WIDE_REF %rcx
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_or_long_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_xor_long_2addr: /* 0xc2 */
    ENTRY nterp_op_xor_long_2addr

/*
 * Generic 64-bit binary operation.
 */
    /* binop/2addr vA, vB */
    movl    rINST, %ecx                     # rcx <- A+
    sarl    $4, rINST                      # rINST <- B
    andb    $0xf, %cl                      # ecx <- A
    GET_WIDE_VREG %rax, rINSTq              # rax <- vB
    xorq %rax,VREG_ADDRESS(%rcx)
    CLEAR_WIDE_REF %rcx
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_xor_long_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_shl_long_2addr: /* 0xc3 */
    ENTRY nterp_op_shl_long_2addr

/*
 * Generic 32-bit "shift/2addr" operation.
 */
    /* shift/2addr vA, vB */
    movl    rINST, %ecx                     # ecx <- BA
    sarl    $4, %ecx                       # ecx <- B
    GET_VREG %ecx, %rcx                     # ecx <- vBB
    andb    $0xf, rINSTbl                  # rINST <- A
    .if 1
    GET_WIDE_VREG %rax, rINSTq              # rax <- vAA
    salq    %cl, %rax                                  # ex: sarl %cl, %eax
    SET_WIDE_VREG %rax, rINSTq
    .else
    GET_VREG %eax, rINSTq                   # eax <- vAA
    salq    %cl, %rax                                  # ex: sarl %cl, %eax
    SET_VREG %eax, rINSTq
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_shl_long_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_shr_long_2addr: /* 0xc4 */
    ENTRY nterp_op_shr_long_2addr

/*
 * Generic 32-bit "shift/2addr" operation.
 */
    /* shift/2addr vA, vB */
    movl    rINST, %ecx                     # ecx <- BA
    sarl    $4, %ecx                       # ecx <- B
    GET_VREG %ecx, %rcx                     # ecx <- vBB
    andb    $0xf, rINSTbl                  # rINST <- A
    .if 1
    GET_WIDE_VREG %rax, rINSTq              # rax <- vAA
    sarq    %cl, %rax                                  # ex: sarl %cl, %eax
    SET_WIDE_VREG %rax, rINSTq
    .else
    GET_VREG %eax, rINSTq                   # eax <- vAA
    sarq    %cl, %rax                                  # ex: sarl %cl, %eax
    SET_VREG %eax, rINSTq
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_shr_long_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_ushr_long_2addr: /* 0xc5 */
    ENTRY nterp_op_ushr_long_2addr

/*
 * Generic 32-bit "shift/2addr" operation.
 */
    /* shift/2addr vA, vB */
    movl    rINST, %ecx                     # ecx <- BA
    sarl    $4, %ecx                       # ecx <- B
    GET_VREG %ecx, %rcx                     # ecx <- vBB
    andb    $0xf, rINSTbl                  # rINST <- A
    .if 1
    GET_WIDE_VREG %rax, rINSTq              # rax <- vAA
    shrq    %cl, %rax                                  # ex: sarl %cl, %eax
    SET_WIDE_VREG %rax, rINSTq
    .else
    GET_VREG %eax, rINSTq                   # eax <- vAA
    shrq    %cl, %rax                                  # ex: sarl %cl, %eax
    SET_VREG %eax, rINSTq
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_ushr_long_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_add_float_2addr: /* 0xc6 */
    ENTRY nterp_op_add_float_2addr

    movl    rINST, %ecx                     # ecx <- A+
    andl    $0xf, %ecx                     # ecx <- A
    GET_VREG_XMMs %xmm0, %rcx         # %xmm0 <- 1st src
    sarl    $4, rINST                      # rINST<- B
#ifdef MTERP_USE_AVX
    vaddss VREG_ADDRESS(rINSTq), %xmm0, %xmm0
    SET_VREG_XMMs %xmm0, %rcx         # vAA <- %xmm0
    vpxor    %xmm0, %xmm0, %xmm0
    vmovss %xmm0, VREG_REF_ADDRESS(rINSTq)  # clear ref
#else
    addss VREG_ADDRESS(rINSTq), %xmm0
    SET_VREG_XMMs %xmm0, %rcx         # vAA <- %xmm0
    pxor    %xmm0, %xmm0
    movss %xmm0, VREG_REF_ADDRESS(rINSTq)  # clear ref
#endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_add_float_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sub_float_2addr: /* 0xc7 */
    ENTRY nterp_op_sub_float_2addr

    movl    rINST, %ecx                     # ecx <- A+
    andl    $0xf, %ecx                     # ecx <- A
    GET_VREG_XMMs %xmm0, %rcx         # %xmm0 <- 1st src
    sarl    $4, rINST                      # rINST<- B
#ifdef MTERP_USE_AVX
    vsubss VREG_ADDRESS(rINSTq), %xmm0, %xmm0
    SET_VREG_XMMs %xmm0, %rcx         # vAA <- %xmm0
    vpxor    %xmm0, %xmm0, %xmm0
    vmovss %xmm0, VREG_REF_ADDRESS(rINSTq)  # clear ref
#else
    subss VREG_ADDRESS(rINSTq), %xmm0
    SET_VREG_XMMs %xmm0, %rcx         # vAA <- %xmm0
    pxor    %xmm0, %xmm0
    movss %xmm0, VREG_REF_ADDRESS(rINSTq)  # clear ref
#endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_sub_float_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_mul_float_2addr: /* 0xc8 */
    ENTRY nterp_op_mul_float_2addr

    movl    rINST, %ecx                     # ecx <- A+
    andl    $0xf, %ecx                     # ecx <- A
    GET_VREG_XMMs %xmm0, %rcx         # %xmm0 <- 1st src
    sarl    $4, rINST                      # rINST<- B
#ifdef MTERP_USE_AVX
    vmulss VREG_ADDRESS(rINSTq), %xmm0, %xmm0
    SET_VREG_XMMs %xmm0, %rcx         # vAA <- %xmm0
    vpxor    %xmm0, %xmm0, %xmm0
    vmovss %xmm0, VREG_REF_ADDRESS(rINSTq)  # clear ref
#else
    mulss VREG_ADDRESS(rINSTq), %xmm0
    SET_VREG_XMMs %xmm0, %rcx         # vAA <- %xmm0
    pxor    %xmm0, %xmm0
    movss %xmm0, VREG_REF_ADDRESS(rINSTq)  # clear ref
#endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_mul_float_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_div_float_2addr: /* 0xc9 */
    ENTRY nterp_op_div_float_2addr

    movl    rINST, %ecx                     # ecx <- A+
    andl    $0xf, %ecx                     # ecx <- A
    GET_VREG_XMMs %xmm0, %rcx         # %xmm0 <- 1st src
    sarl    $4, rINST                      # rINST<- B
#ifdef MTERP_USE_AVX
    vdivss VREG_ADDRESS(rINSTq), %xmm0, %xmm0
    SET_VREG_XMMs %xmm0, %rcx         # vAA <- %xmm0
    vpxor    %xmm0, %xmm0, %xmm0
    vmovss %xmm0, VREG_REF_ADDRESS(rINSTq)  # clear ref
#else
    divss VREG_ADDRESS(rINSTq), %xmm0
    SET_VREG_XMMs %xmm0, %rcx         # vAA <- %xmm0
    pxor    %xmm0, %xmm0
    movss %xmm0, VREG_REF_ADDRESS(rINSTq)  # clear ref
#endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_div_float_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_rem_float_2addr: /* 0xca */
    ENTRY nterp_op_rem_float_2addr

    /* rem_float/2addr vA, vB */
    movzbq  rINSTbl, %rcx                   # ecx <- A+
    sarl    $4, rINST                      # rINST <- B
    flds    VREG_ADDRESS(rINSTq)            # vB to fp stack
    andb    $0xf, %cl                      # ecx <- A
    flds    VREG_ADDRESS(%rcx)              # vA to fp stack
1:
    fprem
    fstsw   %ax
    sahf
    jp      1b
    fstp    %st(1)
    fstps   VREG_ADDRESS(%rcx)              # %st to vA
    CLEAR_REF %rcx
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_rem_float_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_add_double_2addr: /* 0xcb */
    ENTRY nterp_op_add_double_2addr

    movl    rINST, %ecx                     # ecx <- A+
    andl    $0xf, %ecx                     # ecx <- A
    GET_VREG_XMMd %xmm0, %rcx         # %xmm0 <- 1st src
    sarl    $4, rINST                      # rINST<- B
#ifdef MTERP_USE_AVX
    vaddsd VREG_ADDRESS(rINSTq), %xmm0, %xmm0
    SET_VREG_XMMd %xmm0, %rcx         # vAA <- %xmm0
    vpxor    %xmm0, %xmm0, %xmm0
    vmovsd %xmm0, VREG_REF_ADDRESS(rINSTq)  # clear ref
#else
    addsd VREG_ADDRESS(rINSTq), %xmm0
    SET_VREG_XMMd %xmm0, %rcx         # vAA <- %xmm0
    pxor    %xmm0, %xmm0
    movsd %xmm0, VREG_REF_ADDRESS(rINSTq)  # clear ref
#endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_add_double_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sub_double_2addr: /* 0xcc */
    ENTRY nterp_op_sub_double_2addr

    movl    rINST, %ecx                     # ecx <- A+
    andl    $0xf, %ecx                     # ecx <- A
    GET_VREG_XMMd %xmm0, %rcx         # %xmm0 <- 1st src
    sarl    $4, rINST                      # rINST<- B
#ifdef MTERP_USE_AVX
    vsubsd VREG_ADDRESS(rINSTq), %xmm0, %xmm0
    SET_VREG_XMMd %xmm0, %rcx         # vAA <- %xmm0
    vpxor    %xmm0, %xmm0, %xmm0
    vmovsd %xmm0, VREG_REF_ADDRESS(rINSTq)  # clear ref
#else
    subsd VREG_ADDRESS(rINSTq), %xmm0
    SET_VREG_XMMd %xmm0, %rcx         # vAA <- %xmm0
    pxor    %xmm0, %xmm0
    movsd %xmm0, VREG_REF_ADDRESS(rINSTq)  # clear ref
#endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_sub_double_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_mul_double_2addr: /* 0xcd */
    ENTRY nterp_op_mul_double_2addr

    movl    rINST, %ecx                     # ecx <- A+
    andl    $0xf, %ecx                     # ecx <- A
    GET_VREG_XMMd %xmm0, %rcx         # %xmm0 <- 1st src
    sarl    $4, rINST                      # rINST<- B
#ifdef MTERP_USE_AVX
    vmulsd VREG_ADDRESS(rINSTq), %xmm0, %xmm0
    SET_VREG_XMMd %xmm0, %rcx         # vAA <- %xmm0
    vpxor    %xmm0, %xmm0, %xmm0
    vmovsd %xmm0, VREG_REF_ADDRESS(rINSTq)  # clear ref
#else
    mulsd VREG_ADDRESS(rINSTq), %xmm0
    SET_VREG_XMMd %xmm0, %rcx         # vAA <- %xmm0
    pxor    %xmm0, %xmm0
    movsd %xmm0, VREG_REF_ADDRESS(rINSTq)  # clear ref
#endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_mul_double_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_div_double_2addr: /* 0xce */
    ENTRY nterp_op_div_double_2addr

    movl    rINST, %ecx                     # ecx <- A+
    andl    $0xf, %ecx                     # ecx <- A
    GET_VREG_XMMd %xmm0, %rcx         # %xmm0 <- 1st src
    sarl    $4, rINST                      # rINST<- B
#ifdef MTERP_USE_AVX
    vdivsd VREG_ADDRESS(rINSTq), %xmm0, %xmm0
    SET_VREG_XMMd %xmm0, %rcx         # vAA <- %xmm0
    vpxor    %xmm0, %xmm0, %xmm0
    vmovsd %xmm0, VREG_REF_ADDRESS(rINSTq)  # clear ref
#else
    divsd VREG_ADDRESS(rINSTq), %xmm0
    SET_VREG_XMMd %xmm0, %rcx         # vAA <- %xmm0
    pxor    %xmm0, %xmm0
    movsd %xmm0, VREG_REF_ADDRESS(rINSTq)  # clear ref
#endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_div_double_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_rem_double_2addr: /* 0xcf */
    ENTRY nterp_op_rem_double_2addr

    /* rem_double/2addr vA, vB */
    movzbq  rINSTbl, %rcx                   # ecx <- A+
    sarl    $4, rINST                      # rINST <- B
    fldl    VREG_ADDRESS(rINSTq)            # vB to fp stack
    andb    $0xf, %cl                      # ecx <- A
    fldl    VREG_ADDRESS(%rcx)              # vA to fp stack
1:
    fprem
    fstsw   %ax
    sahf
    jp      1b
    fstp    %st(1)
    fstpl   VREG_ADDRESS(%rcx)              # %st to vA
    CLEAR_WIDE_REF %rcx
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 1

    END nterp_op_rem_double_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_add_int_lit16: /* 0xd0 */
    ENTRY nterp_op_add_int_lit16

/*
 * Generic 32-bit "lit16" binary operation.  Provide an "instr" line
 * that specifies an instruction that performs "result = eax op ecx".
 * This could be an x86 instruction or a function call.  (If the result
 * comes back in a register other than eax, you can override "result".)
 *
 * For: add-int/lit16, rsub-int,
 *      and-int/lit16, or-int/lit16, xor-int/lit16
 */
    /* binop/lit16 vA, vB, #+CCCC */
    movl    rINST, %eax                     # rax <- 000000BA
    sarl    $4, %eax                       # eax <- B
    GET_VREG %eax, %rax                     # eax <- vB
    andb    $0xf, rINSTbl                  # rINST <- A
    movswl  2(rPC), %ecx                    # ecx <- ssssCCCC
    addl    %ecx, %eax                                  # for example: addl %ecx, %eax
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_add_int_lit16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_rsub_int: /* 0xd1 */
    ENTRY nterp_op_rsub_int

/* this op is "rsub-int", but can be thought of as "rsub-int/lit16" */
/*
 * Generic 32-bit "lit16" binary operation.  Provide an "instr" line
 * that specifies an instruction that performs "result = eax op ecx".
 * This could be an x86 instruction or a function call.  (If the result
 * comes back in a register other than eax, you can override "result".)
 *
 * For: add-int/lit16, rsub-int,
 *      and-int/lit16, or-int/lit16, xor-int/lit16
 */
    /* binop/lit16 vA, vB, #+CCCC */
    movl    rINST, %eax                     # rax <- 000000BA
    sarl    $4, %eax                       # eax <- B
    GET_VREG %eax, %rax                     # eax <- vB
    andb    $0xf, rINSTbl                  # rINST <- A
    movswl  2(rPC), %ecx                    # ecx <- ssssCCCC
    subl    %eax, %ecx                                  # for example: addl %ecx, %eax
    SET_VREG %ecx, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_rsub_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_mul_int_lit16: /* 0xd2 */
    ENTRY nterp_op_mul_int_lit16

/*
 * Generic 32-bit "lit16" binary operation.  Provide an "instr" line
 * that specifies an instruction that performs "result = eax op ecx".
 * This could be an x86 instruction or a function call.  (If the result
 * comes back in a register other than eax, you can override "result".)
 *
 * For: add-int/lit16, rsub-int,
 *      and-int/lit16, or-int/lit16, xor-int/lit16
 */
    /* binop/lit16 vA, vB, #+CCCC */
    movl    rINST, %eax                     # rax <- 000000BA
    sarl    $4, %eax                       # eax <- B
    GET_VREG %eax, %rax                     # eax <- vB
    andb    $0xf, rINSTbl                  # rINST <- A
    movswl  2(rPC), %ecx                    # ecx <- ssssCCCC
    imull   %ecx, %eax                                  # for example: addl %ecx, %eax
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_mul_int_lit16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_div_int_lit16: /* 0xd3 */
    ENTRY nterp_op_div_int_lit16

/*
 * 32-bit binary div/rem operation.  Handles special case of op1=-1.
 */
    /* div/rem/lit16 vA, vB, #+CCCC */
    /* Need A in rINST, ssssCCCC in ecx, vB in eax */
    movl    rINST, %eax                     # rax <- 000000BA
    sarl    $4, %eax                       # eax <- B
    GET_VREG %eax, %rax                     # eax <- vB
    movswl  2(rPC), %ecx                    # ecx <- ssssCCCC
    andb    $0xf, rINSTbl                  # rINST <- A
    testl   %ecx, %ecx
    jz      common_errDivideByZero
    cmpl    $-1, %ecx
    je      2f
    cdq                                     # rax <- sign-extended of eax
    idivl   %ecx
1:
    SET_VREG %eax, rINSTq                # vA <- result
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
2:
    .if 0
    xorl    %eax, %eax
    .else
    negl    %eax
    .endif
    jmp     1b

    END nterp_op_div_int_lit16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_rem_int_lit16: /* 0xd4 */
    ENTRY nterp_op_rem_int_lit16

/*
 * 32-bit binary div/rem operation.  Handles special case of op1=-1.
 */
    /* div/rem/lit16 vA, vB, #+CCCC */
    /* Need A in rINST, ssssCCCC in ecx, vB in eax */
    movl    rINST, %eax                     # rax <- 000000BA
    sarl    $4, %eax                       # eax <- B
    GET_VREG %eax, %rax                     # eax <- vB
    movswl  2(rPC), %ecx                    # ecx <- ssssCCCC
    andb    $0xf, rINSTbl                  # rINST <- A
    testl   %ecx, %ecx
    jz      common_errDivideByZero
    cmpl    $-1, %ecx
    je      2f
    cdq                                     # rax <- sign-extended of eax
    idivl   %ecx
1:
    SET_VREG %edx, rINSTq                # vA <- result
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
2:
    .if 1
    xorl    %edx, %edx
    .else
    negl    %edx
    .endif
    jmp     1b

    END nterp_op_rem_int_lit16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_and_int_lit16: /* 0xd5 */
    ENTRY nterp_op_and_int_lit16

/*
 * Generic 32-bit "lit16" binary operation.  Provide an "instr" line
 * that specifies an instruction that performs "result = eax op ecx".
 * This could be an x86 instruction or a function call.  (If the result
 * comes back in a register other than eax, you can override "result".)
 *
 * For: add-int/lit16, rsub-int,
 *      and-int/lit16, or-int/lit16, xor-int/lit16
 */
    /* binop/lit16 vA, vB, #+CCCC */
    movl    rINST, %eax                     # rax <- 000000BA
    sarl    $4, %eax                       # eax <- B
    GET_VREG %eax, %rax                     # eax <- vB
    andb    $0xf, rINSTbl                  # rINST <- A
    movswl  2(rPC), %ecx                    # ecx <- ssssCCCC
    andl    %ecx, %eax                                  # for example: addl %ecx, %eax
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_and_int_lit16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_or_int_lit16: /* 0xd6 */
    ENTRY nterp_op_or_int_lit16

/*
 * Generic 32-bit "lit16" binary operation.  Provide an "instr" line
 * that specifies an instruction that performs "result = eax op ecx".
 * This could be an x86 instruction or a function call.  (If the result
 * comes back in a register other than eax, you can override "result".)
 *
 * For: add-int/lit16, rsub-int,
 *      and-int/lit16, or-int/lit16, xor-int/lit16
 */
    /* binop/lit16 vA, vB, #+CCCC */
    movl    rINST, %eax                     # rax <- 000000BA
    sarl    $4, %eax                       # eax <- B
    GET_VREG %eax, %rax                     # eax <- vB
    andb    $0xf, rINSTbl                  # rINST <- A
    movswl  2(rPC), %ecx                    # ecx <- ssssCCCC
    orl     %ecx, %eax                                  # for example: addl %ecx, %eax
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_or_int_lit16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_xor_int_lit16: /* 0xd7 */
    ENTRY nterp_op_xor_int_lit16

/*
 * Generic 32-bit "lit16" binary operation.  Provide an "instr" line
 * that specifies an instruction that performs "result = eax op ecx".
 * This could be an x86 instruction or a function call.  (If the result
 * comes back in a register other than eax, you can override "result".)
 *
 * For: add-int/lit16, rsub-int,
 *      and-int/lit16, or-int/lit16, xor-int/lit16
 */
    /* binop/lit16 vA, vB, #+CCCC */
    movl    rINST, %eax                     # rax <- 000000BA
    sarl    $4, %eax                       # eax <- B
    GET_VREG %eax, %rax                     # eax <- vB
    andb    $0xf, rINSTbl                  # rINST <- A
    movswl  2(rPC), %ecx                    # ecx <- ssssCCCC
    xorl    %ecx, %eax                                  # for example: addl %ecx, %eax
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_xor_int_lit16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_add_int_lit8: /* 0xd8 */
    ENTRY nterp_op_add_int_lit8

/*
 * Generic 32-bit "lit8" binary operation.  Provide an "instr" line
 * that specifies an instruction that performs "result = eax op ecx".
 * This could be an x86 instruction or a function call.  (If the result
 * comes back in a register other than r0, you can override "result".)
 *
 * For: add-int/lit8, rsub-int/lit8
 *      and-int/lit8, or-int/lit8, xor-int/lit8,
 *      shl-int/lit8, shr-int/lit8, ushr-int/lit8
 */
    /* binop/lit8 vAA, vBB, #+CC */
    movzbq  2(rPC), %rax                    # rax <- BB
    movsbl  3(rPC), %ecx                    # rcx <- ssssssCC
    GET_VREG %eax, %rax                     # eax <- rBB
    addl    %ecx, %eax                                  # ex: addl %ecx,%eax
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_add_int_lit8

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_rsub_int_lit8: /* 0xd9 */
    ENTRY nterp_op_rsub_int_lit8

/*
 * Generic 32-bit "lit8" binary operation.  Provide an "instr" line
 * that specifies an instruction that performs "result = eax op ecx".
 * This could be an x86 instruction or a function call.  (If the result
 * comes back in a register other than r0, you can override "result".)
 *
 * For: add-int/lit8, rsub-int/lit8
 *      and-int/lit8, or-int/lit8, xor-int/lit8,
 *      shl-int/lit8, shr-int/lit8, ushr-int/lit8
 */
    /* binop/lit8 vAA, vBB, #+CC */
    movzbq  2(rPC), %rax                    # rax <- BB
    movsbl  3(rPC), %ecx                    # rcx <- ssssssCC
    GET_VREG %eax, %rax                     # eax <- rBB
    subl    %eax, %ecx                                  # ex: addl %ecx,%eax
    SET_VREG %ecx, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_rsub_int_lit8

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_mul_int_lit8: /* 0xda */
    ENTRY nterp_op_mul_int_lit8

/*
 * Generic 32-bit "lit8" binary operation.  Provide an "instr" line
 * that specifies an instruction that performs "result = eax op ecx".
 * This could be an x86 instruction or a function call.  (If the result
 * comes back in a register other than r0, you can override "result".)
 *
 * For: add-int/lit8, rsub-int/lit8
 *      and-int/lit8, or-int/lit8, xor-int/lit8,
 *      shl-int/lit8, shr-int/lit8, ushr-int/lit8
 */
    /* binop/lit8 vAA, vBB, #+CC */
    movzbq  2(rPC), %rax                    # rax <- BB
    movsbl  3(rPC), %ecx                    # rcx <- ssssssCC
    GET_VREG %eax, %rax                     # eax <- rBB
    imull   %ecx, %eax                                  # ex: addl %ecx,%eax
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_mul_int_lit8

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_div_int_lit8: /* 0xdb */
    ENTRY nterp_op_div_int_lit8

/*
 * 32-bit div/rem "lit8" binary operation.  Handles special case of
 * op0=minint & op1=-1
 */
    /* div/rem/lit8 vAA, vBB, #+CC */
    movzbq  2(rPC), %rax                    # eax <- BB
    movsbl  3(rPC), %ecx                    # ecx <- ssssssCC
    GET_VREG  %eax, %rax                    # eax <- rBB
    testl   %ecx, %ecx
    je      common_errDivideByZero
    cmpl    $-1, %ecx
    je      2f
    cdq                                     # rax <- sign-extended of eax
    idivl   %ecx
1:
    SET_VREG %eax, rINSTq                # vA <- result
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
2:
    .if 0
    xorl    %eax, %eax
    .else
    negl    %eax
    .endif
    jmp     1b

    END nterp_op_div_int_lit8

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_rem_int_lit8: /* 0xdc */
    ENTRY nterp_op_rem_int_lit8

/*
 * 32-bit div/rem "lit8" binary operation.  Handles special case of
 * op0=minint & op1=-1
 */
    /* div/rem/lit8 vAA, vBB, #+CC */
    movzbq  2(rPC), %rax                    # eax <- BB
    movsbl  3(rPC), %ecx                    # ecx <- ssssssCC
    GET_VREG  %eax, %rax                    # eax <- rBB
    testl   %ecx, %ecx
    je      common_errDivideByZero
    cmpl    $-1, %ecx
    je      2f
    cdq                                     # rax <- sign-extended of eax
    idivl   %ecx
1:
    SET_VREG %edx, rINSTq                # vA <- result
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
2:
    .if 1
    xorl    %edx, %edx
    .else
    negl    %edx
    .endif
    jmp     1b

    END nterp_op_rem_int_lit8

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_and_int_lit8: /* 0xdd */
    ENTRY nterp_op_and_int_lit8

/*
 * Generic 32-bit "lit8" binary operation.  Provide an "instr" line
 * that specifies an instruction that performs "result = eax op ecx".
 * This could be an x86 instruction or a function call.  (If the result
 * comes back in a register other than r0, you can override "result".)
 *
 * For: add-int/lit8, rsub-int/lit8
 *      and-int/lit8, or-int/lit8, xor-int/lit8,
 *      shl-int/lit8, shr-int/lit8, ushr-int/lit8
 */
    /* binop/lit8 vAA, vBB, #+CC */
    movzbq  2(rPC), %rax                    # rax <- BB
    movsbl  3(rPC), %ecx                    # rcx <- ssssssCC
    GET_VREG %eax, %rax                     # eax <- rBB
    andl    %ecx, %eax                                  # ex: addl %ecx,%eax
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_and_int_lit8

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_or_int_lit8: /* 0xde */
    ENTRY nterp_op_or_int_lit8

/*
 * Generic 32-bit "lit8" binary operation.  Provide an "instr" line
 * that specifies an instruction that performs "result = eax op ecx".
 * This could be an x86 instruction or a function call.  (If the result
 * comes back in a register other than r0, you can override "result".)
 *
 * For: add-int/lit8, rsub-int/lit8
 *      and-int/lit8, or-int/lit8, xor-int/lit8,
 *      shl-int/lit8, shr-int/lit8, ushr-int/lit8
 */
    /* binop/lit8 vAA, vBB, #+CC */
    movzbq  2(rPC), %rax                    # rax <- BB
    movsbl  3(rPC), %ecx                    # rcx <- ssssssCC
    GET_VREG %eax, %rax                     # eax <- rBB
    orl     %ecx, %eax                                  # ex: addl %ecx,%eax
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_or_int_lit8

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_xor_int_lit8: /* 0xdf */
    ENTRY nterp_op_xor_int_lit8

/*
 * Generic 32-bit "lit8" binary operation.  Provide an "instr" line
 * that specifies an instruction that performs "result = eax op ecx".
 * This could be an x86 instruction or a function call.  (If the result
 * comes back in a register other than r0, you can override "result".)
 *
 * For: add-int/lit8, rsub-int/lit8
 *      and-int/lit8, or-int/lit8, xor-int/lit8,
 *      shl-int/lit8, shr-int/lit8, ushr-int/lit8
 */
    /* binop/lit8 vAA, vBB, #+CC */
    movzbq  2(rPC), %rax                    # rax <- BB
    movsbl  3(rPC), %ecx                    # rcx <- ssssssCC
    GET_VREG %eax, %rax                     # eax <- rBB
    xorl    %ecx, %eax                                  # ex: addl %ecx,%eax
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_xor_int_lit8

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_shl_int_lit8: /* 0xe0 */
    ENTRY nterp_op_shl_int_lit8

/*
 * Generic 32-bit "lit8" binary operation.  Provide an "instr" line
 * that specifies an instruction that performs "result = eax op ecx".
 * This could be an x86 instruction or a function call.  (If the result
 * comes back in a register other than r0, you can override "result".)
 *
 * For: add-int/lit8, rsub-int/lit8
 *      and-int/lit8, or-int/lit8, xor-int/lit8,
 *      shl-int/lit8, shr-int/lit8, ushr-int/lit8
 */
    /* binop/lit8 vAA, vBB, #+CC */
    movzbq  2(rPC), %rax                    # rax <- BB
    movsbl  3(rPC), %ecx                    # rcx <- ssssssCC
    GET_VREG %eax, %rax                     # eax <- rBB
    sall    %cl, %eax                                  # ex: addl %ecx,%eax
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_shl_int_lit8

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_shr_int_lit8: /* 0xe1 */
    ENTRY nterp_op_shr_int_lit8

/*
 * Generic 32-bit "lit8" binary operation.  Provide an "instr" line
 * that specifies an instruction that performs "result = eax op ecx".
 * This could be an x86 instruction or a function call.  (If the result
 * comes back in a register other than r0, you can override "result".)
 *
 * For: add-int/lit8, rsub-int/lit8
 *      and-int/lit8, or-int/lit8, xor-int/lit8,
 *      shl-int/lit8, shr-int/lit8, ushr-int/lit8
 */
    /* binop/lit8 vAA, vBB, #+CC */
    movzbq  2(rPC), %rax                    # rax <- BB
    movsbl  3(rPC), %ecx                    # rcx <- ssssssCC
    GET_VREG %eax, %rax                     # eax <- rBB
    sarl    %cl, %eax                                  # ex: addl %ecx,%eax
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_shr_int_lit8

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_ushr_int_lit8: /* 0xe2 */
    ENTRY nterp_op_ushr_int_lit8

/*
 * Generic 32-bit "lit8" binary operation.  Provide an "instr" line
 * that specifies an instruction that performs "result = eax op ecx".
 * This could be an x86 instruction or a function call.  (If the result
 * comes back in a register other than r0, you can override "result".)
 *
 * For: add-int/lit8, rsub-int/lit8
 *      and-int/lit8, or-int/lit8, xor-int/lit8,
 *      shl-int/lit8, shr-int/lit8, ushr-int/lit8
 */
    /* binop/lit8 vAA, vBB, #+CC */
    movzbq  2(rPC), %rax                    # rax <- BB
    movsbl  3(rPC), %ecx                    # rcx <- ssssssCC
    GET_VREG %eax, %rax                     # eax <- rBB
    shrl    %cl, %eax                                  # ex: addl %ecx,%eax
    SET_VREG %eax, rINSTq
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

    END nterp_op_ushr_int_lit8

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_e3: /* 0xe3 */
    ENTRY nterp_op_unused_e3

    int3

    END nterp_op_unused_e3

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_e4: /* 0xe4 */
    ENTRY nterp_op_unused_e4

    int3

    END nterp_op_unused_e4

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_e5: /* 0xe5 */
    ENTRY nterp_op_unused_e5

    int3

    END nterp_op_unused_e5

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_e6: /* 0xe6 */
    ENTRY nterp_op_unused_e6

    int3

    END nterp_op_unused_e6

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_e7: /* 0xe7 */
    ENTRY nterp_op_unused_e7

    int3

    END nterp_op_unused_e7

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_e8: /* 0xe8 */
    ENTRY nterp_op_unused_e8

    int3

    END nterp_op_unused_e8

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_e9: /* 0xe9 */
    ENTRY nterp_op_unused_e9

    int3

    END nterp_op_unused_e9

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_ea: /* 0xea */
    ENTRY nterp_op_unused_ea

    int3

    END nterp_op_unused_ea

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_eb: /* 0xeb */
    ENTRY nterp_op_unused_eb

    int3

    END nterp_op_unused_eb

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_ec: /* 0xec */
    ENTRY nterp_op_unused_ec

    int3

    END nterp_op_unused_ec

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_ed: /* 0xed */
    ENTRY nterp_op_unused_ed

    int3

    END nterp_op_unused_ed

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_ee: /* 0xee */
    ENTRY nterp_op_unused_ee

    int3

    END nterp_op_unused_ee

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_ef: /* 0xef */
    ENTRY nterp_op_unused_ef

    int3

    END nterp_op_unused_ef

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_f0: /* 0xf0 */
    ENTRY nterp_op_unused_f0

    int3

    END nterp_op_unused_f0

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_f1: /* 0xf1 */
    ENTRY nterp_op_unused_f1

    int3

    END nterp_op_unused_f1

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_f2: /* 0xf2 */
    ENTRY nterp_op_unused_f2

    int3

    END nterp_op_unused_f2

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_f3: /* 0xf3 */
    ENTRY nterp_op_unused_f3

    int3

    END nterp_op_unused_f3

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_f4: /* 0xf4 */
    ENTRY nterp_op_unused_f4

    int3

    END nterp_op_unused_f4

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_f5: /* 0xf5 */
    ENTRY nterp_op_unused_f5

    int3

    END nterp_op_unused_f5

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_f6: /* 0xf6 */
    ENTRY nterp_op_unused_f6

    int3

    END nterp_op_unused_f6

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_f7: /* 0xf7 */
    ENTRY nterp_op_unused_f7

    int3

    END nterp_op_unused_f7

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_f8: /* 0xf8 */
    ENTRY nterp_op_unused_f8

    int3

    END nterp_op_unused_f8

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_f9: /* 0xf9 */
    ENTRY nterp_op_unused_f9

    int3

    END nterp_op_unused_f9

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_polymorphic: /* 0xfa */
    ENTRY nterp_op_invoke_polymorphic

   EXPORT_PC
   // No need to fetch the target method.
   // Load the first argument (the 'this' pointer).
   movzwl 4(rPC), %r11d // arguments
   andq $0xf, %r11
   movl (rFP, %r11, 4), %esi
   // NullPointerException check.
   movl (%esi), %eax
   jmp NterpCommonInvokePolymorphic

    END nterp_op_invoke_polymorphic

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_polymorphic_range: /* 0xfb */
    ENTRY nterp_op_invoke_polymorphic_range

   EXPORT_PC
   // No need to fetch the target method.
   // Load the first argument (the 'this' pointer).
   movzwl 4(rPC), %r11d // arguments
   movl (rFP, %r11, 4), %esi
   // NullPointerException check.
   movl (%esi), %eax
   jmp NterpCommonInvokePolymorphicRange

    END nterp_op_invoke_polymorphic_range

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_custom: /* 0xfc */
    ENTRY nterp_op_invoke_custom

   EXPORT_PC
   movzwl 2(rPC), %edi // call_site index, first argument of runtime call.
   jmp NterpCommonInvokeCustom

    END nterp_op_invoke_custom

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_custom_range: /* 0xfd */
    ENTRY nterp_op_invoke_custom_range

   EXPORT_PC
   movzwl 2(rPC), %edi // call_site index, first argument of runtime call.
   jmp NterpCommonInvokeCustomRange

    END nterp_op_invoke_custom_range

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_const_method_handle: /* 0xfe */
    ENTRY nterp_op_const_method_handle

   // Fast-path which gets the object from thread-local cache.
   FETCH_FROM_THREAD_CACHE %rax, 2f
   cmpq MACRO_LITERAL(0), rSELF:THREAD_READ_BARRIER_MARK_REG00_OFFSET
   jne 3f
1:
   SET_VREG_OBJECT %eax, rINSTq            # vAA <- value
   .if 0
   ADVANCE_PC_FETCH_AND_GOTO_NEXT 3
   .else
   ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
   .endif
2:
   EXPORT_PC
   movq rSELF:THREAD_SELF_OFFSET, %rdi
   movq 0(%rsp), %rsi
   movq rPC, %rdx
   call SYMBOL(nterp_load_object)
   jmp 1b
3:
   // 00 is %rax
   call art_quick_read_barrier_mark_reg00
   jmp 1b

    END nterp_op_const_method_handle

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_const_method_type: /* 0xff */
    ENTRY nterp_op_const_method_type

   // Fast-path which gets the object from thread-local cache.
   FETCH_FROM_THREAD_CACHE %rax, 2f
   cmpq MACRO_LITERAL(0), rSELF:THREAD_READ_BARRIER_MARK_REG00_OFFSET
   jne 3f
1:
   SET_VREG_OBJECT %eax, rINSTq            # vAA <- value
   .if 0
   ADVANCE_PC_FETCH_AND_GOTO_NEXT 3
   .else
   ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
   .endif
2:
   EXPORT_PC
   movq rSELF:THREAD_SELF_OFFSET, %rdi
   movq 0(%rsp), %rsi
   movq rPC, %rdx
   call SYMBOL(nterp_load_object)
   jmp 1b
3:
   // 00 is %rax
   call art_quick_read_barrier_mark_reg00
   jmp 1b

    END nterp_op_const_method_type

    .balign MTERP_HANDLER_SIZE

    FUNCTION_TYPE(artNterpAsmInstructionEnd)
    ASM_HIDDEN SYMBOL(artNterpAsmInstructionEnd)
    .global SYMBOL(artNterpAsmInstructionEnd)
SYMBOL(artNterpAsmInstructionEnd):
    // artNterpAsmInstructionEnd is used as landing pad for exception handling.
    FETCH_INST
    GOTO_NEXT

    ENTRY nterp_op_invoke_interface_helper
   movq rSELF:THREAD_SELF_OFFSET, %rdi
   movq 0(%rsp), %rsi
   movq rPC, %rdx
   call nterp_get_method
   jmp .Lop_invoke_interface_resume

    END nterp_op_invoke_interface_helper
    ENTRY nterp_op_invoke_interface_range_helper
   movq rSELF:THREAD_SELF_OFFSET, %rdi
   movq 0(%rsp), %rsi
   movq rPC, %rdx
   call nterp_get_method
   jmp .Lop_invoke_interface_range_resume

    END nterp_op_invoke_interface_range_helper

/*
 * ===========================================================================
 *  Common subroutines and data
 * ===========================================================================
 */

    .text
    .align  2

// Enclose all code below in a symbol (which gets printed in backtraces).
ENTRY nterp_helper

// Note: mterp also uses the common_* names below for helpers, but that's OK
// as the C compiler compiled each interpreter separately.
common_errDivideByZero:
    EXPORT_PC
    call art_quick_throw_div_zero

// Expect array in edi, index in esi.
common_errArrayIndex:
    EXPORT_PC
    movl MIRROR_ARRAY_LENGTH_OFFSET(%edi), %eax
    movl %esi, %edi
    movl %eax, %esi
    call art_quick_throw_array_bounds

common_errNullObject:
    EXPORT_PC
    call art_quick_throw_null_pointer_exception

NterpCommonInvokeStatic:
    COMMON_INVOKE_NON_RANGE is_static=1, is_interface=0, suffix="invokeStatic"

NterpCommonInvokeStaticRange:
    COMMON_INVOKE_RANGE is_static=1, is_interface=0, suffix="invokeStatic"

NterpCommonInvokeInstance:
    COMMON_INVOKE_NON_RANGE is_static=0, is_interface=0, suffix="invokeInstance"

NterpCommonInvokeInstanceRange:
    COMMON_INVOKE_RANGE is_static=0, is_interface=0, suffix="invokeInstance"

NterpCommonInvokeInterface:
    COMMON_INVOKE_NON_RANGE is_static=0, is_interface=1, suffix="invokeInterface"

NterpCommonInvokeInterfaceRange:
    COMMON_INVOKE_RANGE is_static=0, is_interface=1, suffix="invokeInterface"

NterpCommonInvokePolymorphic:
    COMMON_INVOKE_NON_RANGE is_static=0, is_interface=0, is_string_init=0, is_polymorphic=1, suffix="invokePolymorphic"

NterpCommonInvokePolymorphicRange:
    COMMON_INVOKE_RANGE is_static=0, is_interface=0, is_polymorphic=1, suffix="invokePolymorphic"

NterpCommonInvokeCustom:
    COMMON_INVOKE_NON_RANGE is_static=1, is_interface=0, is_string_init=0, is_polymorphic=0, is_custom=1, suffix="invokeCustom"

NterpCommonInvokeCustomRange:
    COMMON_INVOKE_RANGE is_static=1, is_interface=0, is_polymorphic=0, is_custom=1, suffix="invokeCustom"

NterpHandleStringInit:
   COMMON_INVOKE_NON_RANGE is_static=0, is_interface=0, is_string_init=1, suffix="stringInit"

NterpHandleStringInitRange:
   COMMON_INVOKE_RANGE is_static=0, is_interface=0, is_string_init=1, suffix="stringInit"

NterpNewInstance:
   EXPORT_PC
   // Fast-path which gets the class from thread-local cache.
   FETCH_FROM_THREAD_CACHE %rdi, 2f
   cmpq $0, rSELF:THREAD_READ_BARRIER_MARK_REG00_OFFSET
   jne 3f
4:
   callq *rSELF:THREAD_ALLOC_OBJECT_ENTRYPOINT_OFFSET
1:
   SET_VREG_OBJECT %eax, rINSTq            # fp[A] <- value
   ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
2:
   movq rSELF:THREAD_SELF_OFFSET, %rdi
   movq 0(%rsp), %rsi
   movq rPC, %rdx
   call nterp_get_class_or_allocate_object
   jmp 1b
3:
   // 07 is %rdi
   call art_quick_read_barrier_mark_reg07
   jmp 4b

NterpNewArray:
   /* new-array vA, vB, class@CCCC */
   EXPORT_PC
   // Fast-path which gets the class from thread-local cache.
   FETCH_FROM_THREAD_CACHE %rdi, 2f
   cmpq $0, rSELF:THREAD_READ_BARRIER_MARK_REG00_OFFSET
   jne 3f
1:
   movzbl  rINSTbl,%esi
   sarl    $4,%esi                          # esi<- B
   GET_VREG %esi %rsi                        # esi<- vB (array length)
   andb    $0xf,rINSTbl                     # rINST<- A
   callq *rSELF:THREAD_ALLOC_ARRAY_ENTRYPOINT_OFFSET
   SET_VREG_OBJECT %eax, rINSTq            # fp[A] <- value
   ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
2:
   movq rSELF:THREAD_SELF_OFFSET, %rdi
   movq 0(%rsp), %rsi
   movq rPC, %rdx
   call nterp_get_class_or_allocate_object
   movq %rax, %rdi
   jmp 1b
3:
   // 07 is %rdi
   call art_quick_read_barrier_mark_reg07
   jmp 1b

NterpPutObjectInstanceField:
   movl    rINST, %ebp                     # rbp <- BA
   andl    $0xf, %ebp                     # rbp <- A
   GET_VREG %ecx, %rbp                     # ecx <- v[A]
   sarl    $4, rINST
   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE %rax, 2f
1:
   GET_VREG rINST, rINSTq                  # vB (object we're operating on)
   testl   rINST, rINST                    # is object null?
   je      common_errNullObject
   movl %ecx, (rINSTq,%rax,1)
   testl %ecx, %ecx
   je 4f
   movq rSELF:THREAD_CARD_TABLE_OFFSET, %rax
   shrq $CARD_TABLE_CARD_SHIFT, rINSTq
   movb %al, (%rax, rINSTq, 1)
4:
   ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
2:
   EXPORT_PC
   movq rSELF:THREAD_SELF_OFFSET, %rdi
   movq 0(%rsp), %rsi
   movq rPC, %rdx
   // %rcx is already set.
   call nterp_get_instance_field_offset
   // Reload the value as it may have moved.
   GET_VREG %ecx, %rbp                     # ecx <- v[A]
   testl %eax, %eax
   jns 1b
   GET_VREG rINST, rINSTq                  # vB (object we're operating on)
   testl   rINST, rINST                    # is object null?
   je      common_errNullObject
   negl %eax
   movl %ecx, (rINSTq,%rax,1)
   testl %ecx, %ecx
   je 5f
   movq rSELF:THREAD_CARD_TABLE_OFFSET, %rax
   shrq $CARD_TABLE_CARD_SHIFT, rINSTq
   movb %al, (%rax, rINSTq, 1)
5:
   lock addl $0, (%rsp)
   ADVANCE_PC_FETCH_AND_GOTO_NEXT 2

NterpGetObjectInstanceField:
   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE %rax, 2f
1:
   movl    rINST, %ecx                     # rcx <- BA
   sarl    $4, %ecx                       # ecx <- B
   GET_VREG %ecx, %rcx                     # vB (object we're operating on)
   testl   %ecx, %ecx                      # is object null?
   je      common_errNullObject
   testb $READ_BARRIER_TEST_VALUE, GRAY_BYTE_OFFSET(%ecx)
   movl (%rcx,%rax,1), %eax
   jnz 3f
4:
   andb    $0xf,rINSTbl                   # rINST <- A
   SET_VREG_OBJECT %eax, rINSTq            # fp[A] <- value
   ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
2:
   EXPORT_PC
   movq rSELF:THREAD_SELF_OFFSET, %rdi
   movq 0(%rsp), %rsi
   movq rPC, %rdx
   movq $0, %rcx
   call nterp_get_instance_field_offset
   testl %eax, %eax
   jns 1b
   // For volatile fields, we return a negative offset. Remove the sign
   // and no need for any barrier thanks to the memory model.
   negl %eax
   jmp 1b
3:
   // reg00 is eax
   call art_quick_read_barrier_mark_reg00
   jmp 4b

NterpPutObjectStaticField:
   GET_VREG %ebp, rINSTq
   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE %rax, 2f
1:
   movl ART_FIELD_OFFSET_OFFSET(%rax), %edx
   movl ART_FIELD_DECLARING_CLASS_OFFSET(%rax), %eax
   cmpq $0, rSELF:THREAD_READ_BARRIER_MARK_REG00_OFFSET
   jne 3f
5:
   movl %ebp, (%eax, %edx, 1)
   testl %ebp, %ebp
   je 4f
   movq rSELF:THREAD_CARD_TABLE_OFFSET, %rcx
   shrq $CARD_TABLE_CARD_SHIFT, %rax
   movb %cl, (%rax, %rcx, 1)
4:
   ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
2:
   EXPORT_PC
   movq rSELF:THREAD_SELF_OFFSET, %rdi
   movq 0(%rsp), %rsi
   movq rPC, %rdx
   movq %rbp, %rcx
   call nterp_get_static_field
   // Reload the value as it may have moved.
   GET_VREG %ebp, rINSTq
   testq MACRO_LITERAL(1), %rax
   je 1b
   CLEAR_VOLATILE_MARKER %rax
   movl ART_FIELD_OFFSET_OFFSET(%rax), %edx
   movl ART_FIELD_DECLARING_CLASS_OFFSET(%rax), %eax
   cmpq $0, rSELF:THREAD_READ_BARRIER_MARK_REG00_OFFSET
   jne 7f
6:
   movl %ebp, (%eax, %edx, 1)
   testl %ebp, %ebp
   je 8f
   movq rSELF:THREAD_CARD_TABLE_OFFSET, %rcx
   shrq $CARD_TABLE_CARD_SHIFT, %rax
   movb %cl, (%rax, %rcx, 1)
8:
   lock addl $0, (%rsp)
   ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
3:
   call art_quick_read_barrier_mark_reg00
   jmp 5b
7:
   call art_quick_read_barrier_mark_reg00
   jmp 6b

NterpGetObjectStaticField:
   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE %rax, 2f
1:
   movl ART_FIELD_OFFSET_OFFSET(%rax), %edx
   movl ART_FIELD_DECLARING_CLASS_OFFSET(%rax), %eax
   cmpq $0, rSELF:THREAD_READ_BARRIER_MARK_REG00_OFFSET
   jne 5f
6:
   testb $READ_BARRIER_TEST_VALUE, GRAY_BYTE_OFFSET(%eax)
   movl (%eax, %edx, 1), %eax
   jnz 3f
4:
   SET_VREG_OBJECT %eax, rINSTq            # fp[A] <- value
   ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
2:
   EXPORT_PC
   movq rSELF:THREAD_SELF_OFFSET, %rdi
   movq 0(%rsp), %rsi
   movq rPC, %rdx
   movq $0, %rcx
   call nterp_get_static_field
   andq $-2, %rax
   jmp 1b
3:
   call art_quick_read_barrier_mark_reg00
   jmp 4b
5:
   call art_quick_read_barrier_mark_reg00
   jmp 6b

NterpGetBooleanStaticField:
  OP_SGET load="movsbl", wide=0

NterpGetByteStaticField:
  OP_SGET load="movsbl", wide=0

NterpGetCharStaticField:
  OP_SGET load="movzwl", wide=0

NterpGetShortStaticField:
  OP_SGET load="movswl", wide=0

NterpGetWideStaticField:
  OP_SGET load="movq", wide=1

NterpGetIntStaticField:
  OP_SGET load="movl", wide=0

NterpPutStaticField:
  OP_SPUT rINST_reg=rINST, store="movl", wide=0

NterpPutBooleanStaticField:
NterpPutByteStaticField:
  OP_SPUT rINST_reg=rINSTbl, store="movb", wide=0

NterpPutCharStaticField:
NterpPutShortStaticField:
  OP_SPUT rINST_reg=rINSTw, store="movw", wide=0

NterpPutWideStaticField:
  OP_SPUT rINST_reg=rINSTq, store="movq", wide=1

NterpPutInstanceField:
  OP_IPUT rINST_reg=rINST, store="movl", wide=0

NterpPutBooleanInstanceField:
NterpPutByteInstanceField:
  OP_IPUT rINST_reg=rINSTbl, store="movb", wide=0

NterpPutCharInstanceField:
NterpPutShortInstanceField:
  OP_IPUT rINST_reg=rINSTw, store="movw", wide=0

NterpPutWideInstanceField:
  OP_IPUT rINST_reg=rINSTq, store="movq", wide=1

NterpGetBooleanInstanceField:
  OP_IGET load="movzbl", wide=0

NterpGetByteInstanceField:
  OP_IGET load="movsbl", wide=0

NterpGetCharInstanceField:
  OP_IGET load="movzwl", wide=0

NterpGetShortInstanceField:
  OP_IGET load="movswl", wide=0

NterpGetWideInstanceField:
  OP_IGET load="movq", wide=1

NterpGetInstanceField:
  OP_IGET load="movl", wide=0

NterpInstanceOf:
    /* instance-of vA, vB, class@CCCC */
   // Fast-path which gets the class from thread-local cache.
   EXPORT_PC
   FETCH_FROM_THREAD_CACHE %rsi, 2f
   cmpq $0, rSELF:THREAD_READ_BARRIER_MARK_REG00_OFFSET
   jne 5f
1:
   movzbl  rINSTbl,%edi
   sarl    $4,%edi                          # edi<- B
   GET_VREG %edi %rdi                        # edi<- vB (object)
   andb    $0xf,rINSTbl                     # rINST<- A
   testl %edi, %edi
   je 3f
   call art_quick_instance_of
   SET_VREG %eax, rINSTq            # fp[A] <- value
4:
   ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
3:
   SET_VREG %edi, rINSTq            # fp[A] <-0
   jmp 4b
2:
   movq rSELF:THREAD_SELF_OFFSET, %rdi
   movq 0(%rsp), %rsi
   movq rPC, %rdx
   call nterp_get_class_or_allocate_object
   movq %rax, %rsi
   jmp 1b
5:
   // 06 is %rsi
   call art_quick_read_barrier_mark_reg06
   jmp 1b

NterpCheckCast:
   // Fast-path which gets the class from thread-local cache.
   EXPORT_PC
   FETCH_FROM_THREAD_CACHE %rsi, 3f
   cmpq $0, rSELF:THREAD_READ_BARRIER_MARK_REG00_OFFSET
   jne 4f
1:
   GET_VREG %edi, rINSTq
   testl %edi, %edi
   je 2f
   call art_quick_check_instance_of
2:
   ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
3:
   movq rSELF:THREAD_SELF_OFFSET, %rdi
   movq 0(%rsp), %rsi
   movq rPC, %rdx
   call nterp_get_class_or_allocate_object
   movq %rax, %rsi
   jmp 1b
4:
   // 06 is %rsi
   call art_quick_read_barrier_mark_reg06
   jmp 1b

NterpHandleHotnessOverflow:
    leaq (rPC, rINSTq, 2), %rsi
    movq rFP, %rdx
    call nterp_hot_method
    testq %rax, %rax
    jne 1f
    leaq    (rPC, rINSTq, 2), rPC
    FETCH_INST
    GOTO_NEXT
1:
    // Drop the current frame.
    movq -8(rREFS), %rsp
    CFI_DEF_CFA(rsp, CALLEE_SAVES_SIZE)

    // Setup the new frame
    movq OSR_DATA_FRAME_SIZE(%rax), %rcx
    // Given stack size contains all callee saved registers, remove them.
    subq $CALLEE_SAVES_SIZE, %rcx

    // Remember CFA.
    movq %rsp, %rbp
    CFI_DEF_CFA_REGISTER(rbp)

    subq %rcx, %rsp
    movq %rsp, %rdi               // rdi := beginning of stack
    leaq OSR_DATA_MEMORY(%rax), %rsi  // rsi := memory to copy
    rep movsb                     // while (rcx--) { *rdi++ = *rsi++ }

    // Fetch the native PC to jump to and save it in a callee-save register.
    movq OSR_DATA_NATIVE_PC(%rax), %rbx

    // Free the memory holding OSR Data.
    movq %rax, %rdi
    call free

    // Jump to the compiled code.
    jmp *%rbx

NterpHandleInvokeInterfaceOnObjectMethodRange:
   shrl $16, %eax
   movq MIRROR_CLASS_VTABLE_OFFSET_64(%edx, %eax, 8), %rdi
   jmp NterpCommonInvokeInstanceRange

NterpHandleInvokeInterfaceOnObjectMethod:
   shrl $16, %eax
   movq MIRROR_CLASS_VTABLE_OFFSET_64(%edx, %eax, 8), %rdi
   jmp NterpCommonInvokeInstance

// This is the logical end of ExecuteNterpImpl, where the frame info applies.
// EndExecuteNterpImpl includes the methods below as we want the runtime to
// see them as part of the Nterp PCs.
.cfi_endproc

nterp_to_nterp_static_non_range:
    .cfi_startproc
    .cfi_def_cfa rsp, 8
    SETUP_STACK_FOR_INVOKE
    SETUP_NON_RANGE_ARGUMENTS_AND_EXECUTE is_static=1, is_string_init=0
    .cfi_endproc

nterp_to_nterp_string_init_non_range:
    .cfi_startproc
    .cfi_def_cfa rsp, 8
    SETUP_STACK_FOR_INVOKE
    SETUP_NON_RANGE_ARGUMENTS_AND_EXECUTE is_static=0, is_string_init=1
    .cfi_endproc

nterp_to_nterp_instance_non_range:
    .cfi_startproc
    .cfi_def_cfa rsp, 8
    SETUP_STACK_FOR_INVOKE
    SETUP_NON_RANGE_ARGUMENTS_AND_EXECUTE is_static=0, is_string_init=0
    .cfi_endproc

nterp_to_nterp_static_range:
    .cfi_startproc
    .cfi_def_cfa rsp, 8
    SETUP_STACK_FOR_INVOKE
    SETUP_RANGE_ARGUMENTS_AND_EXECUTE is_static=1
    .cfi_endproc

nterp_to_nterp_instance_range:
    .cfi_startproc
    .cfi_def_cfa rsp, 8
    SETUP_STACK_FOR_INVOKE
    SETUP_RANGE_ARGUMENTS_AND_EXECUTE is_static=0
    .cfi_endproc

nterp_to_nterp_string_init_range:
    .cfi_startproc
    .cfi_def_cfa rsp, 8
    SETUP_STACK_FOR_INVOKE
    SETUP_RANGE_ARGUMENTS_AND_EXECUTE is_static=0, is_string_init=1
    .cfi_endproc

END nterp_helper

// This is the end of PCs contained by the OatQuickMethodHeader created for the interpreter
// entry point.
    FUNCTION_TYPE(EndExecuteNterpImpl)
    ASM_HIDDEN SYMBOL(EndExecuteNterpImpl)
    .global SYMBOL(EndExecuteNterpImpl)
SYMBOL(EndExecuteNterpImpl):

// Entrypoints into runtime.
NTERP_TRAMPOLINE nterp_get_static_field, NterpGetStaticField
NTERP_TRAMPOLINE nterp_get_instance_field_offset, NterpGetInstanceFieldOffset
NTERP_TRAMPOLINE nterp_filled_new_array, NterpFilledNewArray
NTERP_TRAMPOLINE nterp_filled_new_array_range, NterpFilledNewArrayRange
NTERP_TRAMPOLINE nterp_get_class_or_allocate_object, NterpGetClassOrAllocateObject
NTERP_TRAMPOLINE nterp_get_method, NterpGetMethod
NTERP_TRAMPOLINE nterp_hot_method, NterpHotMethod
NTERP_TRAMPOLINE nterp_load_object, NterpLoadObject

// gen_mterp.py will inline the following definitions
// within [ExecuteNterpImpl, EndExecuteNterpImpl).
