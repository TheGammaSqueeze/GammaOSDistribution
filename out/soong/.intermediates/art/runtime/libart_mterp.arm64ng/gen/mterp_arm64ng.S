/* DO NOT EDIT: This file was generated by gen-mterp.py. */
/*
 * Copyright (C) 2020 The Android Open Source Project
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/*
 * This is a #include, not a %include, because we want the C pre-processor
 * to expand the macros into assembler assignment statements.
 */
#include "asm_support.h"
#include "arch/arm64/asm_support_arm64.S"

/**
 * ARM64 Runtime register usage conventions.
 *
 *   r0     : w0 is 32-bit return register and x0 is 64-bit.
 *   r0-r7  : Argument registers.
 *   r8-r15 : Caller save registers (used as temporary registers).
 *   r16-r17: Also known as ip0-ip1, respectively. Used as scratch registers by
 *            the linker, by the trampolines and other stubs (the compiler uses
 *            these as temporary registers).
 *   r18    : Reserved for platform (SCS, shadow call stack)
 *   r19    : Pointer to thread-local storage.
 *   r20-r29: Callee save registers.
 *   r30    : (lr) is reserved (the link register).
 *   rsp    : (sp) is reserved (the stack pointer).
 *   rzr    : (zr) is reserved (the zero register).
 *
 *   Floating-point registers
 *   v0-v31
 *
 *   v0     : s0 is return register for singles (32-bit) and d0 for doubles (64-bit).
 *            This is analogous to the C/C++ (hard-float) calling convention.
 *   v0-v7  : Floating-point argument registers in both Dalvik and C/C++ conventions.
 *            Also used as temporary and codegen scratch registers.
 *
 *   v0-v7 and v16-v31 : Caller save registers (used as temporary registers).
 *   v8-v15 : bottom 64-bits preserved across C calls (d8-d15 are preserved).
 *
 *   v16-v31: Used as codegen temp/scratch.
 *   v8-v15 : Can be used for promotion.
 *
 *   Must maintain 16-byte stack alignment.
 *
 * Nterp notes:
 *
 * The following registers have fixed assignments:
 *
 *   reg nick      purpose
 *   x19  xSELF     self (Thread) pointer
 *   x20  wMR       marking register
 *   x29  xFP       interpreted frame pointer, used for accessing locals and args
 *   x22  xPC       interpreted program counter, used for fetching instructions
 *   x23  xINST     first 16-bit code unit of current instruction
 *   x24  xIBASE    interpreted instruction base pointer, used for computed goto
 *   x25  xREFS     base of object references of dex registers.
 *   x16  ip        scratch reg
 *   x17  ip2       scratch reg (used by macros)
 *
 * Macros are provided for common operations.  They MUST NOT alter unspecified registers or
 * condition codes.
*/

/* single-purpose registers, given names for clarity */
#define xSELF    x19
#define CFI_DEX  22 // DWARF register number of the register holding dex-pc (xPC).
#define CFI_TMP  0  // DWARF register number of the first argument register (r0).
#define xPC      x22
#define xINST    x23
#define wINST    w23
#define xIBASE   x24
#define xREFS    x25
#define CFI_REFS 25
#define ip       x16
#define ip2      x17
#define wip      w16
#define wip2     w17

// To avoid putting ifdefs arond the use of wMR, make sure it's defined.
// IsNterpSupported returns false for configurations that don't have wMR (typically CMS).
#ifndef wMR
#define wMR w20
#endif

// Temporary registers while setting up a frame.
#define xNEW_FP   x26
#define xNEW_REFS x27
#define CFI_NEW_REFS 27

// +8 for the ArtMethod of the caller.
#define OFFSET_TO_FIRST_ARGUMENT_IN_STACK (CALLEE_SAVES_SIZE + 8)

/*
 * Fetch the next instruction from xPC into wINST.  Does not advance xPC.
 */
.macro FETCH_INST
    ldrh    wINST, [xPC]
.endm

/*
 * Fetch the next instruction from the specified offset.  Advances xPC
 * to point to the next instruction.  "count" is in 16-bit code units.
 *
 * Because of the limited size of immediate constants on ARM, this is only
 * suitable for small forward movements (i.e. don't try to implement "goto"
 * with this).
 *
 * This must come AFTER anything that can throw an exception, or the
 * exception catch may miss.  (This also implies that it must come after
 * EXPORT_PC.)
 */
.macro FETCH_ADVANCE_INST count
    ldrh    wINST, [xPC, #((\count)*2)]!
.endm

/*
 * Similar to FETCH_ADVANCE_INST, but does not update xPC.  Used to load
 * xINST ahead of possible exception point.  Be sure to manually advance xPC
 * later.
 */
.macro PREFETCH_INST count
    ldrh    wINST, [xPC, #((\count)*2)]
.endm

/* Advance xPC by some number of code units. */
.macro ADVANCE count
  add  xPC, xPC, #((\count)*2)
.endm

/*
 * Fetch a half-word code unit from an offset past the current PC.  The
 * "count" value is in 16-bit code units.  Does not advance xPC.
 *
 * The "_S" variant works the same but treats the value as signed.
 */
.macro FETCH reg, count
    ldrh    \reg, [xPC, #((\count)*2)]
.endm

.macro FETCH_S reg, count
    ldrsh   \reg, [xPC, #((\count)*2)]
.endm

/*
 * Fetch one byte from an offset past the current PC.  Pass in the same
 * "count" as you would for FETCH, and an additional 0/1 indicating which
 * byte of the halfword you want (lo/hi).
 */
.macro FETCH_B reg, count, byte
    ldrb     \reg, [xPC, #((\count)*2+(\byte))]
.endm

/*
 * Put the instruction's opcode field into the specified register.
 */
.macro GET_INST_OPCODE reg
    and     \reg, xINST, #255
.endm

/*
 * Begin executing the opcode in _reg.  Clobbers reg
 */

.macro GOTO_OPCODE reg
    add     \reg, xIBASE, \reg, lsl #MTERP_HANDLER_SIZE_LOG2
    br      \reg
.endm

/*
 * Get/set the 32-bit value from a Dalvik register.
 */
.macro GET_VREG reg, vreg
    ldr     \reg, [xFP, \vreg, uxtw #2]
.endm
.macro GET_VREG_OBJECT reg, vreg
    ldr     \reg, [xREFS, \vreg, uxtw #2]
.endm
.macro SET_VREG reg, vreg
    str     \reg, [xFP, \vreg, uxtw #2]
    str     wzr, [xREFS, \vreg, uxtw #2]
.endm
.macro SET_VREG_OBJECT reg, vreg
    str     \reg, [xFP, \vreg, uxtw #2]
    str     \reg, [xREFS, \vreg, uxtw #2]
.endm
.macro SET_VREG_FLOAT reg, vreg
    str     \reg, [xFP, \vreg, uxtw #2]
    str     wzr, [xREFS, \vreg, uxtw #2]
.endm

/*
 * Get/set the 64-bit value from a Dalvik register.
 */
.macro GET_VREG_WIDE reg, vreg
    add     ip2, xFP, \vreg, uxtw #2
    ldr     \reg, [ip2]
.endm
.macro SET_VREG_WIDE reg, vreg
    add     ip2, xFP, \vreg, uxtw #2
    str     \reg, [ip2]
    add     ip2, xREFS, \vreg, uxtw #2
    str     xzr, [ip2]
.endm
.macro GET_VREG_DOUBLE reg, vreg
    add     ip2, xFP, \vreg, uxtw #2
    ldr     \reg, [ip2]
.endm
.macro SET_VREG_DOUBLE reg, vreg
    add     ip2, xFP, \vreg, uxtw #2
    str     \reg, [ip2]
    add     ip2, xREFS, \vreg, uxtw #2
    str     xzr, [ip2]
.endm

/*
 * Get the 32-bit value from a Dalvik register and sign-extend to 64-bit.
 * Used to avoid an extra instruction in int-to-long.
 */
.macro GET_VREG_S reg, vreg
    ldrsw   \reg, [xFP, \vreg, uxtw #2]
.endm

// An assembly entry that has a OatQuickMethodHeader prefix.
.macro OAT_ENTRY name, end
    .type \name, #function
    .hidden \name
    .global \name
    .balign 16
    // Padding of 3 * 8 bytes to get 16 bytes alignment of code entry.
    .long 0
    .long 0
    .long 0
    // OatQuickMethodHeader. Note that the top two bits must be clear.
    .long (\end - \name)
\name:
.endm

.macro SIZE name
    .size \name, .-\name
.endm

.macro NAME_START name
    .type \name, #function
    .hidden \name  // Hide this as a global symbol, so we do not incur plt calls.
    .global \name
    /* Cache alignment for function entry */
    .balign 16
\name:
.endm

.macro NAME_END name
  SIZE \name
.endm

// Macro for defining entrypoints into runtime. We don't need to save registers
// (we're not holding references there), but there is no
// kDontSave runtime method. So just use the kSaveRefsOnly runtime method.
.macro NTERP_TRAMPOLINE name, helper
ENTRY \name
  SETUP_SAVE_REFS_ONLY_FRAME
  bl \helper
  RESTORE_SAVE_REFS_ONLY_FRAME
  REFRESH_MARKING_REGISTER
  RETURN_OR_DELIVER_PENDING_EXCEPTION
END \name
.endm

.macro CLEAR_STATIC_VOLATILE_MARKER reg
  and \reg, \reg, #-2
.endm

.macro CLEAR_INSTANCE_VOLATILE_MARKER reg
  neg \reg, \reg
.endm

.macro EXPORT_PC
    str    xPC, [xREFS, #-16]
.endm

.macro BRANCH
    // Update method counter and do a suspend check if the branch is negative.
    tbnz wINST, #31, 2f
1:
    add     xPC, xPC, wINST, sxtw #1    // update xPC
    FETCH wINST, 0                      // load wINST
    GET_INST_OPCODE ip                  // extract opcode from wINST
    GOTO_OPCODE ip                      // jump to next instruction
2:
    ldr x0, [sp]
    ldrh w2, [x0, #ART_METHOD_HOTNESS_COUNT_OFFSET]
    add x2, x2, #1
    and w2, w2, #NTERP_HOTNESS_MASK
    strh w2, [x0, #ART_METHOD_HOTNESS_COUNT_OFFSET]
    // If the counter overflows, handle this in the runtime.
    cbz w2, NterpHandleHotnessOverflow
    // Otherwise, do a suspend check.
    ldr x0, [xSELF, #THREAD_FLAGS_OFFSET]
    ands x0, x0, #THREAD_SUSPEND_OR_CHECKPOINT_REQUEST
    b.eq 1b
    EXPORT_PC
    bl    art_quick_test_suspend
    b 1b
.endm

// Uses x12, x13, and x14 as temporaries.
.macro FETCH_CODE_ITEM_INFO code_item, registers, outs, ins, load_ins
    tbz \code_item, #0, 4f
    and \code_item, \code_item, #-2 // Remove the extra bit that marks it's a compact dex file
    ldrh w13, [\code_item, #COMPACT_CODE_ITEM_FIELDS_OFFSET]
    ubfx \registers, w13, #COMPACT_CODE_ITEM_REGISTERS_SIZE_SHIFT, #4
    ubfx \outs, w13, #COMPACT_CODE_ITEM_OUTS_SIZE_SHIFT, #4
    .if \load_ins
    ubfx \ins, w13, #COMPACT_CODE_ITEM_INS_SIZE_SHIFT, #4
    .else
    ubfx w14, w13, #COMPACT_CODE_ITEM_INS_SIZE_SHIFT, #4
    add \registers, \registers, w14
    .endif
    ldrh w13, [\code_item, #COMPACT_CODE_ITEM_FLAGS_OFFSET]
    tst w13, #COMPACT_CODE_ITEM_REGISTERS_INS_OUTS_FLAGS
    b.eq 3f
    sub x14, \code_item, #4
    tst w13, #COMPACT_CODE_ITEM_INSNS_FLAG
    csel x14, x14, \code_item, ne

    tbz w13, #COMPACT_CODE_ITEM_REGISTERS_BIT, 1f
    ldrh w12, [x14, #-2]!
    add \registers, \registers, w12
1:
    tbz w13, #COMPACT_CODE_ITEM_INS_BIT, 2f
    ldrh w12, [x14, #-2]!
    .if \load_ins
    add \ins, \ins, w12
    .else
    add \registers, \registers, w12
    .endif
2:
    tbz w13, #COMPACT_CODE_ITEM_OUTS_BIT, 3f
    ldrh w12, [x14, #-2]!
    add \outs, \outs, w12
3:
    .if \load_ins
    add \registers, \registers, \ins
    .endif
    add \code_item, \code_item, #COMPACT_CODE_ITEM_INSNS_OFFSET
    b 5f
4:
    // Fetch dex register size.
    ldrh \registers, [\code_item, #CODE_ITEM_REGISTERS_SIZE_OFFSET]
    // Fetch outs size.
    ldrh \outs, [\code_item, #CODE_ITEM_OUTS_SIZE_OFFSET]
    .if \load_ins
    ldrh \ins, [\code_item, #CODE_ITEM_INS_SIZE_OFFSET]
    .endif
    add \code_item, \code_item, #CODE_ITEM_INSNS_OFFSET
5:
.endm

// Setup the stack to start executing the method. Expects:
// - x0 to contain the ArtMethod
//
// Outputs
// - ip contains the dex registers size
// - x28 contains the old stack pointer.
// - \code_item is replaced with a pointer to the instructions
// - if load_ins is 1, w15 contains the ins
//
// Uses ip, ip2, x12, x13, x14 as temporaries.
.macro SETUP_STACK_FRAME code_item, refs, fp, cfi_refs, load_ins
    FETCH_CODE_ITEM_INFO \code_item, wip, wip2, w15, \load_ins

    // Compute required frame size: ((2 * ip) + ip2) * 4 + 24
    // 24 is for saving the previous frame, pc, and method being executed.
    add x14, ip, ip
    add x14, x14, ip2
    lsl x14, x14, #2
    add x14, x14, #24

    // Compute new stack pointer in x14
    sub x14, sp, x14
    // Alignment
    and x14, x14, #-16

    // Set reference and dex registers, align to pointer size for previous frame and dex pc.
    add \refs, x14, ip2, lsl #2
    add \refs, \refs, 28
    and \refs, \refs, #(-__SIZEOF_POINTER__)
    add \fp, \refs, ip, lsl #2

    // Now setup the stack pointer.
    mov x28, sp
    .cfi_def_cfa_register x28
    mov sp, x14
    str x28, [\refs, #-8]
    CFI_DEF_CFA_BREG_PLUS_UCONST \cfi_refs, -8, CALLEE_SAVES_SIZE

    // Put nulls in reference frame.
    cbz ip, 2f
    mov ip2, \refs
1:
    str xzr, [ip2], #8  // May clear vreg[0].
    cmp ip2, \fp
    b.lo 1b
2:
    // Save the ArtMethod.
    str x0, [sp]
.endm

// Increase method hotness and do suspend check before starting executing the method.
.macro START_EXECUTING_INSTRUCTIONS
    ldr x0, [sp]
    ldrh w2, [x0, #ART_METHOD_HOTNESS_COUNT_OFFSET]
    add x2, x2, #1
    and w2, w2, #NTERP_HOTNESS_MASK
    strh w2, [x0, #ART_METHOD_HOTNESS_COUNT_OFFSET]
    // If the counter overflows, handle this in the runtime.
    cbz w2, 2f
    ldr x0, [xSELF, #THREAD_FLAGS_OFFSET]
    tst x0, #THREAD_SUSPEND_OR_CHECKPOINT_REQUEST
    b.ne 3f
1:
    FETCH_INST
    GET_INST_OPCODE ip
    GOTO_OPCODE ip
2:
    mov x1, xzr
    mov x2, xFP
    bl nterp_hot_method
    b 1b
3:
    EXPORT_PC
    bl art_quick_test_suspend
    b 1b
.endm

.macro SPILL_ALL_CALLEE_SAVES
    INCREASE_FRAME CALLEE_SAVES_SIZE
    // Note: we technically don't need to save x19 and x20,
    // but the runtime will expect those values to be there when unwinding
    // (see Arm64Context::DoLongJump checking for the thread register).
    SAVE_ALL_CALLEE_SAVES 0
.endm

.macro RESTORE_ALL_CALLEE_SAVES
    // FP callee-saves
    ldp d8, d9, [sp, #0]
    ldp d10, d11, [sp, #16]
    ldp d12, d13, [sp, #32]
    ldp d14, d15, [sp, #48]

    // GP callee-saves.
    // No need to restore x19 (it's always the thread), and
    // don't restore x20 (the marking register) as it may have been updated.
    RESTORE_TWO_REGS x21, x22, 80
    RESTORE_TWO_REGS x23, x24, 96
    RESTORE_TWO_REGS x25, x26, 112
    RESTORE_TWO_REGS x27, x28, 128
    RESTORE_TWO_REGS x29, lr, 144

    DECREASE_FRAME CALLEE_SAVES_SIZE
.endm

.macro SPILL_ALL_ARGUMENTS
    stp x0, x1, [sp, #-128]!
    stp x2, x3, [sp, #16]
    stp x4, x5, [sp, #32]
    stp x6, x7, [sp, #48]
    stp d0, d1, [sp, #64]
    stp d2, d3, [sp, #80]
    stp d4, d5, [sp, #96]
    stp d6, d7, [sp, #112]
.endm

.macro RESTORE_ALL_ARGUMENTS
    ldp x2, x3, [sp, #16]
    ldp x4, x5, [sp, #32]
    ldp x6, x7, [sp, #48]
    ldp d0, d1, [sp, #64]
    ldp d2, d3, [sp, #80]
    ldp d4, d5, [sp, #96]
    ldp d6, d7, [sp, #112]
    ldp x0, x1, [sp], #128
.endm

// Helper to setup the stack after doing a nterp to nterp call. This will setup:
// - xNEW_FP: the new pointer to dex registers
// - xNEW_REFS: the new pointer to references
// - xPC: the new PC pointer to execute
// - x2: value in instruction to decode the number of arguments.
// - x3: first dex register
// - x4: top of dex register array
//
// The method expects:
// - x0 to contain the ArtMethod
// - x8 to contain the code item
.macro SETUP_STACK_FOR_INVOKE
   // We do the same stack overflow check as the compiler. See CanMethodUseNterp
   // in how we limit the maximum nterp frame size.
   sub x16, sp, #STACK_OVERFLOW_RESERVED_BYTES
   ldr wzr, [x16]

   // Spill all callee saves to have a consistent stack frame whether we
   // are called by compiled code or nterp.
   SPILL_ALL_CALLEE_SAVES

   // Setup the frame.
   SETUP_STACK_FRAME x8, xNEW_REFS, xNEW_FP, CFI_NEW_REFS, load_ins=0
   // Make x4 point to the top of the dex register array.
   add x4, xNEW_FP, ip, uxtx #2

   // Fetch instruction information before replacing xPC.
   // TODO: move this down to the method that uses it, fetching it directly from wINST.
   FETCH_B w2, 0, 1
   // TODO: we could avoid this as instance invokes already fetch it.
   FETCH w3, 2

   // Set the dex pc pointer.
   mov xPC, x8
   CFI_DEFINE_DEX_PC_WITH_OFFSET(CFI_TMP, CFI_DEX, 0)
.endm

// Setup arguments based on a non-range nterp to nterp call, and start executing
// the method. We expect:
// - xNEW_FP: the new pointer to dex registers
// - xNEW_REFS: the new pointer to references
// - xPC: the new PC pointer to execute
// - x2: number of arguments (bits 4-7), 5th argument if any (bits 0-3)
// - x3: first dex register
// - x4: top of dex register array
// - x1: receiver if non-static.
.macro SETUP_NON_RANGE_ARGUMENTS_AND_EXECUTE is_static=0, is_string_init=0
   // /* op vA, vB, {vC...vG} */
   asr ip2, x2, #4
   cbz ip2, 6f
   mov ip, #-4
   cmp ip2, #2
   b.lt 1f
   b.eq 2f
   cmp ip2, #4
   b.lt 3f
   b.eq 4f

  // We use a decrementing ip to store references relative
  // to xNEW_FP and dex registers relative to x4
  //
  // TODO: We could set up ip as the number of registers (this can be an additional output from
  // SETUP_STACK_FOR_INVOKE) and then just decrement it by one before copying each arg.
  // Maybe even introduce macros NEW_VREG_ADDRESS/NEW_VREG_REF_ADDRESS.
5:
   and         x2, x2, #15
   GET_VREG_OBJECT w5, w2
   str         w5, [xNEW_FP, ip]
   GET_VREG    w5, w2
   str         w5, [x4, ip]
   sub         ip, ip, #4
4:
   asr         x2, x3, #12
   GET_VREG_OBJECT w5, w2
   str         w5, [xNEW_FP, ip]
   GET_VREG    w5, w2
   str         w5, [x4, ip]
   sub         ip, ip, #4
3:
   ubfx        x2, x3, #8, #4
   GET_VREG_OBJECT w5, w2
   str         w5, [xNEW_FP, ip]
   GET_VREG    w5, w2
   str         w5, [x4, ip]
   sub         ip, ip, #4
2:
   ubfx        x2, x3, #4, #4
   GET_VREG_OBJECT w5, w2
   str         w5, [xNEW_FP, ip]
   GET_VREG    w5, w2
   str         w5, [x4, ip]
   .if !\is_string_init
   sub         ip, ip, #4
   .endif
1:
   .if \is_string_init
   // Ignore the first argument
   .elseif \is_static
   and         x2, x3, #0xf
   GET_VREG_OBJECT w5, w2
   str         w5, [xNEW_FP, ip]
   GET_VREG    w5, w2
   str         w5, [x4, ip]
   .else
   str         w1, [xNEW_FP, ip]
   str         w1, [x4, ip]
   .endif

6:
   // Start executing the method.
   mov xFP, xNEW_FP
   mov xREFS, xNEW_REFS
   CFI_DEF_CFA_BREG_PLUS_UCONST CFI_REFS, -8, CALLEE_SAVES_SIZE
   START_EXECUTING_INSTRUCTIONS
.endm

// Setup arguments based on a range nterp to nterp call, and start executing
// the method.
// - xNEW_FP: the new pointer to dex registers
// - xNEW_REFS: the new pointer to references
// - xPC: the new PC pointer to execute
// - x2: number of arguments
// - x3: first dex register
// - x4: top of dex register array
// - x1: receiver if non-static.
//
// Uses ip, ip2, x5, x6 as temporaries.
.macro SETUP_RANGE_ARGUMENTS_AND_EXECUTE is_static=0, is_string_init=0
   mov ip, #-4
   .if \is_string_init
   // Ignore the first argument
   sub x2, x2, #1
   add x3, x3, #1
   .elseif !\is_static
   sub x2, x2, #1
   add x3, x3, #1
   .endif

   cbz x2, 2f
   add ip2, xREFS, x3, lsl #2  // pointer to first argument in reference array
   add ip2, ip2, x2, lsl #2    // pointer to last argument in reference array
   add x5, xFP, x3, lsl #2     // pointer to first argument in register array
   add x6, x5, x2, lsl #2      // pointer to last argument in register array
1:
   ldr  w7, [ip2, #-4]!
   str  w7, [xNEW_FP, ip]
   sub  x2, x2, 1
   ldr  w7, [x6, #-4]!
   str  w7, [x4, ip]
   sub ip, ip, 4
   cbnz x2, 1b
2:
   .if \is_string_init
   // Ignore first argument
   .elseif !\is_static
   str w1, [xNEW_FP, ip]
   str w1, [x4, ip]
   .endif
   mov xFP, xNEW_FP
   mov xREFS, xNEW_REFS
   CFI_DEF_CFA_BREG_PLUS_UCONST CFI_REFS, -8, CALLEE_SAVES_SIZE
   START_EXECUTING_INSTRUCTIONS
.endm

.macro GET_SHORTY dest, is_interface, is_polymorphic, is_custom
   stp x0, x1, [sp, #-16]!
   .if \is_polymorphic
   ldr x0, [sp, #16]
   mov x1, xPC
   bl NterpGetShortyFromInvokePolymorphic
   .elseif \is_custom
   ldr x0, [sp, #16]
   mov x1, xPC
   bl NterpGetShortyFromInvokeCustom
   .elseif \is_interface
   ldr x0, [sp, #16]
   FETCH w1, 1
   bl NterpGetShortyFromMethodId
   .else
   bl NterpGetShorty
   .endif
   mov \dest, x0
   ldp x0, x1, [sp], #16
.endm

.macro GET_SHORTY_SLOW_PATH dest, is_interface
   // Save all registers that can hold arguments in the fast path.
   stp x0, x1, [sp, #-32]!
   str w2, [sp, #16]
   str s0, [sp, #20]
   .if \is_interface
   ldr x0, [sp, #32]
   FETCH w1, 1
   bl NterpGetShortyFromMethodId
   .else
   bl NterpGetShorty
   .endif
   mov \dest, x0
   ldr w2, [sp, #16]
   ldr s0, [sp, #20]
   ldp x0, x1, [sp], #32
.endm

// Input:  x0 contains the ArtMethod
// Output: x8 contains the code item
.macro GET_CODE_ITEM
   ldr x8, [x0, #ART_METHOD_DATA_OFFSET_64]
.endm

.macro DO_ENTRY_POINT_CHECK call_compiled_code
   // On entry, the method is x0, the instance is x1
   adr x2, ExecuteNterpImpl
   ldr x3, [x0, #ART_METHOD_QUICK_CODE_OFFSET_64]
   cmp x2, x3
   b.ne  \call_compiled_code
.endm

.macro UPDATE_REGISTERS_FOR_STRING_INIT old_value, new_value
   mov wip, wzr
1:
   GET_VREG_OBJECT wip2, wip
   cmp wip2, \old_value
   b.ne 2f
   SET_VREG_OBJECT \new_value, wip
2:
   add wip, wip, #1
   add ip2, xREFS, wip, uxtw #2
   cmp ip2, xFP
   b.ne 1b
.endm

// Puts the next floating point argument into the expected register,
// fetching values based on a non-range invoke.
// Uses ip and ip2.
.macro LOOP_OVER_SHORTY_LOADING_FPS dreg, sreg, inst, shorty, arg_index, finished
1: // LOOP
    ldrb wip, [\shorty], #1         // Load next character in shorty, and increment.
    cbz wip, \finished              // if (wip == '\0') goto finished
    cmp wip, #68                    // if (wip == 'D') goto FOUND_DOUBLE
    b.eq 2f
    cmp wip, #70                    // if (wip == 'F') goto FOUND_FLOAT
    b.eq 3f
    lsr \inst, \inst, #4
    add \arg_index, \arg_index, #1
    //  Handle extra argument in arg array taken by a long.
    cmp wip, #74                   // if (wip != 'J') goto LOOP
    b.ne 1b
    lsr \inst, \inst, #4
    add \arg_index, \arg_index, #1
    b 1b                        // goto LOOP
2:  // FOUND_DOUBLE
    and ip, \inst, #0xf
    GET_VREG wip, wip
    lsr \inst, \inst, #4
    add \arg_index, \arg_index, #1
    cmp \arg_index, #4
    b.eq 5f
    and ip2, \inst, #0xf
    lsr \inst, \inst, #4
    add \arg_index, \arg_index, #1
    b 6f
5:
    // TODO: Extract from wINST here and below? (Requires using a different register
    // in the COMMON_INVOKE_NON_RANGE.)
    FETCH_B wip2, 0, 1
    and wip2, wip2, #0xf
6:
    GET_VREG wip2, wip2
    add ip, ip, ip2, lsl #32
    fmov \dreg, ip
    b 4f
3:  // FOUND_FLOAT
    cmp \arg_index, #4
    b.eq 7f
    and ip, \inst, #0xf
    lsr \inst, \inst, #4
    add \arg_index, \arg_index, #1
    b 8f
7:
    FETCH_B wip, 0, 1
    and wip, wip, #0xf
8:
    GET_VREG \sreg, wip
4:
.endm

// Puts the next int/long/object argument in the expected register,
// fetching values based on a non-range invoke.
// Uses ip and ip2.
.macro LOOP_OVER_SHORTY_LOADING_GPRS gpr_reg64, gpr_reg32, inst, shorty, arg_index, finished
1: // LOOP
    ldrb wip, [\shorty], #1         // Load next character in shorty, and increment.
    cbz wip, \finished              // if (wip == '\0') goto finished
    cmp wip, #74                    // if (wip == 'J') goto FOUND_LONG
    b.eq 2f
    cmp wip, #70                    // if (wip == 'F') goto SKIP_FLOAT
    b.eq 3f
    cmp wip, #68                    // if (wip == 'D') goto SKIP_DOUBLE
    b.eq 4f
    cmp \arg_index, #4
    b.eq 7f
    and ip, \inst, #0xf
    lsr \inst, \inst, #4
    add \arg_index, \arg_index, #1
    b 8f
7:
    FETCH_B wip, 0, 1
    and wip, wip, #0xf
8:
    GET_VREG \gpr_reg32, wip
    b 5f
2:  // FOUND_LONG
    and ip, \inst, #0xf
    GET_VREG wip, wip
    lsr \inst, \inst, #4
    add \arg_index, \arg_index, #1
    cmp \arg_index, #4
    b.eq 9f
    and ip2, \inst, #0xf
    lsr \inst, \inst, #4
    add \arg_index, \arg_index, #1
    b 10f
9:
    FETCH_B wip2, 0, 1
    and wip2, wip2, #0xf
10:
    GET_VREG wip2, wip2
    add \gpr_reg64, ip, ip2, lsl #32
    b 5f
3:  // SKIP_FLOAT
    lsr \inst, \inst, #4
    add \arg_index, \arg_index, #1
    b 1b
4:  // SKIP_DOUBLE
    lsr \inst, \inst, #4
    add \arg_index, \arg_index, #1
    cmp \arg_index, #4
    b.eq 1b
    lsr \inst, \inst, #4
    add \arg_index, \arg_index, #1
    b 1b
5:
.endm

.macro SETUP_RETURN_VALUE shorty
   ldrb wip, [\shorty]
   cmp ip, #68       // Test if result type char == 'D'.
   b.eq 1f
   cmp ip, #70       // Test if result type char == 'F'.
   b.ne 2f
   fmov w0, s0
   b 2f
1:
   fmov x0, d0
2:
.endm

.macro COMMON_INVOKE_NON_RANGE is_static=0, is_interface=0, suffix="", is_string_init=0, is_polymorphic=0, is_custom=0
   .if \is_polymorphic
   // We always go to compiled code for polymorphic calls.
   .elseif \is_custom
   // We always go to compiled code for custom calls.
   .else
     DO_ENTRY_POINT_CHECK .Lcall_compiled_code_\suffix
     GET_CODE_ITEM
     .if \is_string_init
     bl nterp_to_nterp_string_init_non_range
     .elseif \is_static
     bl nterp_to_nterp_static_non_range
     .else
     bl nterp_to_nterp_instance_non_range
     .endif
     b .Ldone_return_\suffix
   .endif

.Lcall_compiled_code_\suffix:
   .if \is_polymorphic
   // No fast path for polymorphic calls.
   .elseif \is_custom
   // No fast path for custom calls.
   .elseif \is_string_init
   // No fast path for string.init.
   .else
     ldr wip, [x0, #ART_METHOD_ACCESS_FLAGS_OFFSET]
     tbz wip, #ART_METHOD_NTERP_INVOKE_FAST_PATH_FLAG_BIT, .Lfast_path_with_few_args_\suffix
     FETCH_B wip2, 0, 1
     asr ip, ip2, #4
     .if \is_static
     cbz ip, .Linvoke_fast_path_\suffix
     .else
     cmp ip, #1
     b.eq .Linvoke_fast_path_\suffix
     .endif
     FETCH w8, 2
     cmp ip, #2
     .if \is_static
     b.lt .Lone_arg_fast_path_\suffix
     .endif
     b.eq .Ltwo_args_fast_path_\suffix
     cmp ip, #4
     b.lt .Lthree_args_fast_path_\suffix
     b.eq .Lfour_args_fast_path_\suffix

     and         ip, ip2, #15
     GET_VREG    w5, wip
.Lfour_args_fast_path_\suffix:
     asr         ip, x8, #12
     GET_VREG    w4, wip
.Lthree_args_fast_path_\suffix:
     ubfx        ip, x8, #8, #4
     GET_VREG    w3, wip
.Ltwo_args_fast_path_\suffix:
     ubfx        ip, x8, #4, #4
     GET_VREG    w2, wip
.Lone_arg_fast_path_\suffix:
     .if \is_static
     and         ip, x8, #0xf
     GET_VREG    w1, wip
     .else
     // First argument already in w1.
     .endif
.Linvoke_fast_path_\suffix:
     .if \is_interface
     // Setup hidden argument.
     mov ip2, x26
     .endif
     ldr lr, [x0, #ART_METHOD_QUICK_CODE_OFFSET_64]
     blr lr
     FETCH_ADVANCE_INST 3
     GET_INST_OPCODE ip
     GOTO_OPCODE ip

.Lfast_path_with_few_args_\suffix:
     // Fast path when we have zero or one argument (modulo 'this'). If there
     // is one argument, we can put it in both floating point and core register.
     FETCH_B w2, 0, 1
     .if \is_static
     cmp w2, #(2 << 4)
     .else
     cmp w2, #(3 << 4)
     .endif
     b.ge .Lget_shorty_\suffix
     .if \is_static
     tbz w2, #4, .Linvoke_with_few_args_\suffix
     .else
     tbnz w2, #4, .Linvoke_with_few_args_\suffix
     .endif
     FETCH w2, 2
     .if \is_static
     and w2, w2, #0xf  // dex register of first argument
     GET_VREG w1, w2
     fmov s0, w1
     .else
     ubfx x2, x2, #4, #4  // dex register of second argument
     GET_VREG w2, w2
     fmov s0, w2
     .endif
.Linvoke_with_few_args_\suffix:
     // Check if the next instruction is move-result or move-result-wide.
     // If it is, we fetch the shorty and jump to the regular invocation.
     FETCH w27, 3
     and ip, x27, #0xfe
     cmp ip, #0x0a
     b.eq .Lget_shorty_and_invoke_\suffix
     .if \is_interface
     // Setup hidden argument.
     mov ip2, x26
     .endif
     ldr lr, [x0, #ART_METHOD_QUICK_CODE_OFFSET_64]
     blr lr
     # TODO: Use some other register for shorty and prefetch the instruction directly to wINST.
     mov xINST, x27
     ADVANCE 3
     GET_INST_OPCODE ip
     GOTO_OPCODE ip
.Lget_shorty_and_invoke_\suffix:
     GET_SHORTY_SLOW_PATH xINST, \is_interface
     b .Lgpr_setup_finished_\suffix
   .endif

.Lget_shorty_\suffix:
   GET_SHORTY xINST, \is_interface, \is_polymorphic, \is_custom
   // From this point:
   // - xINST contains shorty (in callee-save to switch over return value after call).
   // - x0 contains method
   // - x1 contains 'this' pointer for instance method.
   // - for interface calls, x26 contains the interface method.
   add x9, xINST, #1  // shorty + 1  ; ie skip return arg character
   FETCH w11, 2 // arguments
   .if \is_string_init
   lsr x11, x11, #4
   mov x10, #1       // ignore first argument
   .elseif \is_static
   mov x10, xzr      // arg_index
   .else
   lsr x11, x11, #4
   mov x10, #1       // ignore first argument
   .endif
   LOOP_OVER_SHORTY_LOADING_FPS d0, s0, x11, x9, x10, .Lxmm_setup_finished_\suffix
   LOOP_OVER_SHORTY_LOADING_FPS d1, s1, x11, x9, x10, .Lxmm_setup_finished_\suffix
   LOOP_OVER_SHORTY_LOADING_FPS d2, s2, x11, x9, x10, .Lxmm_setup_finished_\suffix
   LOOP_OVER_SHORTY_LOADING_FPS d3, s3, x11, x9, x10, .Lxmm_setup_finished_\suffix
   LOOP_OVER_SHORTY_LOADING_FPS d4, s4, x11, x9, x10, .Lxmm_setup_finished_\suffix
.Lxmm_setup_finished_\suffix:
   add x9, xINST, #1  // shorty + 1  ; ie skip return arg character
   FETCH w11, 2 // arguments
   .if \is_string_init
   lsr x11, x11, #4
   mov x10, #1       // ignore first argument
   LOOP_OVER_SHORTY_LOADING_GPRS x1, w1, x11, x9, x10, .Lgpr_setup_finished_\suffix
   .elseif \is_static
   mov x10, xzr      // arg_index
   LOOP_OVER_SHORTY_LOADING_GPRS x1, w1, x11, x9, x10, .Lgpr_setup_finished_\suffix
   .else
   lsr x11, x11, #4
   mov x10, #1       // ignore first argument
   .endif
   LOOP_OVER_SHORTY_LOADING_GPRS x2, w2, x11, x9, x10, .Lgpr_setup_finished_\suffix
   LOOP_OVER_SHORTY_LOADING_GPRS x3, w3, x11, x9, x10, .Lgpr_setup_finished_\suffix
   LOOP_OVER_SHORTY_LOADING_GPRS x4, w4, x11, x9, x10, .Lgpr_setup_finished_\suffix
   LOOP_OVER_SHORTY_LOADING_GPRS x5, w5, x11, x9, x10, .Lgpr_setup_finished_\suffix
.Lgpr_setup_finished_\suffix:
   .if \is_polymorphic
   bl art_quick_invoke_polymorphic
   .elseif \is_custom
   bl art_quick_invoke_custom
   .else
      .if \is_interface
      // Setup hidden argument.
      mov ip2, x26
      .endif
      ldr lr, [x0, #ART_METHOD_QUICK_CODE_OFFSET_64]
      blr lr
   .endif
   SETUP_RETURN_VALUE xINST
.Ldone_return_\suffix:
   /* resume execution of caller */
   .if \is_string_init
   FETCH w11, 2 // arguments
   and x11, x11, #0xf
   GET_VREG w1, w11
   UPDATE_REGISTERS_FOR_STRING_INIT w1, w0
   .endif

   .if \is_polymorphic
   FETCH_ADVANCE_INST 4
   .else
   FETCH_ADVANCE_INST 3
   .endif
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.endm

// Puts the next floating point argument into the expected register,
// fetching values based on a range invoke.
// Uses ip as temporary.
.macro LOOP_RANGE_OVER_SHORTY_LOADING_FPS dreg, sreg, shorty, arg_index, stack_index, finished
1: // LOOP
    ldrb wip, [\shorty], #1         // Load next character in shorty, and increment.
    cbz wip, \finished              // if (wip == '\0') goto finished
    cmp wip, #68                    // if (wip == 'D') goto FOUND_DOUBLE
    b.eq 2f
    cmp wip, #70                    // if (wip == 'F') goto FOUND_FLOAT
    b.eq 3f
    add \arg_index, \arg_index, #1
    add \stack_index, \stack_index, #1
    //  Handle extra argument in arg array taken by a long.
    cmp wip, #74                    // if (wip != 'J') goto LOOP
    b.ne 1b
    add \arg_index, \arg_index, #1
    add \stack_index, \stack_index, #1
    b 1b                        // goto LOOP
2:  // FOUND_DOUBLE
    GET_VREG_DOUBLE \dreg, \arg_index
    add \arg_index, \arg_index, #2
    add \stack_index, \stack_index, #2
    b 4f
3:  // FOUND_FLOAT
    GET_VREG \sreg, \arg_index
    add \arg_index, \arg_index, #1
    add \stack_index, \stack_index, #1
4:
.endm

// Puts the next floating point argument into the expected stack slot,
// fetching values based on a range invoke.
// Uses ip as temporary.
//
// TODO: We could just copy all the vregs to the stack slots in a simple loop
// without looking at the shorty at all. (We could also drop
// the "stack_index" from the macros for loading registers.) We could also do
// that conditionally if argument word count > 6; otherwise we know that all
// args fit into registers.
.macro LOOP_RANGE_OVER_FPs shorty, arg_index, stack_index, finished
1: // LOOP
    ldrb wip, [\shorty], #1         // Load next character in shorty, and increment.
    cbz wip, \finished              // if (wip == '\0') goto finished
    cmp wip, #68                    // if (wip == 'D') goto FOUND_DOUBLE
    b.eq 2f
    cmp wip, #70                    // if (wip == 'F') goto FOUND_FLOAT
    b.eq 3f
    add \arg_index, \arg_index, #1
    add \stack_index, \stack_index, #1
    //  Handle extra argument in arg array taken by a long.
    cmp wip, #74                    // if (wip != 'J') goto LOOP
    b.ne 1b
    add \arg_index, \arg_index, #1
    add \stack_index, \stack_index, #1
    b 1b                        // goto LOOP
2:  // FOUND_DOUBLE
    GET_VREG_WIDE ip, \arg_index
    add ip2, sp, \stack_index, uxtw #2
    str ip, [ip2]
    add \arg_index, \arg_index, #2
    add \stack_index, \stack_index, #2
    b 1b
3:  // FOUND_FLOAT
    GET_VREG wip, \arg_index
    str wip, [sp, \stack_index, uxtw #2]
    add \arg_index, \arg_index, #1
    add \stack_index, \stack_index, #1
    b 1b
.endm

// Puts the next int/long/object argument in the expected register,
// fetching values based on a range invoke.
// Uses ip as temporary.
.macro LOOP_RANGE_OVER_SHORTY_LOADING_GPRS reg64, reg32, shorty, arg_index, stack_index, finished
1: // LOOP
    ldrb wip, [\shorty], #1         // Load next character in shorty, and increment.
    cbz wip, \finished              // if (wip == '\0') goto finished
    cmp wip, #74                    // if (wip == 'J') goto FOUND_LONG
    b.eq 2f
    cmp wip, #70                   // if (wip == 'F') goto SKIP_FLOAT
    b.eq 3f
    cmp wip, #68                    // if (wip == 'D') goto SKIP_DOUBLE
    b.eq 4f
    GET_VREG \reg32, \arg_index
    add \arg_index, \arg_index, #1
    add \stack_index, \stack_index, #1
    b 5f
2:  // FOUND_LONG
    GET_VREG_WIDE \reg64, \arg_index
    add \arg_index, \arg_index, #2
    add \stack_index, \stack_index, #2
    b 5f
3:  // SKIP_FLOAT
    add \arg_index, \arg_index, #1
    add \stack_index, \stack_index, #1
    b 1b
4:  // SKIP_DOUBLE
    add \arg_index, \arg_index, #2
    add \stack_index, \stack_index, #2
    b 1b
5:
.endm

// Puts the next int/long/object argument in the expected stack slot,
// fetching values based on a range invoke.
// Uses ip as temporary.
.macro LOOP_RANGE_OVER_INTs shorty, arg_index, stack_index, finished
1: // LOOP
    ldrb wip, [\shorty], #1         // Load next character in shorty, and increment.
    cbz wip, \finished              // if (wip == '\0') goto finished
    cmp wip, #74                    // if (wip == 'J') goto FOUND_LONG
    b.eq 2f
    cmp wip, #70                    // if (wip == 'F') goto SKIP_FLOAT
    b.eq 3f
    cmp wip, #68                    // if (wip == 'D') goto SKIP_DOUBLE
    b.eq 4f
    GET_VREG wip, \arg_index
    str wip, [sp, \stack_index, uxtw #2]
    add \arg_index, \arg_index, #1
    add \stack_index, \stack_index, #1
    b 1b
2:  // FOUND_LONG
    GET_VREG_WIDE ip, \arg_index
    add ip2, sp, \stack_index, uxtw #2
    str ip, [ip2]
    add \arg_index, \arg_index, #2
    add \stack_index, \stack_index, #2
    b 1b
3:  // SKIP_FLOAT
    add \arg_index, \arg_index, #1
    add \stack_index, \stack_index, #1
    b 1b
4:  // SKIP_DOUBLE
    add \arg_index, \arg_index, #2
    add \stack_index, \stack_index, #2
    b 1b
.endm

.macro COMMON_INVOKE_RANGE is_static=0, is_interface=0, suffix="", is_string_init=0, is_polymorphic=0, is_custom=0
   .if \is_polymorphic
   // We always go to compiled code for polymorphic calls.
   .elseif \is_custom
   // We always go to compiled code for custom calls.
   .else
     DO_ENTRY_POINT_CHECK .Lcall_compiled_code_range_\suffix
     GET_CODE_ITEM
     .if \is_string_init
     bl nterp_to_nterp_string_init_range
     .elseif \is_static
     bl nterp_to_nterp_static_range
     .else
     bl nterp_to_nterp_instance_range
     .endif
     b .Ldone_return_range_\suffix
   .endif

.Lcall_compiled_code_range_\suffix:
   .if \is_polymorphic
   // No fast path for polymorphic calls.
   .elseif \is_custom
   // No fast path for custom calls.
   .elseif \is_string_init
   // No fast path for string.init.
   .else
     ldr wip, [x0, #ART_METHOD_ACCESS_FLAGS_OFFSET]
     tbz wip, #ART_METHOD_NTERP_INVOKE_FAST_PATH_FLAG_BIT, .Lfast_path_with_few_args_range_\suffix
     FETCH_B wip2, 0, 1  // Number of arguments
     .if \is_static
     cbz ip2, .Linvoke_fast_path_range_\suffix
     .else
     cmp ip2, #1
     b.eq .Linvoke_fast_path_range_\suffix
     .endif
     FETCH wip, 2  // dex register of first argument
     add x8, xFP, wip, uxtw #2  // location of first dex register value
     cmp ip2, #2
     .if \is_static
     b.lt .Lone_arg_fast_path_range_\suffix
     .endif
     b.eq .Ltwo_args_fast_path_range_\suffix
     cmp ip2, #4
     b.lt .Lthree_args_fast_path_range_\suffix
     b.eq .Lfour_args_fast_path_range_\suffix
     cmp ip2, #6
     b.lt .Lfive_args_fast_path_range_\suffix
     b.eq .Lsix_args_fast_path_range_\suffix
     cmp ip2, #7
     b.eq .Lseven_args_fast_path_range_\suffix
     // Setup x8 to point to the stack location of parameters we do not need
     // to put parameters in.
     add x9, sp, #8  // Add space for the ArtMethod

.Lloop_over_fast_path_range_\suffix:
     sub ip2, ip2, #1
     ldr wip, [x8, ip2, lsl #2]
     str wip, [x9, ip2, lsl #2]
     cmp ip2, #7
     b.ne .Lloop_over_fast_path_range_\suffix

.Lseven_args_fast_path_range_\suffix:
     ldr w7, [x8, #24]
.Lsix_args_fast_path_range_\suffix:
     ldr w6, [x8, #20]
.Lfive_args_fast_path_range_\suffix:
     ldr w5, [x8, #16]
.Lfour_args_fast_path_range_\suffix:
     ldr w4, [x8, #12]
.Lthree_args_fast_path_range_\suffix:
     ldr w3, [x8, #8]
.Ltwo_args_fast_path_range_\suffix:
     ldr w2, [x8, #4]
.Lone_arg_fast_path_range_\suffix:
     .if \is_static
     ldr w1, [x8, #0]
     .else
     // First argument already in w1.
     .endif
.Linvoke_fast_path_range_\suffix:
     .if \is_interface
     // Setup hidden argument.
     mov ip2, x26
     .endif
     ldr lr, [x0, #ART_METHOD_QUICK_CODE_OFFSET_64]
     blr lr
     FETCH_ADVANCE_INST 3
     GET_INST_OPCODE ip
     GOTO_OPCODE ip

.Lfast_path_with_few_args_range_\suffix:
     // Fast path when we have zero or one argument (modulo 'this'). If there
     // is one argument, we can put it in both floating point and core register.
     FETCH_B w2, 0, 1 // number of arguments
     .if \is_static
     cmp w2, #1
     .else
     cmp w2, #2
     .endif
     b.lt .Linvoke_with_few_args_range_\suffix
     b.ne .Lget_shorty_range_\suffix
     FETCH w3, 2  // dex register of first argument
     .if \is_static
     GET_VREG w1, w3
     fmov s0, w1
     .else
     add w3, w3, #1  // Add 1 for next argument
     GET_VREG w2, w3
     fmov s0, w2
     .endif
.Linvoke_with_few_args_range_\suffix:
     // Check if the next instruction is move-result or move-result-wide.
     // If it is, we fetch the shorty and jump to the regular invocation.
     FETCH w27, 3
     and ip, x27, #0xfe
     cmp ip, #0x0a
     b.eq .Lget_shorty_and_invoke_range_\suffix
     .if \is_interface
     // Setup hidden argument.
     mov ip2, x26
     .endif
     ldr lr, [x0, #ART_METHOD_QUICK_CODE_OFFSET_64]
     blr lr
     mov xINST, x27
     ADVANCE 3
     GET_INST_OPCODE ip
     GOTO_OPCODE ip
.Lget_shorty_and_invoke_range_\suffix:
     GET_SHORTY_SLOW_PATH xINST, \is_interface
     b .Lgpr_setup_finished_range_\suffix
   .endif

.Lget_shorty_range_\suffix:
   GET_SHORTY xINST, \is_interface, \is_polymorphic, \is_custom
   // From this point:
   // - xINST contains shorty (in callee-save to switch over return value after call).
   // - x0 contains method
   // - x1 contains 'this' pointer for instance method.
   // - for interface calls, x26 contains the interface method.
   add x9, xINST, #1  // shorty + 1  ; ie skip return arg character
   FETCH w10, 2 // arguments
   .if \is_string_init
   add x10, x10, #1  // arg start index
   mov x11, #1       // index in stack
   .elseif \is_static
   mov x11, xzr      // index in stack
   .else
   add x10, x10, #1  // arg start index
   mov x11, #1       // index in stack
   .endif
   LOOP_RANGE_OVER_SHORTY_LOADING_FPS d0, s0, x9, w10, w11, .Lxmm_setup_finished_range_\suffix
   LOOP_RANGE_OVER_SHORTY_LOADING_FPS d1, s1, x9, w10, w11, .Lxmm_setup_finished_range_\suffix
   LOOP_RANGE_OVER_SHORTY_LOADING_FPS d2, s2, x9, w10, w11, .Lxmm_setup_finished_range_\suffix
   LOOP_RANGE_OVER_SHORTY_LOADING_FPS d3, s3, x9, w10, w11, .Lxmm_setup_finished_range_\suffix
   LOOP_RANGE_OVER_SHORTY_LOADING_FPS d4, s4, x9, w10, w11, .Lxmm_setup_finished_range_\suffix
   LOOP_RANGE_OVER_SHORTY_LOADING_FPS d5, s5, x9, w10, w11, .Lxmm_setup_finished_range_\suffix
   LOOP_RANGE_OVER_SHORTY_LOADING_FPS d6, s6, x9, w10, w11, .Lxmm_setup_finished_range_\suffix
   LOOP_RANGE_OVER_SHORTY_LOADING_FPS d7, s7, x9, w10, w11, .Lxmm_setup_finished_range_\suffix
   // Store in the outs array (stored above the ArtMethod in the stack)
   add x11, x11, #2 // Add two words for the ArtMethod stored before the outs.
   LOOP_RANGE_OVER_FPs x9, w10, w11, .Lxmm_setup_finished_range_\suffix
.Lxmm_setup_finished_range_\suffix:
   add x9, xINST, #1  // shorty + 1  ; ie skip return arg character
   FETCH w10, 2 // arguments
   .if \is_string_init
   add x10, x10, #1  // arg start index
   mov x11, #1       // index in stack
   LOOP_RANGE_OVER_SHORTY_LOADING_GPRS x1, w1, x9, w10, w11, .Lgpr_setup_finished_range_\suffix
   .elseif \is_static
   mov x11, xzr      // index in stack
   LOOP_RANGE_OVER_SHORTY_LOADING_GPRS x1, w1, x9, w10, w11 .Lgpr_setup_finished_range_\suffix
   .else
   add x10, x10, #1  // arg start index
   mov x11, #1       // index in stack
   .endif
   LOOP_RANGE_OVER_SHORTY_LOADING_GPRS x2, w2, x9, w10, w11, .Lgpr_setup_finished_range_\suffix
   LOOP_RANGE_OVER_SHORTY_LOADING_GPRS x3, w3, x9, w10, w11, .Lgpr_setup_finished_range_\suffix
   LOOP_RANGE_OVER_SHORTY_LOADING_GPRS x4, w4, x9, w10, w11, .Lgpr_setup_finished_range_\suffix
   LOOP_RANGE_OVER_SHORTY_LOADING_GPRS x5, w5, x9, w10, w11, .Lgpr_setup_finished_range_\suffix
   LOOP_RANGE_OVER_SHORTY_LOADING_GPRS x6, w6, x9, w10, w11, .Lgpr_setup_finished_range_\suffix
   LOOP_RANGE_OVER_SHORTY_LOADING_GPRS x7, w7, x9, w10, w11, .Lgpr_setup_finished_range_\suffix
   // Store in the outs array (stored above the ArtMethod in the stack)
   add x11, x11, #2 // Add two words for the ArtMethod stored before the outs.
   LOOP_RANGE_OVER_INTs x9, w10, w11, .Lgpr_setup_finished_range_\suffix
.Lgpr_setup_finished_range_\suffix:
   .if \is_polymorphic
   bl art_quick_invoke_polymorphic
   .elseif \is_custom
   bl art_quick_invoke_custom
   .else
      .if \is_interface
      // Setup hidden argument.
      mov ip2, x26
      .endif
      ldr lr, [x0, #ART_METHOD_QUICK_CODE_OFFSET_64]
      blr lr
   .endif
   SETUP_RETURN_VALUE xINST
.Ldone_return_range_\suffix:
   /* resume execution of caller */
   .if \is_string_init
   FETCH w11, 2 // arguments
   GET_VREG w1, w11
   UPDATE_REGISTERS_FOR_STRING_INIT w1, w0
   .endif

   .if \is_polymorphic
   FETCH_ADVANCE_INST 4
   .else
   FETCH_ADVANCE_INST 3
   .endif
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.endm

.macro WRITE_BARRIER_IF_OBJECT is_object, value, holder, label
   .if \is_object
   cbz     \value, \label
   ldr     ip, [xSELF, #THREAD_CARD_TABLE_OFFSET]
   lsr     wip2, \holder, #CARD_TABLE_CARD_SHIFT
   strb    wip, [ip, ip2]
\label:
   .endif
.endm

// Fetch some information from the thread cache.
// Uses ip and ip2 as temporaries.
.macro FETCH_FROM_THREAD_CACHE dest_reg, slow_path
   add      ip, xSELF, #THREAD_INTERPRETER_CACHE_OFFSET       // cache address
   ubfx     ip2, xPC, #2, #THREAD_INTERPRETER_CACHE_SIZE_LOG2  // entry index
   add      ip, ip, ip2, lsl #4            // entry address within the cache
   ldp      ip, \dest_reg, [ip]           // entry key (pc) and value (offset)
   cmp      ip, xPC
   b.ne \slow_path
.endm

// Puts the next int/long/object parameter passed in physical register
// in the expected dex register array entry, and in case of object in the
// expected reference array entry.
.macro LOOP_OVER_SHORTY_STORING_GPRS gpr_64, gpr_32, shorty, arg_offset, regs, refs, finished
1: // LOOP
    ldrb wip, [\shorty], #1       // Load next character in shorty, and increment.
    cbz wip, \finished            // if (wip == '\0') goto finished
    cmp wip, #74                  // if (wip == 'J') goto FOUND_LONG
    b.eq 2f
    cmp wip, #70                  // if (wip == 'F') goto SKIP_FLOAT
    b.eq 3f
    cmp wip, #68                  // if (wip == 'D') goto SKIP_DOUBLE
    b.eq 4f
    str \gpr_32, [\regs, \arg_offset]
    cmp wip, #76                  // if (wip != 'L') goto NOT_REFERENCE
    b.ne 6f
    str \gpr_32, [\refs, \arg_offset]
6:  // NOT_REFERENCE
    add \arg_offset, \arg_offset, #4
    b 5f
2:  // FOUND_LONG
    str \gpr_64, [\regs, \arg_offset]
    add \arg_offset, \arg_offset, #8
    b 5f
3:  // SKIP_FLOAT
    add \arg_offset, \arg_offset, #4
    b 1b
4:  // SKIP_DOUBLE
    add \arg_offset, \arg_offset, #8
    b 1b
5:
.endm

// Puts the next floating point parameter passed in physical register
// in the expected dex register array entry.
// Uses ip as temporary.
.macro LOOP_OVER_SHORTY_STORING_FPS dreg, sreg, shorty, arg_offset, fp, finished
1: // LOOP
    ldrb wip, [\shorty], #1                 // Load next character in shorty, and increment.
    cbz wip, \finished                      // if (wip == '\0') goto finished
    cmp wip, #68                            // if (wip == 'D') goto FOUND_DOUBLE
    b.eq 2f
    cmp wip, #70                            // if (wip == 'F') goto FOUND_FLOAT
    b.eq 3f
    add \arg_offset, \arg_offset, #4
    //  Handle extra argument in arg array taken by a long.
    cmp wip, #74                            // if (wip != 'J') goto LOOP
    b.ne 1b
    add \arg_offset, \arg_offset, #4
    b 1b                        // goto LOOP
2:  // FOUND_DOUBLE
    str \dreg, [\fp, \arg_offset]
    add \arg_offset, \arg_offset, #8
    b 4f
3:  // FOUND_FLOAT
    str \sreg, [\fp, \arg_offset]
    add \arg_offset, \arg_offset, #4
4:
.endm

// Puts the next floating point parameter passed in stack
// in the expected dex register array entry.
// Uses ip as temporary.
//
// TODO: Or we could just spill regs to the reserved slots in the caller's
// frame and copy all regs in a simple loop. This time, however, we would
// need to look at the shorty anyway to look for the references.
// (The trade-off is different for passing arguments and receiving them.)
.macro LOOP_OVER_FPs shorty, arg_offset, regs, stack_ptr, finished
1: // LOOP
    ldrb wip, [\shorty], #1                 // Load next character in shorty, and increment.
    cbz wip, \finished                      // if (wip == '\0') goto finished
    cmp wip, #68                            // if (wip == 'D') goto FOUND_DOUBLE
    b.eq 2f
    cmp wip, #70                            // if (wip == 'F') goto FOUND_FLOAT
    b.eq 3f
    add \arg_offset, \arg_offset, #4
    //  Handle extra argument in arg array taken by a long.
    cmp wip, #74                            // if (wip != 'J') goto LOOP
    b.ne 1b
    add \arg_offset, \arg_offset, #4
    b 1b                        // goto LOOP
2:  // FOUND_DOUBLE
    add ip, \stack_ptr, \arg_offset
    ldr ip, [ip,  #OFFSET_TO_FIRST_ARGUMENT_IN_STACK]
    str ip, [\regs, \arg_offset]
    add \arg_offset, \arg_offset, #8
    b 1b
3:  // FOUND_FLOAT
    add ip, \stack_ptr, \arg_offset
    ldr wip, [ip,  #OFFSET_TO_FIRST_ARGUMENT_IN_STACK]
    str wip, [\regs, \arg_offset]
    add \arg_offset, \arg_offset, #4
    b 1b
.endm

// Puts the next int/long/object parameter passed in stack
// in the expected dex register array entry, and in case of object in the
// expected reference array entry.
// Uses ip and ip2 as temporary.
.macro LOOP_OVER_INTs shorty, arg_offset, regs, refs, stack_ptr, finished
1: // LOOP
    ldrb wip, [\shorty], #1       // Load next character in shorty, and increment.
    cbz wip, \finished            // if (wip == '\0') goto finished
    cmp wip, #74                  // if (wip == 'J') goto FOUND_LONG
    b.eq 2f
    cmp wip, #70                  // if (wip == 'F') goto SKIP_FLOAT
    b.eq 3f
    cmp wip, #68                  // if (wip == 'D') goto SKIP_DOUBLE
    b.eq 4f
    add ip2, \stack_ptr, \arg_offset
    ldr wip2, [ip2,  #OFFSET_TO_FIRST_ARGUMENT_IN_STACK]
    str wip2, [\regs, \arg_offset]
    cmp wip, #76                  // if (wip != 'L') goto loop
    b.ne 3f
    str wip2, [\refs, \arg_offset]
    add \arg_offset, \arg_offset, #4
    b 1b
2:  // FOUND_LONG
    add ip, \stack_ptr, \arg_offset
    ldr ip, [ip,  #OFFSET_TO_FIRST_ARGUMENT_IN_STACK]
    str ip, [\regs, \arg_offset]
    add \arg_offset, \arg_offset, #8
    b 1b
3:  // SKIP_FLOAT
    add \arg_offset, \arg_offset, #4
    b 1b
4:  // SKIP_DOUBLE
    add \arg_offset, \arg_offset, #8
    b 1b
.endm

.macro SETUP_REFERENCE_PARAMETER_IN_GPR gpr32, regs, refs, ins, arg_offset, finished
    str \gpr32, [\regs, \arg_offset]
    sub \ins, \ins, #1
    str \gpr32, [\refs, \arg_offset]
    add \arg_offset, \arg_offset, #4
    cbz \ins, \finished
.endm

// Uses ip2 as temporary.
.macro SETUP_REFERENCE_PARAMETERS_IN_STACK regs, refs, ins, stack_ptr, arg_offset
1:
    ldr wip2, [\stack_ptr, \arg_offset]
    sub \ins, \ins, #1
    str wip2, [\regs, \arg_offset]
    str wip2, [\refs, \arg_offset]
    add \arg_offset, \arg_offset, #4
    cbnz \ins, 1b
.endm

/*
 * ArtMethod entry point.
 *
 * On entry:
 *  x0   ArtMethod* callee
 *  rest  method parameters
 */

OAT_ENTRY ExecuteNterpImpl, EndExecuteNterpImpl
    .cfi_startproc
    sub x16, sp, #STACK_OVERFLOW_RESERVED_BYTES
    ldr wzr, [x16]
    /* Spill callee save regs */
    SPILL_ALL_CALLEE_SAVES

    ldr xPC, [x0, #ART_METHOD_DATA_OFFSET_64]
    // Setup the stack for executing the method.
    SETUP_STACK_FRAME xPC, xREFS, xFP, CFI_REFS, load_ins=1

    // Setup the parameters
    cbz w15, .Lxmm_setup_finished

    sub ip2, ip, x15
    ldr w26, [x0, #ART_METHOD_ACCESS_FLAGS_OFFSET]
    lsl x21, ip2, #2 // x21 is now the offset for inputs into the registers array.

    tbz w26, #ART_METHOD_NTERP_ENTRY_POINT_FAST_PATH_FLAG_BIT, .Lsetup_slow_path
    // Setup pointer to inputs in FP and pointer to inputs in REFS
    add x10, xFP, x21
    add x11, xREFS, x21
    mov x12, #0
    SETUP_REFERENCE_PARAMETER_IN_GPR w1, x10, x11, w15, x12, .Lxmm_setup_finished
    SETUP_REFERENCE_PARAMETER_IN_GPR w2, x10, x11, w15, x12, .Lxmm_setup_finished
    SETUP_REFERENCE_PARAMETER_IN_GPR w3, x10, x11, w15, x12, .Lxmm_setup_finished
    SETUP_REFERENCE_PARAMETER_IN_GPR w4, x10, x11, w15, x12, .Lxmm_setup_finished
    SETUP_REFERENCE_PARAMETER_IN_GPR w5, x10, x11, w15, x12, .Lxmm_setup_finished
    SETUP_REFERENCE_PARAMETER_IN_GPR w6, x10, x11, w15, x12, .Lxmm_setup_finished
    SETUP_REFERENCE_PARAMETER_IN_GPR w7, x10, x11, w15, x12, .Lxmm_setup_finished
    add x28, x28, #OFFSET_TO_FIRST_ARGUMENT_IN_STACK
    SETUP_REFERENCE_PARAMETERS_IN_STACK x10, x11, w15, x28, x12
    b .Lxmm_setup_finished

.Lsetup_slow_path:
    // If the method is not static and there is one argument ('this'), we don't need to fetch the
    // shorty.
    tbnz w26, #ART_METHOD_IS_STATIC_FLAG_BIT, .Lsetup_with_shorty
    str w1, [xFP, x21]
    str w1, [xREFS, x21]
    cmp w15, #1
    b.eq .Lxmm_setup_finished

.Lsetup_with_shorty:
    // TODO: Get shorty in a better way and remove below
    SPILL_ALL_ARGUMENTS
    bl NterpGetShorty
    // Save shorty in callee-save xIBASE.
    mov xIBASE, x0
    RESTORE_ALL_ARGUMENTS

    // Setup pointer to inputs in FP and pointer to inputs in REFS
    add x10, xFP, x21
    add x11, xREFS, x21
    mov x12, #0

    add x9, xIBASE, #1  // shorty + 1  ; ie skip return arg character
    tbnz w26, #ART_METHOD_IS_STATIC_FLAG_BIT, .Lhandle_static_method
    add x10, x10, #4
    add x11, x11, #4
    add x28, x28, #4
    b .Lcontinue_setup_gprs
.Lhandle_static_method:
    LOOP_OVER_SHORTY_STORING_GPRS x1, w1, x9, x12, x10, x11, .Lgpr_setup_finished
.Lcontinue_setup_gprs:
    LOOP_OVER_SHORTY_STORING_GPRS x2, w2, x9, x12, x10, x11, .Lgpr_setup_finished
    LOOP_OVER_SHORTY_STORING_GPRS x3, w3, x9, x12, x10, x11, .Lgpr_setup_finished
    LOOP_OVER_SHORTY_STORING_GPRS x4, w4, x9, x12, x10, x11, .Lgpr_setup_finished
    LOOP_OVER_SHORTY_STORING_GPRS x5, w5, x9, x12, x10, x11, .Lgpr_setup_finished
    LOOP_OVER_SHORTY_STORING_GPRS x6, w6, x9, x12, x10, x11, .Lgpr_setup_finished
    LOOP_OVER_SHORTY_STORING_GPRS x7, w7, x9, x12, x10, x11, .Lgpr_setup_finished
    LOOP_OVER_INTs x9, x12, x10, x11, x28, .Lgpr_setup_finished
.Lgpr_setup_finished:
    add x9, xIBASE, #1  // shorty + 1  ; ie skip return arg character
    mov x12, #0  // reset counter
    LOOP_OVER_SHORTY_STORING_FPS d0, s0, x9, x12, x10, .Lxmm_setup_finished
    LOOP_OVER_SHORTY_STORING_FPS d1, s1, x9, x12, x10, .Lxmm_setup_finished
    LOOP_OVER_SHORTY_STORING_FPS d2, s2, x9, x12, x10, .Lxmm_setup_finished
    LOOP_OVER_SHORTY_STORING_FPS d3, s3, x9, x12, x10, .Lxmm_setup_finished
    LOOP_OVER_SHORTY_STORING_FPS d4, s4, x9, x12, x10, .Lxmm_setup_finished
    LOOP_OVER_SHORTY_STORING_FPS d5, s5, x9, x12, x10, .Lxmm_setup_finished
    LOOP_OVER_SHORTY_STORING_FPS d6, s6, x9, x12, x10, .Lxmm_setup_finished
    LOOP_OVER_SHORTY_STORING_FPS d7, s7, x9, x12, x10, .Lxmm_setup_finished
    LOOP_OVER_FPs x9, x12, x10, x28, .Lxmm_setup_finished
.Lxmm_setup_finished:
    CFI_DEFINE_DEX_PC_WITH_OFFSET(CFI_TMP, CFI_DEX, 0)

    // Set rIBASE
    adr xIBASE, artNterpAsmInstructionStart
    /* start executing the instruction at xPC */
    START_EXECUTING_INSTRUCTIONS
    /* NOTE: no fallthrough */
    // cfi info continues, and covers the whole nterp implementation.
    SIZE ExecuteNterpImpl

    .type artNterpAsmInstructionStart, #function
    .hidden artNterpAsmInstructionStart
    .global artNterpAsmInstructionStart
artNterpAsmInstructionStart = .L_op_nop
    .text

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_nop: /* 0x00 */
    NAME_START nterp_op_nop

    FETCH_ADVANCE_INST 1                // advance to next instr, load rINST
    GET_INST_OPCODE ip                  // ip<- opcode from rINST
    GOTO_OPCODE ip                      // execute it

    NAME_END nterp_op_nop

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_move: /* 0x01 */
    NAME_START nterp_op_move

    /* for move, move-object, long-to-int */
    /* op vA, vB */
    lsr     w1, wINST, #12              // x1<- B from 15:12
    ubfx    w0, wINST, #8, #4           // x0<- A from 11:8
    FETCH_ADVANCE_INST 1                // advance rPC, load wINST
    GET_VREG w2, w1                     // x2<- fp[B]
    GET_INST_OPCODE ip                  // ip<- opcode from wINST
    .if 0
    SET_VREG_OBJECT w2, w0              // fp[A]<- x2
    .else
    SET_VREG w2, w0                     // fp[A]<- x2
    .endif
    GOTO_OPCODE ip                      // execute next instruction

    NAME_END nterp_op_move

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_move_from16: /* 0x02 */
    NAME_START nterp_op_move_from16

    /* for: move/from16, move-object/from16 */
    /* op vAA, vBBBB */
    FETCH w1, 1                         // r1<- BBBB
    lsr     w0, wINST, #8               // r0<- AA
    FETCH_ADVANCE_INST 2                // advance rPC, load wINST
    GET_VREG w2, w1                     // r2<- fp[BBBB]
    GET_INST_OPCODE ip                  // extract opcode from wINST
    .if 0
    SET_VREG_OBJECT w2, w0              // fp[AA]<- r2
    .else
    SET_VREG w2, w0                     // fp[AA]<- r2
    .endif
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_move_from16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_move_16: /* 0x03 */
    NAME_START nterp_op_move_16

    /* for: move/16, move-object/16 */
    /* op vAAAA, vBBBB */
    FETCH w1, 2                         // w1<- BBBB
    FETCH w0, 1                         // w0<- AAAA
    FETCH_ADVANCE_INST 3                // advance xPC, load xINST
    GET_VREG w2, w1                     // w2<- fp[BBBB]
    GET_INST_OPCODE ip                  // extract opcode from xINST
    .if 0
    SET_VREG_OBJECT w2, w0              // fp[AAAA]<- w2
    .else
    SET_VREG w2, w0                     // fp[AAAA]<- w2
    .endif
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_move_16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_move_wide: /* 0x04 */
    NAME_START nterp_op_move_wide

    /* move-wide vA, vB */
    /* NOTE: regs can overlap, e.g. "move v6,v7" or "move v7,v6" */
    lsr     w3, wINST, #12              // w3<- B
    ubfx    w2, wINST, #8, #4           // w2<- A
    GET_VREG_WIDE  x3, w3
    FETCH_ADVANCE_INST 1                // advance rPC, load wINST
    GET_INST_OPCODE ip                  // extract opcode from wINST
    SET_VREG_WIDE  x3, w2
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_move_wide

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_move_wide_from16: /* 0x05 */
    NAME_START nterp_op_move_wide_from16

    /* move-wide/from16 vAA, vBBBB */
    /* NOTE: regs can overlap, e.g. "move v6,v7" or "move v7,v6" */
    FETCH w3, 1                         // w3<- BBBB
    lsr     w2, wINST, #8               // w2<- AA
    GET_VREG_WIDE x3, w3
    FETCH_ADVANCE_INST 2                // advance rPC, load wINST
    GET_INST_OPCODE ip                  // extract opcode from wINST
    SET_VREG_WIDE x3, w2
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_move_wide_from16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_move_wide_16: /* 0x06 */
    NAME_START nterp_op_move_wide_16

    /* move-wide/16 vAAAA, vBBBB */
    /* NOTE: regs can overlap, e.g. "move v6,v7" or "move v7,v6" */
    FETCH w3, 2                         // w3<- BBBB
    FETCH w2, 1                         // w2<- AAAA
    GET_VREG_WIDE x3, w3
    FETCH_ADVANCE_INST 3                // advance rPC, load rINST
    SET_VREG_WIDE x3, w2
    GET_INST_OPCODE ip                  // extract opcode from rINST
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_move_wide_16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_move_object: /* 0x07 */
    NAME_START nterp_op_move_object

    /* for move, move-object, long-to-int */
    /* op vA, vB */
    lsr     w1, wINST, #12              // x1<- B from 15:12
    ubfx    w0, wINST, #8, #4           // x0<- A from 11:8
    FETCH_ADVANCE_INST 1                // advance rPC, load wINST
    GET_VREG w2, w1                     // x2<- fp[B]
    GET_INST_OPCODE ip                  // ip<- opcode from wINST
    .if 1
    SET_VREG_OBJECT w2, w0              // fp[A]<- x2
    .else
    SET_VREG w2, w0                     // fp[A]<- x2
    .endif
    GOTO_OPCODE ip                      // execute next instruction

    NAME_END nterp_op_move_object

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_move_object_from16: /* 0x08 */
    NAME_START nterp_op_move_object_from16

    /* for: move/from16, move-object/from16 */
    /* op vAA, vBBBB */
    FETCH w1, 1                         // r1<- BBBB
    lsr     w0, wINST, #8               // r0<- AA
    FETCH_ADVANCE_INST 2                // advance rPC, load wINST
    GET_VREG w2, w1                     // r2<- fp[BBBB]
    GET_INST_OPCODE ip                  // extract opcode from wINST
    .if 1
    SET_VREG_OBJECT w2, w0              // fp[AA]<- r2
    .else
    SET_VREG w2, w0                     // fp[AA]<- r2
    .endif
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_move_object_from16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_move_object_16: /* 0x09 */
    NAME_START nterp_op_move_object_16

    /* for: move/16, move-object/16 */
    /* op vAAAA, vBBBB */
    FETCH w1, 2                         // w1<- BBBB
    FETCH w0, 1                         // w0<- AAAA
    FETCH_ADVANCE_INST 3                // advance xPC, load xINST
    GET_VREG w2, w1                     // w2<- fp[BBBB]
    GET_INST_OPCODE ip                  // extract opcode from xINST
    .if 1
    SET_VREG_OBJECT w2, w0              // fp[AAAA]<- w2
    .else
    SET_VREG w2, w0                     // fp[AAAA]<- w2
    .endif
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_move_object_16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_move_result: /* 0x0a */
    NAME_START nterp_op_move_result

    /* for: move-result, move-result-object */
    /* op vAA */
    lsr     w2, wINST, #8               // r2<- AA
    FETCH_ADVANCE_INST 1                // advance rPC, load wINST
    GET_INST_OPCODE ip                  // extract opcode from wINST
    .if 0
    SET_VREG_OBJECT w0, w2              // fp[AA]<- r0
    .else
    SET_VREG w0, w2                     // fp[AA]<- r0
    .endif
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_move_result

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_move_result_wide: /* 0x0b */
    NAME_START nterp_op_move_result_wide

    /* for: move-result-wide */
    /* op vAA */
    lsr     w2, wINST, #8               // r2<- AA
    FETCH_ADVANCE_INST 1                // advance rPC, load wINST
    GET_INST_OPCODE ip                  // extract opcode from wINST
    SET_VREG_WIDE x0, w2                // fp[AA]<- r0
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_move_result_wide

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_move_result_object: /* 0x0c */
    NAME_START nterp_op_move_result_object

    /* for: move-result, move-result-object */
    /* op vAA */
    lsr     w2, wINST, #8               // r2<- AA
    FETCH_ADVANCE_INST 1                // advance rPC, load wINST
    GET_INST_OPCODE ip                  // extract opcode from wINST
    .if 1
    SET_VREG_OBJECT w0, w2              // fp[AA]<- r0
    .else
    SET_VREG w0, w2                     // fp[AA]<- r0
    .endif
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_move_result_object

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_move_exception: /* 0x0d */
    NAME_START nterp_op_move_exception

    /* move-exception vAA */
    lsr     w2, wINST, #8               // w2<- AA
    ldr     x3, [xSELF, #THREAD_EXCEPTION_OFFSET]
    FETCH_ADVANCE_INST 1                // advance rPC, load rINST
    SET_VREG_OBJECT w3, w2              // fp[AA]<- exception obj
    GET_INST_OPCODE ip                  // extract opcode from rINST
    str     xzr, [xSELF, #THREAD_EXCEPTION_OFFSET]  // clear exception
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_move_exception

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_return_void: /* 0x0e */
    NAME_START nterp_op_return_void

    .if 1
      // Thread fence for constructor
      dmb ishst
    .else
      lsr     w2, wINST, #8               // w2<- AA
      .if 0
        GET_VREG_WIDE x0, w2                // x0<- vAA
        // In case we're going back to compiled code, put the
        // result also in d0
        fmov d0, x0
      .else
        GET_VREG w0, w2                     // r0<- vAA
        .if !0
        // In case we're going back to compiled code, put the
        // result also in s0.
        fmov s0, w0
        .endif
      .endif
    .endif
    .cfi_remember_state
    ldr ip, [xREFS, #-8]
    mov sp, ip
    .cfi_def_cfa sp, CALLEE_SAVES_SIZE
    RESTORE_ALL_CALLEE_SAVES
    ret
    .cfi_restore_state

    NAME_END nterp_op_return_void

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_return: /* 0x0f */
    NAME_START nterp_op_return

    .if 0
      // Thread fence for constructor
      dmb ishst
    .else
      lsr     w2, wINST, #8               // w2<- AA
      .if 0
        GET_VREG_WIDE x0, w2                // x0<- vAA
        // In case we're going back to compiled code, put the
        // result also in d0
        fmov d0, x0
      .else
        GET_VREG w0, w2                     // r0<- vAA
        .if !0
        // In case we're going back to compiled code, put the
        // result also in s0.
        fmov s0, w0
        .endif
      .endif
    .endif
    .cfi_remember_state
    ldr ip, [xREFS, #-8]
    mov sp, ip
    .cfi_def_cfa sp, CALLEE_SAVES_SIZE
    RESTORE_ALL_CALLEE_SAVES
    ret
    .cfi_restore_state

    NAME_END nterp_op_return

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_return_wide: /* 0x10 */
    NAME_START nterp_op_return_wide

    .if 0
      // Thread fence for constructor
      dmb ishst
    .else
      lsr     w2, wINST, #8               // w2<- AA
      .if 1
        GET_VREG_WIDE x0, w2                // x0<- vAA
        // In case we're going back to compiled code, put the
        // result also in d0
        fmov d0, x0
      .else
        GET_VREG w0, w2                     // r0<- vAA
        .if !0
        // In case we're going back to compiled code, put the
        // result also in s0.
        fmov s0, w0
        .endif
      .endif
    .endif
    .cfi_remember_state
    ldr ip, [xREFS, #-8]
    mov sp, ip
    .cfi_def_cfa sp, CALLEE_SAVES_SIZE
    RESTORE_ALL_CALLEE_SAVES
    ret
    .cfi_restore_state

    NAME_END nterp_op_return_wide

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_return_object: /* 0x11 */
    NAME_START nterp_op_return_object

    .if 0
      // Thread fence for constructor
      dmb ishst
    .else
      lsr     w2, wINST, #8               // w2<- AA
      .if 0
        GET_VREG_WIDE x0, w2                // x0<- vAA
        // In case we're going back to compiled code, put the
        // result also in d0
        fmov d0, x0
      .else
        GET_VREG w0, w2                     // r0<- vAA
        .if !1
        // In case we're going back to compiled code, put the
        // result also in s0.
        fmov s0, w0
        .endif
      .endif
    .endif
    .cfi_remember_state
    ldr ip, [xREFS, #-8]
    mov sp, ip
    .cfi_def_cfa sp, CALLEE_SAVES_SIZE
    RESTORE_ALL_CALLEE_SAVES
    ret
    .cfi_restore_state

    NAME_END nterp_op_return_object

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_const_4: /* 0x12 */
    NAME_START nterp_op_const_4

    /* const/4 vA, #+B */
    sbfx    w1, wINST, #12, #4          // w1<- sssssssB
    ubfx    w0, wINST, #8, #4           // w0<- A
    FETCH_ADVANCE_INST 1                // advance xPC, load wINST
    GET_INST_OPCODE ip                  // ip<- opcode from xINST
    SET_VREG w1, w0                     // fp[A]<- w1
    GOTO_OPCODE ip                      // execute next instruction

    NAME_END nterp_op_const_4

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_const_16: /* 0x13 */
    NAME_START nterp_op_const_16

    /* const/16 vAA, #+BBBB */
    FETCH_S w0, 1                       // w0<- ssssBBBB (sign-extended)
    lsr     w3, wINST, #8               // w3<- AA
    FETCH_ADVANCE_INST 2                // advance xPC, load wINST
    SET_VREG w0, w3                     // vAA<- w0
    GET_INST_OPCODE ip                  // extract opcode from wINST
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_const_16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_const: /* 0x14 */
    NAME_START nterp_op_const

    /* const vAA, #+BBBBbbbb */
    lsr     w3, wINST, #8               // w3<- AA
    FETCH w0, 1                         // w0<- bbbb (low)
    FETCH w1, 2                         // w1<- BBBB (high)
    FETCH_ADVANCE_INST 3                // advance rPC, load wINST
    orr     w0, w0, w1, lsl #16         // w0<- BBBBbbbb
    GET_INST_OPCODE ip                  // extract opcode from wINST
    SET_VREG w0, w3                     // vAA<- w0
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_const

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_const_high16: /* 0x15 */
    NAME_START nterp_op_const_high16

    /* const/high16 vAA, #+BBBB0000 */
    FETCH   w0, 1                       // r0<- 0000BBBB (zero-extended)
    lsr     w3, wINST, #8               // r3<- AA
    lsl     w0, w0, #16                 // r0<- BBBB0000
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
    SET_VREG w0, w3                     // vAA<- r0
    GET_INST_OPCODE ip                  // extract opcode from rINST
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_const_high16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_const_wide_16: /* 0x16 */
    NAME_START nterp_op_const_wide_16

    /* const-wide/16 vAA, #+BBBB */
    FETCH_S x0, 1                       // x0<- ssssssssssssBBBB (sign-extended)
    lsr     w3, wINST, #8               // w3<- AA
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG_WIDE x0, w3
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_const_wide_16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_const_wide_32: /* 0x17 */
    NAME_START nterp_op_const_wide_32

    /* const-wide/32 vAA, #+BBBBbbbb */
    FETCH   w0, 1                       // x0<- 000000000000bbbb (low)
    lsr     w3, wINST, #8               // w3<- AA
    FETCH_S x2, 2                       // x2<- ssssssssssssBBBB (high)
    FETCH_ADVANCE_INST 3                // advance rPC, load wINST
    GET_INST_OPCODE ip                  // extract opcode from wINST
    orr     x0, x0, x2, lsl #16         // x0<- ssssssssBBBBbbbb
    SET_VREG_WIDE x0, w3
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_const_wide_32

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_const_wide: /* 0x18 */
    NAME_START nterp_op_const_wide

    /* const-wide vAA, #+HHHHhhhhBBBBbbbb */
    FETCH w0, 1                         // w0<- bbbb (low)
    FETCH w1, 2                         // w1<- BBBB (low middle)
    FETCH w2, 3                         // w2<- hhhh (high middle)
    FETCH w3, 4                         // w3<- HHHH (high)
    lsr     w4, wINST, #8               // r4<- AA
    FETCH_ADVANCE_INST 5                // advance rPC, load wINST
    GET_INST_OPCODE ip                  // extract opcode from wINST
    orr     w0, w0, w1, lsl #16         // w0<-         BBBBbbbb
    orr     x0, x0, x2, lsl #32         // w0<-     hhhhBBBBbbbb
    orr     x0, x0, x3, lsl #48         // w0<- HHHHhhhhBBBBbbbb
    SET_VREG_WIDE x0, w4
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_const_wide

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_const_wide_high16: /* 0x19 */
    NAME_START nterp_op_const_wide_high16

    /* const-wide/high16 vAA, #+BBBB000000000000 */
    FETCH w0, 1                         // w0<- 0000BBBB (zero-extended)
    lsr     w1, wINST, #8               // w1<- AA
    FETCH_ADVANCE_INST 2                // advance rPC, load wINST
    lsl     x0, x0, #48
    SET_VREG_WIDE x0, w1
    GET_INST_OPCODE ip                  // extract opcode from wINST
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_const_wide_high16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_const_string: /* 0x1a */
    NAME_START nterp_op_const_string

   /* const/string vAA, String@BBBB */
   // Fast-path which gets the object from thread-local cache.
   FETCH_FROM_THREAD_CACHE x0, 2f
   cbnz wMR, 3f
1:
   lsr     w1, wINST, #8               // w1<- AA
   .if 0
   FETCH_ADVANCE_INST 3                // advance rPC, load wINST
   .else
   FETCH_ADVANCE_INST 2                // advance rPC, load wINST
   .endif
   GET_INST_OPCODE ip                  // extract opcode from wINST
   SET_VREG_OBJECT w0, w1              // vAA <- value
   GOTO_OPCODE ip                      // jump to next instruction
2:
   EXPORT_PC
   mov x0, xSELF
   ldr x1, [sp]
   mov x2, xPC
   bl nterp_load_object
   b 1b
3:
   bl art_quick_read_barrier_mark_reg00
   b 1b

    NAME_END nterp_op_const_string

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_const_string_jumbo: /* 0x1b */
    NAME_START nterp_op_const_string_jumbo

   /* const/string vAA, String@BBBBBBBB */
   // Fast-path which gets the object from thread-local cache.
   FETCH_FROM_THREAD_CACHE x0, 2f
   cbnz wMR, 3f
1:
   lsr     w1, wINST, #8               // w1<- AA
   .if 1
   FETCH_ADVANCE_INST 3                // advance rPC, load wINST
   .else
   FETCH_ADVANCE_INST 2                // advance rPC, load wINST
   .endif
   GET_INST_OPCODE ip                  // extract opcode from wINST
   SET_VREG_OBJECT w0, w1              // vAA <- value
   GOTO_OPCODE ip                      // jump to next instruction
2:
   EXPORT_PC
   mov x0, xSELF
   ldr x1, [sp]
   mov x2, xPC
   bl nterp_load_object
   b 1b
3:
   bl art_quick_read_barrier_mark_reg00
   b 1b

    NAME_END nterp_op_const_string_jumbo

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_const_class: /* 0x1c */
    NAME_START nterp_op_const_class

   // Fast-path which gets the object from thread-local cache.
   FETCH_FROM_THREAD_CACHE x0, 2f
   cbnz wMR, 3f
1:
   lsr     w1, wINST, #8               // w1<- AA
   .if 0
   FETCH_ADVANCE_INST 3                // advance rPC, load wINST
   .else
   FETCH_ADVANCE_INST 2                // advance rPC, load wINST
   .endif
   GET_INST_OPCODE ip                  // extract opcode from wINST
   SET_VREG_OBJECT w0, w1              // vAA <- value
   GOTO_OPCODE ip                      // jump to next instruction
2:
   EXPORT_PC
   mov x0, xSELF
   ldr x1, [sp]
   mov x2, xPC
   bl nterp_get_class_or_allocate_object
   b 1b
3:
   bl art_quick_read_barrier_mark_reg00
   b 1b

    NAME_END nterp_op_const_class

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_monitor_enter: /* 0x1d */
    NAME_START nterp_op_monitor_enter

/*
 * Synchronize on an object.
 */
    /* monitor-enter vAA */
    EXPORT_PC
    lsr      w2, wINST, #8               // w2<- AA
    GET_VREG w0, w2
    bl art_quick_lock_object
    FETCH_ADVANCE_INST 1
    GET_INST_OPCODE ip                   // extract opcode from rINST
    GOTO_OPCODE ip                       // jump to next instruction

    NAME_END nterp_op_monitor_enter

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_monitor_exit: /* 0x1e */
    NAME_START nterp_op_monitor_exit

/*
 * Unlock an object.
 *
 * Exceptions that occur when unlocking a monitor need to appear as
 * if they happened at the following instruction.  See the Dalvik
 * instruction spec.
 */
    /* monitor-exit vAA */
    EXPORT_PC
    lsr      w2, wINST, #8               // w2<- AA
    GET_VREG w0, w2
    bl art_quick_unlock_object
    FETCH_ADVANCE_INST 1
    GET_INST_OPCODE ip                   // extract opcode from rINST
    GOTO_OPCODE ip                       // jump to next instruction

    NAME_END nterp_op_monitor_exit

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_check_cast: /* 0x1f */
    NAME_START nterp_op_check_cast

   // Fast-path which gets the class from thread-local cache.
   EXPORT_PC
   FETCH_FROM_THREAD_CACHE x1, 3f
   cbnz    wMR, 4f
1:
   lsr     w2, wINST, #8               // w2<- A
   GET_VREG w0, w2                     // w0<- vA (object)
   cbz     w0, 2f
   bl      art_quick_check_instance_of
2:
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
3:
   mov     x0, xSELF
   ldr     x1, [sp]
   mov     x2, xPC
   bl      nterp_get_class_or_allocate_object
   mov     x1, x0
   b       1b
4:
   bl      art_quick_read_barrier_mark_reg01
   b       1b

    NAME_END nterp_op_check_cast

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_instance_of: /* 0x20 */
    NAME_START nterp_op_instance_of

   /* instance-of vA, vB, class@CCCC */
   // Fast-path which gets the class from thread-local cache.
   EXPORT_PC
   FETCH_FROM_THREAD_CACHE x1, 3f
   cbnz    wMR, 4f
1:
   lsr     w2, wINST, #12              // w2<- B
   GET_VREG w0, w2                     // w0<- vB (object)
   cbz     w0, 2f
   bl      artInstanceOfFromCode
2:
   ubfx    w1, wINST, #8, #4           // w1<- A
   SET_VREG w0, w1
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
3:
   mov     x0, xSELF
   ldr     x1, [sp]
   mov     x2, xPC
   bl      nterp_get_class_or_allocate_object
   mov     x1, x0
   b       1b
4:
   bl      art_quick_read_barrier_mark_reg01
   b       1b

    NAME_END nterp_op_instance_of

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_array_length: /* 0x21 */
    NAME_START nterp_op_array_length

    /*
     * Return the length of an array.
     */
    lsr     w1, wINST, #12              // w1<- B
    ubfx    w2, wINST, #8, #4           // w2<- A
    GET_VREG w0, w1                     // w0<- vB (object ref)
    cbz     w0, common_errNullObject    // bail if null
    FETCH_ADVANCE_INST 1                // advance rPC, load rINST
    ldr     w3, [x0, #MIRROR_ARRAY_LENGTH_OFFSET]    // w3<- array length
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG w3, w2                     // vB<- length
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_array_length

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_new_instance: /* 0x22 */
    NAME_START nterp_op_new_instance

   EXPORT_PC
   // Fast-path which gets the class from thread-local cache.
   FETCH_FROM_THREAD_CACHE x0, 2f
   cbnz    wMR, 3f
4:
   ldr     lr, [xSELF, #THREAD_ALLOC_OBJECT_ENTRYPOINT_OFFSET]
   blr     lr
1:
   lsr     w1, wINST, #8               // w1 <- A
   SET_VREG_OBJECT w0, w1              // fp[A] <- value
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
2:
   mov     x0, xSELF
   ldr     x1, [sp]
   mov     x2, xPC
   bl      nterp_get_class_or_allocate_object
   b       1b
3:
   bl      art_quick_read_barrier_mark_reg00
   b       4b
    NAME_END nterp_op_new_instance

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_new_array: /* 0x23 */
    NAME_START nterp_op_new_array

  b NterpNewArray
    NAME_END nterp_op_new_array

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_filled_new_array: /* 0x24 */
    NAME_START nterp_op_filled_new_array

/*
 * Create a new array with elements filled from registers.
 *
 * for: filled-new-array, filled-new-array/range
 */
    /* op vB, {vD, vE, vF, vG, vA}, class@CCCC */
    /* op {vCCCC..v(CCCC+AA-1)}, type@BBBB */
    EXPORT_PC
    mov     x0, xSELF
    ldr     x1, [sp]
    mov     x2, xFP
    mov     x3, xPC
    bl      nterp_filled_new_array
    FETCH_ADVANCE_INST 3                // advance rPC, load rINST
    GET_INST_OPCODE ip                  // extract opcode from rINST
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_filled_new_array

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_filled_new_array_range: /* 0x25 */
    NAME_START nterp_op_filled_new_array_range

/*
 * Create a new array with elements filled from registers.
 *
 * for: filled-new-array, filled-new-array/range
 */
    /* op vB, {vD, vE, vF, vG, vA}, class@CCCC */
    /* op {vCCCC..v(CCCC+AA-1)}, type@BBBB */
    EXPORT_PC
    mov     x0, xSELF
    ldr     x1, [sp]
    mov     x2, xFP
    mov     x3, xPC
    bl      nterp_filled_new_array_range
    FETCH_ADVANCE_INST 3                // advance rPC, load rINST
    GET_INST_OPCODE ip                  // extract opcode from rINST
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_filled_new_array_range

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_fill_array_data: /* 0x26 */
    NAME_START nterp_op_fill_array_data

    /* fill-array-data vAA, +BBBBBBBB */
    EXPORT_PC
    FETCH   w0, 1                       // x0<- 000000000000bbbb (lo)
    FETCH_S x1, 2                       // x1<- ssssssssssssBBBB (hi)
    lsr     w3, wINST, #8               // w3<- AA
    orr     x0, x0, x1, lsl #16         // x0<- ssssssssBBBBbbbb
    GET_VREG w1, w3                     // w1<- vAA (array object)
    add     x0, xPC, x0, lsl #1         // x0<- PC + ssssssssBBBBbbbb*2 (array data off.)
    bl      art_quick_handle_fill_data
    FETCH_ADVANCE_INST 3                // advance rPC, load rINST
    GET_INST_OPCODE ip                  // extract opcode from rINST
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_fill_array_data

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_throw: /* 0x27 */
    NAME_START nterp_op_throw

  EXPORT_PC
  lsr      w2, wINST, #8               // r2<- AA
  GET_VREG w0, w2                      // r0<- vAA (exception object)
  mov x1, xSELF
  bl art_quick_deliver_exception
  brk 0
    NAME_END nterp_op_throw

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_goto: /* 0x28 */
    NAME_START nterp_op_goto

/*
 * Unconditional branch, 8-bit offset.
 *
 * The branch distance is a signed code-unit offset, which we need to
 * double to get a byte offset.
 */
    /* goto +AA */
    sbfx    wINST, wINST, #8, #8           // wINST<- ssssssAA (sign-extended)
    BRANCH

    NAME_END nterp_op_goto

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_goto_16: /* 0x29 */
    NAME_START nterp_op_goto_16

/*
 * Unconditional branch, 16-bit offset.
 *
 * The branch distance is a signed code-unit offset, which we need to
 * double to get a byte offset.
 */
    /* goto/16 +AAAA */
    FETCH_S wINST, 1                    // wINST<- ssssAAAA (sign-extended)
    BRANCH

    NAME_END nterp_op_goto_16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_goto_32: /* 0x2a */
    NAME_START nterp_op_goto_32

/*
 * Unconditional branch, 32-bit offset.
 *
 * The branch distance is a signed code-unit offset, which we need to
 * double to get a byte offset.
 *
 * Because we need the SF bit set, we'll use an adds
 * to convert from Dalvik offset to byte offset.
 */
    /* goto/32 +AAAAAAAA */
    FETCH w0, 1                         // w0<- aaaa (lo)
    FETCH w1, 2                         // w1<- AAAA (hi)
    orr     wINST, w0, w1, lsl #16      // wINST<- AAAAaaaa
    BRANCH

    NAME_END nterp_op_goto_32

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_packed_switch: /* 0x2b */
    NAME_START nterp_op_packed_switch

/*
 * Handle a packed-switch or sparse-switch instruction.  In both cases
 * we decode it and hand it off to a helper function.
 *
 * We don't really expect backward branches in a switch statement, but
 * they're perfectly legal, so we check for them here.
 *
 * for: packed-switch, sparse-switch
 */
    /* op vAA, +BBBB */
    FETCH   w0, 1                       // x0<- 000000000000bbbb (lo)
    FETCH_S x1, 2                       // x1<- ssssssssssssBBBB (hi)
    lsr     w3, wINST, #8               // w3<- AA
    orr     x0, x0, x1, lsl #16         // x0<- ssssssssBBBBbbbb
    GET_VREG w1, w3                     // w1<- vAA
    add     x0, xPC, x0, lsl #1         // x0<- PC + ssssssssBBBBbbbb*2
    bl      NterpDoPackedSwitch                       // w0<- code-unit branch offset
    sxtw    xINST, w0
    BRANCH

    NAME_END nterp_op_packed_switch

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sparse_switch: /* 0x2c */
    NAME_START nterp_op_sparse_switch

/*
 * Handle a packed-switch or sparse-switch instruction.  In both cases
 * we decode it and hand it off to a helper function.
 *
 * We don't really expect backward branches in a switch statement, but
 * they're perfectly legal, so we check for them here.
 *
 * for: packed-switch, sparse-switch
 */
    /* op vAA, +BBBB */
    FETCH   w0, 1                       // x0<- 000000000000bbbb (lo)
    FETCH_S x1, 2                       // x1<- ssssssssssssBBBB (hi)
    lsr     w3, wINST, #8               // w3<- AA
    orr     x0, x0, x1, lsl #16         // x0<- ssssssssBBBBbbbb
    GET_VREG w1, w3                     // w1<- vAA
    add     x0, xPC, x0, lsl #1         // x0<- PC + ssssssssBBBBbbbb*2
    bl      NterpDoSparseSwitch                       // w0<- code-unit branch offset
    sxtw    xINST, w0
    BRANCH

/*
 * Return a 32-bit value.
 */
    NAME_END nterp_op_sparse_switch

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_cmpl_float: /* 0x2d */
    NAME_START nterp_op_cmpl_float

    /*
     * Compare two floating-point values.  Puts 0, 1, or -1 into the
     * destination register based on the results of the comparison.
     */
    /* op vAA, vBB, vCC */
    FETCH w0, 1                         // w0<- CCBB
    lsr     w4, wINST, #8               // w4<- AA
    and     w2, w0, #255                // w2<- BB
    lsr     w3, w0, #8                  // w3<- CC
    GET_VREG s1, w2
    GET_VREG s2, w3
    fcmp s1, s2
    cset w0, ne
    cneg w0, w0, lt
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG w0, w4                     // vAA<- w0
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_cmpl_float

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_cmpg_float: /* 0x2e */
    NAME_START nterp_op_cmpg_float

    /*
     * Compare two floating-point values.  Puts 0, 1, or -1 into the
     * destination register based on the results of the comparison.
     */
    /* op vAA, vBB, vCC */
    FETCH w0, 1                         // w0<- CCBB
    lsr     w4, wINST, #8               // w4<- AA
    and     w2, w0, #255                // w2<- BB
    lsr     w3, w0, #8                  // w3<- CC
    GET_VREG s1, w2
    GET_VREG s2, w3
    fcmp s1, s2
    cset w0, ne
    cneg w0, w0, cc
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG w0, w4                     // vAA<- w0
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_cmpg_float

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_cmpl_double: /* 0x2f */
    NAME_START nterp_op_cmpl_double

    /*
     * Compare two floating-point values.  Puts 0, 1, or -1 into the
     * destination register based on the results of the comparison.
     */
    /* op vAA, vBB, vCC */
    FETCH w0, 1                         // w0<- CCBB
    lsr     w4, wINST, #8               // w4<- AA
    and     w2, w0, #255                // w2<- BB
    lsr     w3, w0, #8                  // w3<- CC
    GET_VREG_DOUBLE d1, w2
    GET_VREG_DOUBLE d2, w3
    fcmp d1, d2
    cset w0, ne
    cneg w0, w0, lt
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG w0, w4                     // vAA<- w0
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_cmpl_double

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_cmpg_double: /* 0x30 */
    NAME_START nterp_op_cmpg_double

    /*
     * Compare two floating-point values.  Puts 0, 1, or -1 into the
     * destination register based on the results of the comparison.
     */
    /* op vAA, vBB, vCC */
    FETCH w0, 1                         // w0<- CCBB
    lsr     w4, wINST, #8               // w4<- AA
    and     w2, w0, #255                // w2<- BB
    lsr     w3, w0, #8                  // w3<- CC
    GET_VREG_DOUBLE d1, w2
    GET_VREG_DOUBLE d2, w3
    fcmp d1, d2
    cset w0, ne
    cneg w0, w0, cc
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG w0, w4                     // vAA<- w0
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_cmpg_double

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_cmp_long: /* 0x31 */
    NAME_START nterp_op_cmp_long

    FETCH w0, 1                         // w0<- CCBB
    lsr     w4, wINST, #8               // w4<- AA
    and     w2, w0, #255                // w2<- BB
    lsr     w3, w0, #8                  // w3<- CC
    GET_VREG_WIDE x1, w2
    GET_VREG_WIDE x2, w3
    cmp     x1, x2
    cset    w0, ne
    cneg    w0, w0, lt
    FETCH_ADVANCE_INST 2                // advance rPC, load wINST
    SET_VREG w0, w4
    GET_INST_OPCODE ip                  // extract opcode from wINST
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_cmp_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_if_eq: /* 0x32 */
    NAME_START nterp_op_if_eq

    /*
     * Generic two-operand compare-and-branch operation.  Provide a "condition"
     * fragment that specifies the comparison to perform.
     *
     * For: if-eq, if-ne, if-lt, if-ge, if-gt, if-le
     */
    /* if-cmp vA, vB, +CCCC */
    lsr     w1, wINST, #12              // w1<- B
    ubfx    w0, wINST, #8, #4           // w0<- A
    GET_VREG w3, w1                     // w3<- vB
    GET_VREG w2, w0                     // w2<- vA
    cmp     w2, w3                      // compare (vA, vB)
    b.eq 1f
    FETCH_ADVANCE_INST 2
    GET_INST_OPCODE ip                  // extract opcode from wINST
    GOTO_OPCODE ip                      // jump to next instruction
1:
    FETCH_S wINST, 1                    // wINST<- branch offset, in code units
    BRANCH

    NAME_END nterp_op_if_eq

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_if_ne: /* 0x33 */
    NAME_START nterp_op_if_ne

    /*
     * Generic two-operand compare-and-branch operation.  Provide a "condition"
     * fragment that specifies the comparison to perform.
     *
     * For: if-eq, if-ne, if-lt, if-ge, if-gt, if-le
     */
    /* if-cmp vA, vB, +CCCC */
    lsr     w1, wINST, #12              // w1<- B
    ubfx    w0, wINST, #8, #4           // w0<- A
    GET_VREG w3, w1                     // w3<- vB
    GET_VREG w2, w0                     // w2<- vA
    cmp     w2, w3                      // compare (vA, vB)
    b.ne 1f
    FETCH_ADVANCE_INST 2
    GET_INST_OPCODE ip                  // extract opcode from wINST
    GOTO_OPCODE ip                      // jump to next instruction
1:
    FETCH_S wINST, 1                    // wINST<- branch offset, in code units
    BRANCH

    NAME_END nterp_op_if_ne

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_if_lt: /* 0x34 */
    NAME_START nterp_op_if_lt

    /*
     * Generic two-operand compare-and-branch operation.  Provide a "condition"
     * fragment that specifies the comparison to perform.
     *
     * For: if-eq, if-ne, if-lt, if-ge, if-gt, if-le
     */
    /* if-cmp vA, vB, +CCCC */
    lsr     w1, wINST, #12              // w1<- B
    ubfx    w0, wINST, #8, #4           // w0<- A
    GET_VREG w3, w1                     // w3<- vB
    GET_VREG w2, w0                     // w2<- vA
    cmp     w2, w3                      // compare (vA, vB)
    b.lt 1f
    FETCH_ADVANCE_INST 2
    GET_INST_OPCODE ip                  // extract opcode from wINST
    GOTO_OPCODE ip                      // jump to next instruction
1:
    FETCH_S wINST, 1                    // wINST<- branch offset, in code units
    BRANCH

    NAME_END nterp_op_if_lt

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_if_ge: /* 0x35 */
    NAME_START nterp_op_if_ge

    /*
     * Generic two-operand compare-and-branch operation.  Provide a "condition"
     * fragment that specifies the comparison to perform.
     *
     * For: if-eq, if-ne, if-lt, if-ge, if-gt, if-le
     */
    /* if-cmp vA, vB, +CCCC */
    lsr     w1, wINST, #12              // w1<- B
    ubfx    w0, wINST, #8, #4           // w0<- A
    GET_VREG w3, w1                     // w3<- vB
    GET_VREG w2, w0                     // w2<- vA
    cmp     w2, w3                      // compare (vA, vB)
    b.ge 1f
    FETCH_ADVANCE_INST 2
    GET_INST_OPCODE ip                  // extract opcode from wINST
    GOTO_OPCODE ip                      // jump to next instruction
1:
    FETCH_S wINST, 1                    // wINST<- branch offset, in code units
    BRANCH

    NAME_END nterp_op_if_ge

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_if_gt: /* 0x36 */
    NAME_START nterp_op_if_gt

    /*
     * Generic two-operand compare-and-branch operation.  Provide a "condition"
     * fragment that specifies the comparison to perform.
     *
     * For: if-eq, if-ne, if-lt, if-ge, if-gt, if-le
     */
    /* if-cmp vA, vB, +CCCC */
    lsr     w1, wINST, #12              // w1<- B
    ubfx    w0, wINST, #8, #4           // w0<- A
    GET_VREG w3, w1                     // w3<- vB
    GET_VREG w2, w0                     // w2<- vA
    cmp     w2, w3                      // compare (vA, vB)
    b.gt 1f
    FETCH_ADVANCE_INST 2
    GET_INST_OPCODE ip                  // extract opcode from wINST
    GOTO_OPCODE ip                      // jump to next instruction
1:
    FETCH_S wINST, 1                    // wINST<- branch offset, in code units
    BRANCH

    NAME_END nterp_op_if_gt

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_if_le: /* 0x37 */
    NAME_START nterp_op_if_le

    /*
     * Generic two-operand compare-and-branch operation.  Provide a "condition"
     * fragment that specifies the comparison to perform.
     *
     * For: if-eq, if-ne, if-lt, if-ge, if-gt, if-le
     */
    /* if-cmp vA, vB, +CCCC */
    lsr     w1, wINST, #12              // w1<- B
    ubfx    w0, wINST, #8, #4           // w0<- A
    GET_VREG w3, w1                     // w3<- vB
    GET_VREG w2, w0                     // w2<- vA
    cmp     w2, w3                      // compare (vA, vB)
    b.le 1f
    FETCH_ADVANCE_INST 2
    GET_INST_OPCODE ip                  // extract opcode from wINST
    GOTO_OPCODE ip                      // jump to next instruction
1:
    FETCH_S wINST, 1                    // wINST<- branch offset, in code units
    BRANCH

    NAME_END nterp_op_if_le

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_if_eqz: /* 0x38 */
    NAME_START nterp_op_if_eqz

    /*
     * Generic one-operand compare-and-branch operation.  Provide a "condition"
     * fragment that specifies the comparison to perform.
     *
     * for: if-eqz, if-nez, if-ltz, if-gez, if-gtz, if-lez
     */
    /* if-cmp vAA, +BBBB */
    lsr     w0, wINST, #8               // w0<- AA
    GET_VREG w2, w0                     // w2<- vAA
    .if 0
    cmp     w2, #0                      // compare (vA, 0)
    .endif
    cbz     w2, 1f
    FETCH_ADVANCE_INST 2
    GET_INST_OPCODE ip                  // extract opcode from wINST
    GOTO_OPCODE ip                      // jump to next instruction
1:
    FETCH_S wINST, 1                    // w1<- branch offset, in code units
    BRANCH

    NAME_END nterp_op_if_eqz

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_if_nez: /* 0x39 */
    NAME_START nterp_op_if_nez

    /*
     * Generic one-operand compare-and-branch operation.  Provide a "condition"
     * fragment that specifies the comparison to perform.
     *
     * for: if-eqz, if-nez, if-ltz, if-gez, if-gtz, if-lez
     */
    /* if-cmp vAA, +BBBB */
    lsr     w0, wINST, #8               // w0<- AA
    GET_VREG w2, w0                     // w2<- vAA
    .if 0
    cmp     w2, #0                      // compare (vA, 0)
    .endif
    cbnz    w2, 1f
    FETCH_ADVANCE_INST 2
    GET_INST_OPCODE ip                  // extract opcode from wINST
    GOTO_OPCODE ip                      // jump to next instruction
1:
    FETCH_S wINST, 1                    // w1<- branch offset, in code units
    BRANCH

    NAME_END nterp_op_if_nez

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_if_ltz: /* 0x3a */
    NAME_START nterp_op_if_ltz

    /*
     * Generic one-operand compare-and-branch operation.  Provide a "condition"
     * fragment that specifies the comparison to perform.
     *
     * for: if-eqz, if-nez, if-ltz, if-gez, if-gtz, if-lez
     */
    /* if-cmp vAA, +BBBB */
    lsr     w0, wINST, #8               // w0<- AA
    GET_VREG w2, w0                     // w2<- vAA
    .if 0
    cmp     w2, #0                      // compare (vA, 0)
    .endif
    tbnz    w2, #31, 1f
    FETCH_ADVANCE_INST 2
    GET_INST_OPCODE ip                  // extract opcode from wINST
    GOTO_OPCODE ip                      // jump to next instruction
1:
    FETCH_S wINST, 1                    // w1<- branch offset, in code units
    BRANCH

    NAME_END nterp_op_if_ltz

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_if_gez: /* 0x3b */
    NAME_START nterp_op_if_gez

    /*
     * Generic one-operand compare-and-branch operation.  Provide a "condition"
     * fragment that specifies the comparison to perform.
     *
     * for: if-eqz, if-nez, if-ltz, if-gez, if-gtz, if-lez
     */
    /* if-cmp vAA, +BBBB */
    lsr     w0, wINST, #8               // w0<- AA
    GET_VREG w2, w0                     // w2<- vAA
    .if 0
    cmp     w2, #0                      // compare (vA, 0)
    .endif
    tbz     w2, #31, 1f
    FETCH_ADVANCE_INST 2
    GET_INST_OPCODE ip                  // extract opcode from wINST
    GOTO_OPCODE ip                      // jump to next instruction
1:
    FETCH_S wINST, 1                    // w1<- branch offset, in code units
    BRANCH

    NAME_END nterp_op_if_gez

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_if_gtz: /* 0x3c */
    NAME_START nterp_op_if_gtz

    /*
     * Generic one-operand compare-and-branch operation.  Provide a "condition"
     * fragment that specifies the comparison to perform.
     *
     * for: if-eqz, if-nez, if-ltz, if-gez, if-gtz, if-lez
     */
    /* if-cmp vAA, +BBBB */
    lsr     w0, wINST, #8               // w0<- AA
    GET_VREG w2, w0                     // w2<- vAA
    .if 1
    cmp     w2, #0                      // compare (vA, 0)
    .endif
    b.gt 1f
    FETCH_ADVANCE_INST 2
    GET_INST_OPCODE ip                  // extract opcode from wINST
    GOTO_OPCODE ip                      // jump to next instruction
1:
    FETCH_S wINST, 1                    // w1<- branch offset, in code units
    BRANCH

    NAME_END nterp_op_if_gtz

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_if_lez: /* 0x3d */
    NAME_START nterp_op_if_lez

    /*
     * Generic one-operand compare-and-branch operation.  Provide a "condition"
     * fragment that specifies the comparison to perform.
     *
     * for: if-eqz, if-nez, if-ltz, if-gez, if-gtz, if-lez
     */
    /* if-cmp vAA, +BBBB */
    lsr     w0, wINST, #8               // w0<- AA
    GET_VREG w2, w0                     // w2<- vAA
    .if 1
    cmp     w2, #0                      // compare (vA, 0)
    .endif
    b.le 1f
    FETCH_ADVANCE_INST 2
    GET_INST_OPCODE ip                  // extract opcode from wINST
    GOTO_OPCODE ip                      // jump to next instruction
1:
    FETCH_S wINST, 1                    // w1<- branch offset, in code units
    BRANCH

    NAME_END nterp_op_if_lez

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_3e: /* 0x3e */
    NAME_START nterp_op_unused_3e

    brk 42

    NAME_END nterp_op_unused_3e

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_3f: /* 0x3f */
    NAME_START nterp_op_unused_3f

    brk 42

    NAME_END nterp_op_unused_3f

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_40: /* 0x40 */
    NAME_START nterp_op_unused_40

    brk 42

    NAME_END nterp_op_unused_40

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_41: /* 0x41 */
    NAME_START nterp_op_unused_41

    brk 42

    NAME_END nterp_op_unused_41

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_42: /* 0x42 */
    NAME_START nterp_op_unused_42

    brk 42

    NAME_END nterp_op_unused_42

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_43: /* 0x43 */
    NAME_START nterp_op_unused_43

    brk 42

    NAME_END nterp_op_unused_43

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aget: /* 0x44 */
    NAME_START nterp_op_aget

/*
 * Array get.  vAA <- vBB[vCC].
 *
 * for: aget, aget-boolean, aget-byte, aget-char, aget-short, aget-wide, aget-object
 *
 */
    FETCH_B w2, 1, 0                    // w2<- BB
    lsr     w9, wINST, #8               // w9<- AA
    FETCH_B w3, 1, 1                    // w3<- CC
    GET_VREG w0, w2                     // w0<- vBB (array object)
    GET_VREG w1, w3                     // w1<- vCC (requested index)
    cbz     x0, common_errNullObject    // bail if null array object.
    ldr     w3, [x0, #MIRROR_ARRAY_LENGTH_OFFSET]    // w3<- arrayObj->length
    add     x0, x0, w1, uxtw #2    // w0<- arrayObj + index*width
    cmp     w1, w3                      // compare unsigned index, length
    bcs     common_errArrayIndex        // index >= length, bail
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
    GET_INST_OPCODE x10                 // extract opcode from wINST
    .if 0
    ldr     x2, [x0, #MIRROR_INT_ARRAY_DATA_OFFSET]     // x2<- vBB[vCC]
    SET_VREG_WIDE x2, w9
    GOTO_OPCODE x10                     // jump to next instruction
    .elseif 0
    ldr   w2, [x0, #MIRROR_INT_ARRAY_DATA_OFFSET]     // w2<- vBB[vCC]
    cbnz wMR, 2f
1:
    SET_VREG_OBJECT w2, w9              // vAA<- w2
    GOTO_OPCODE x10                     // jump to next instruction
2:
    bl art_quick_read_barrier_mark_reg02
    b 1b
    .else
    ldr   w2, [x0, #MIRROR_INT_ARRAY_DATA_OFFSET]     // w2<- vBB[vCC]
    SET_VREG w2, w9                     // vAA<- w2
    GOTO_OPCODE x10                     // jump to next instruction
    .endif

    NAME_END nterp_op_aget

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aget_wide: /* 0x45 */
    NAME_START nterp_op_aget_wide

/*
 * Array get.  vAA <- vBB[vCC].
 *
 * for: aget, aget-boolean, aget-byte, aget-char, aget-short, aget-wide, aget-object
 *
 */
    FETCH_B w2, 1, 0                    // w2<- BB
    lsr     w9, wINST, #8               // w9<- AA
    FETCH_B w3, 1, 1                    // w3<- CC
    GET_VREG w0, w2                     // w0<- vBB (array object)
    GET_VREG w1, w3                     // w1<- vCC (requested index)
    cbz     x0, common_errNullObject    // bail if null array object.
    ldr     w3, [x0, #MIRROR_ARRAY_LENGTH_OFFSET]    // w3<- arrayObj->length
    add     x0, x0, w1, uxtw #3    // w0<- arrayObj + index*width
    cmp     w1, w3                      // compare unsigned index, length
    bcs     common_errArrayIndex        // index >= length, bail
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
    GET_INST_OPCODE x10                 // extract opcode from wINST
    .if 1
    ldr     x2, [x0, #MIRROR_WIDE_ARRAY_DATA_OFFSET]     // x2<- vBB[vCC]
    SET_VREG_WIDE x2, w9
    GOTO_OPCODE x10                     // jump to next instruction
    .elseif 0
    ldr   w2, [x0, #MIRROR_WIDE_ARRAY_DATA_OFFSET]     // w2<- vBB[vCC]
    cbnz wMR, 2f
1:
    SET_VREG_OBJECT w2, w9              // vAA<- w2
    GOTO_OPCODE x10                     // jump to next instruction
2:
    bl art_quick_read_barrier_mark_reg02
    b 1b
    .else
    ldr   w2, [x0, #MIRROR_WIDE_ARRAY_DATA_OFFSET]     // w2<- vBB[vCC]
    SET_VREG w2, w9                     // vAA<- w2
    GOTO_OPCODE x10                     // jump to next instruction
    .endif

    NAME_END nterp_op_aget_wide

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aget_object: /* 0x46 */
    NAME_START nterp_op_aget_object

/*
 * Array get.  vAA <- vBB[vCC].
 *
 * for: aget, aget-boolean, aget-byte, aget-char, aget-short, aget-wide, aget-object
 *
 */
    FETCH_B w2, 1, 0                    // w2<- BB
    lsr     w9, wINST, #8               // w9<- AA
    FETCH_B w3, 1, 1                    // w3<- CC
    GET_VREG w0, w2                     // w0<- vBB (array object)
    GET_VREG w1, w3                     // w1<- vCC (requested index)
    cbz     x0, common_errNullObject    // bail if null array object.
    ldr     w3, [x0, #MIRROR_ARRAY_LENGTH_OFFSET]    // w3<- arrayObj->length
    add     x0, x0, w1, uxtw #2    // w0<- arrayObj + index*width
    cmp     w1, w3                      // compare unsigned index, length
    bcs     common_errArrayIndex        // index >= length, bail
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
    GET_INST_OPCODE x10                 // extract opcode from wINST
    .if 0
    ldr     x2, [x0, #MIRROR_OBJECT_ARRAY_DATA_OFFSET]     // x2<- vBB[vCC]
    SET_VREG_WIDE x2, w9
    GOTO_OPCODE x10                     // jump to next instruction
    .elseif 1
    ldr   w2, [x0, #MIRROR_OBJECT_ARRAY_DATA_OFFSET]     // w2<- vBB[vCC]
    cbnz wMR, 2f
1:
    SET_VREG_OBJECT w2, w9              // vAA<- w2
    GOTO_OPCODE x10                     // jump to next instruction
2:
    bl art_quick_read_barrier_mark_reg02
    b 1b
    .else
    ldr   w2, [x0, #MIRROR_OBJECT_ARRAY_DATA_OFFSET]     // w2<- vBB[vCC]
    SET_VREG w2, w9                     // vAA<- w2
    GOTO_OPCODE x10                     // jump to next instruction
    .endif

    NAME_END nterp_op_aget_object

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aget_boolean: /* 0x47 */
    NAME_START nterp_op_aget_boolean

/*
 * Array get.  vAA <- vBB[vCC].
 *
 * for: aget, aget-boolean, aget-byte, aget-char, aget-short, aget-wide, aget-object
 *
 */
    FETCH_B w2, 1, 0                    // w2<- BB
    lsr     w9, wINST, #8               // w9<- AA
    FETCH_B w3, 1, 1                    // w3<- CC
    GET_VREG w0, w2                     // w0<- vBB (array object)
    GET_VREG w1, w3                     // w1<- vCC (requested index)
    cbz     x0, common_errNullObject    // bail if null array object.
    ldr     w3, [x0, #MIRROR_ARRAY_LENGTH_OFFSET]    // w3<- arrayObj->length
    add     x0, x0, w1, uxtw #0    // w0<- arrayObj + index*width
    cmp     w1, w3                      // compare unsigned index, length
    bcs     common_errArrayIndex        // index >= length, bail
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
    GET_INST_OPCODE x10                 // extract opcode from wINST
    .if 0
    ldr     x2, [x0, #MIRROR_BOOLEAN_ARRAY_DATA_OFFSET]     // x2<- vBB[vCC]
    SET_VREG_WIDE x2, w9
    GOTO_OPCODE x10                     // jump to next instruction
    .elseif 0
    ldrb   w2, [x0, #MIRROR_BOOLEAN_ARRAY_DATA_OFFSET]     // w2<- vBB[vCC]
    cbnz wMR, 2f
1:
    SET_VREG_OBJECT w2, w9              // vAA<- w2
    GOTO_OPCODE x10                     // jump to next instruction
2:
    bl art_quick_read_barrier_mark_reg02
    b 1b
    .else
    ldrb   w2, [x0, #MIRROR_BOOLEAN_ARRAY_DATA_OFFSET]     // w2<- vBB[vCC]
    SET_VREG w2, w9                     // vAA<- w2
    GOTO_OPCODE x10                     // jump to next instruction
    .endif

    NAME_END nterp_op_aget_boolean

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aget_byte: /* 0x48 */
    NAME_START nterp_op_aget_byte

/*
 * Array get.  vAA <- vBB[vCC].
 *
 * for: aget, aget-boolean, aget-byte, aget-char, aget-short, aget-wide, aget-object
 *
 */
    FETCH_B w2, 1, 0                    // w2<- BB
    lsr     w9, wINST, #8               // w9<- AA
    FETCH_B w3, 1, 1                    // w3<- CC
    GET_VREG w0, w2                     // w0<- vBB (array object)
    GET_VREG w1, w3                     // w1<- vCC (requested index)
    cbz     x0, common_errNullObject    // bail if null array object.
    ldr     w3, [x0, #MIRROR_ARRAY_LENGTH_OFFSET]    // w3<- arrayObj->length
    add     x0, x0, w1, uxtw #0    // w0<- arrayObj + index*width
    cmp     w1, w3                      // compare unsigned index, length
    bcs     common_errArrayIndex        // index >= length, bail
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
    GET_INST_OPCODE x10                 // extract opcode from wINST
    .if 0
    ldr     x2, [x0, #MIRROR_BYTE_ARRAY_DATA_OFFSET]     // x2<- vBB[vCC]
    SET_VREG_WIDE x2, w9
    GOTO_OPCODE x10                     // jump to next instruction
    .elseif 0
    ldrsb   w2, [x0, #MIRROR_BYTE_ARRAY_DATA_OFFSET]     // w2<- vBB[vCC]
    cbnz wMR, 2f
1:
    SET_VREG_OBJECT w2, w9              // vAA<- w2
    GOTO_OPCODE x10                     // jump to next instruction
2:
    bl art_quick_read_barrier_mark_reg02
    b 1b
    .else
    ldrsb   w2, [x0, #MIRROR_BYTE_ARRAY_DATA_OFFSET]     // w2<- vBB[vCC]
    SET_VREG w2, w9                     // vAA<- w2
    GOTO_OPCODE x10                     // jump to next instruction
    .endif

    NAME_END nterp_op_aget_byte

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aget_char: /* 0x49 */
    NAME_START nterp_op_aget_char

/*
 * Array get.  vAA <- vBB[vCC].
 *
 * for: aget, aget-boolean, aget-byte, aget-char, aget-short, aget-wide, aget-object
 *
 */
    FETCH_B w2, 1, 0                    // w2<- BB
    lsr     w9, wINST, #8               // w9<- AA
    FETCH_B w3, 1, 1                    // w3<- CC
    GET_VREG w0, w2                     // w0<- vBB (array object)
    GET_VREG w1, w3                     // w1<- vCC (requested index)
    cbz     x0, common_errNullObject    // bail if null array object.
    ldr     w3, [x0, #MIRROR_ARRAY_LENGTH_OFFSET]    // w3<- arrayObj->length
    add     x0, x0, w1, uxtw #1    // w0<- arrayObj + index*width
    cmp     w1, w3                      // compare unsigned index, length
    bcs     common_errArrayIndex        // index >= length, bail
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
    GET_INST_OPCODE x10                 // extract opcode from wINST
    .if 0
    ldr     x2, [x0, #MIRROR_CHAR_ARRAY_DATA_OFFSET]     // x2<- vBB[vCC]
    SET_VREG_WIDE x2, w9
    GOTO_OPCODE x10                     // jump to next instruction
    .elseif 0
    ldrh   w2, [x0, #MIRROR_CHAR_ARRAY_DATA_OFFSET]     // w2<- vBB[vCC]
    cbnz wMR, 2f
1:
    SET_VREG_OBJECT w2, w9              // vAA<- w2
    GOTO_OPCODE x10                     // jump to next instruction
2:
    bl art_quick_read_barrier_mark_reg02
    b 1b
    .else
    ldrh   w2, [x0, #MIRROR_CHAR_ARRAY_DATA_OFFSET]     // w2<- vBB[vCC]
    SET_VREG w2, w9                     // vAA<- w2
    GOTO_OPCODE x10                     // jump to next instruction
    .endif

    NAME_END nterp_op_aget_char

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aget_short: /* 0x4a */
    NAME_START nterp_op_aget_short

/*
 * Array get.  vAA <- vBB[vCC].
 *
 * for: aget, aget-boolean, aget-byte, aget-char, aget-short, aget-wide, aget-object
 *
 */
    FETCH_B w2, 1, 0                    // w2<- BB
    lsr     w9, wINST, #8               // w9<- AA
    FETCH_B w3, 1, 1                    // w3<- CC
    GET_VREG w0, w2                     // w0<- vBB (array object)
    GET_VREG w1, w3                     // w1<- vCC (requested index)
    cbz     x0, common_errNullObject    // bail if null array object.
    ldr     w3, [x0, #MIRROR_ARRAY_LENGTH_OFFSET]    // w3<- arrayObj->length
    add     x0, x0, w1, uxtw #1    // w0<- arrayObj + index*width
    cmp     w1, w3                      // compare unsigned index, length
    bcs     common_errArrayIndex        // index >= length, bail
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
    GET_INST_OPCODE x10                 // extract opcode from wINST
    .if 0
    ldr     x2, [x0, #MIRROR_SHORT_ARRAY_DATA_OFFSET]     // x2<- vBB[vCC]
    SET_VREG_WIDE x2, w9
    GOTO_OPCODE x10                     // jump to next instruction
    .elseif 0
    ldrsh   w2, [x0, #MIRROR_SHORT_ARRAY_DATA_OFFSET]     // w2<- vBB[vCC]
    cbnz wMR, 2f
1:
    SET_VREG_OBJECT w2, w9              // vAA<- w2
    GOTO_OPCODE x10                     // jump to next instruction
2:
    bl art_quick_read_barrier_mark_reg02
    b 1b
    .else
    ldrsh   w2, [x0, #MIRROR_SHORT_ARRAY_DATA_OFFSET]     // w2<- vBB[vCC]
    SET_VREG w2, w9                     // vAA<- w2
    GOTO_OPCODE x10                     // jump to next instruction
    .endif

    NAME_END nterp_op_aget_short

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aput: /* 0x4b */
    NAME_START nterp_op_aput

/*
 * Array put.  vBB[vCC] <- vAA.
 *
 * for: aput, aput-boolean, aput-byte, aput-char, aput-short, aput-wide, aput-object
 *
 */
    FETCH_B w2, 1, 0                    // w2<- BB
    lsr     w9, wINST, #8               // w9<- AA
    FETCH_B w3, 1, 1                    // w3<- CC
    GET_VREG w0, w2                     // w0<- vBB (array object)
    GET_VREG w1, w3                     // w1<- vCC (requested index)
    cbz     w0, common_errNullObject    // bail if null
    ldr     w3, [x0, #MIRROR_ARRAY_LENGTH_OFFSET]     // w3<- arrayObj->length
    .if !0
    add     x0, x0, w1, uxtw #2    // w0<- arrayObj + index*width
    .endif
    cmp     w1, w3                      // compare unsigned index, length
    bcs     common_errArrayIndex        // index >= length, bail
    .if 0
    EXPORT_PC                           // Export PC before overwriting it.
    .endif
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
    .if 0
    GET_VREG_WIDE x2, w9                // x2<- vAA
    .else
    GET_VREG w2, w9                     // w2<- vAA
    .endif
    .if 0
    str  x2, [x0, #MIRROR_INT_ARRAY_DATA_OFFSET]     // vBB[vCC]<- x2
    .elseif 0
    bl art_quick_aput_obj
    .else
    str  w2, [x0, #MIRROR_INT_ARRAY_DATA_OFFSET]     // vBB[vCC]<- w2
    .endif
    GET_INST_OPCODE ip                  // extract opcode from rINST
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_aput

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aput_wide: /* 0x4c */
    NAME_START nterp_op_aput_wide

/*
 * Array put.  vBB[vCC] <- vAA.
 *
 * for: aput, aput-boolean, aput-byte, aput-char, aput-short, aput-wide, aput-object
 *
 */
    FETCH_B w2, 1, 0                    // w2<- BB
    lsr     w9, wINST, #8               // w9<- AA
    FETCH_B w3, 1, 1                    // w3<- CC
    GET_VREG w0, w2                     // w0<- vBB (array object)
    GET_VREG w1, w3                     // w1<- vCC (requested index)
    cbz     w0, common_errNullObject    // bail if null
    ldr     w3, [x0, #MIRROR_ARRAY_LENGTH_OFFSET]     // w3<- arrayObj->length
    .if !0
    add     x0, x0, w1, uxtw #3    // w0<- arrayObj + index*width
    .endif
    cmp     w1, w3                      // compare unsigned index, length
    bcs     common_errArrayIndex        // index >= length, bail
    .if 0
    EXPORT_PC                           // Export PC before overwriting it.
    .endif
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
    .if 1
    GET_VREG_WIDE x2, w9                // x2<- vAA
    .else
    GET_VREG w2, w9                     // w2<- vAA
    .endif
    .if 1
    str  x2, [x0, #MIRROR_WIDE_ARRAY_DATA_OFFSET]     // vBB[vCC]<- x2
    .elseif 0
    bl art_quick_aput_obj
    .else
    str  w2, [x0, #MIRROR_WIDE_ARRAY_DATA_OFFSET]     // vBB[vCC]<- w2
    .endif
    GET_INST_OPCODE ip                  // extract opcode from rINST
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_aput_wide

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aput_object: /* 0x4d */
    NAME_START nterp_op_aput_object

/*
 * Array put.  vBB[vCC] <- vAA.
 *
 * for: aput, aput-boolean, aput-byte, aput-char, aput-short, aput-wide, aput-object
 *
 */
    FETCH_B w2, 1, 0                    // w2<- BB
    lsr     w9, wINST, #8               // w9<- AA
    FETCH_B w3, 1, 1                    // w3<- CC
    GET_VREG w0, w2                     // w0<- vBB (array object)
    GET_VREG w1, w3                     // w1<- vCC (requested index)
    cbz     w0, common_errNullObject    // bail if null
    ldr     w3, [x0, #MIRROR_ARRAY_LENGTH_OFFSET]     // w3<- arrayObj->length
    .if !1
    add     x0, x0, w1, uxtw #2    // w0<- arrayObj + index*width
    .endif
    cmp     w1, w3                      // compare unsigned index, length
    bcs     common_errArrayIndex        // index >= length, bail
    .if 1
    EXPORT_PC                           // Export PC before overwriting it.
    .endif
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
    .if 0
    GET_VREG_WIDE x2, w9                // x2<- vAA
    .else
    GET_VREG w2, w9                     // w2<- vAA
    .endif
    .if 0
    str  x2, [x0, #MIRROR_INT_ARRAY_DATA_OFFSET]     // vBB[vCC]<- x2
    .elseif 1
    bl art_quick_aput_obj
    .else
    str  w2, [x0, #MIRROR_INT_ARRAY_DATA_OFFSET]     // vBB[vCC]<- w2
    .endif
    GET_INST_OPCODE ip                  // extract opcode from rINST
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_aput_object

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aput_boolean: /* 0x4e */
    NAME_START nterp_op_aput_boolean

/*
 * Array put.  vBB[vCC] <- vAA.
 *
 * for: aput, aput-boolean, aput-byte, aput-char, aput-short, aput-wide, aput-object
 *
 */
    FETCH_B w2, 1, 0                    // w2<- BB
    lsr     w9, wINST, #8               // w9<- AA
    FETCH_B w3, 1, 1                    // w3<- CC
    GET_VREG w0, w2                     // w0<- vBB (array object)
    GET_VREG w1, w3                     // w1<- vCC (requested index)
    cbz     w0, common_errNullObject    // bail if null
    ldr     w3, [x0, #MIRROR_ARRAY_LENGTH_OFFSET]     // w3<- arrayObj->length
    .if !0
    add     x0, x0, w1, uxtw #0    // w0<- arrayObj + index*width
    .endif
    cmp     w1, w3                      // compare unsigned index, length
    bcs     common_errArrayIndex        // index >= length, bail
    .if 0
    EXPORT_PC                           // Export PC before overwriting it.
    .endif
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
    .if 0
    GET_VREG_WIDE x2, w9                // x2<- vAA
    .else
    GET_VREG w2, w9                     // w2<- vAA
    .endif
    .if 0
    strb  x2, [x0, #MIRROR_BOOLEAN_ARRAY_DATA_OFFSET]     // vBB[vCC]<- x2
    .elseif 0
    bl art_quick_aput_obj
    .else
    strb  w2, [x0, #MIRROR_BOOLEAN_ARRAY_DATA_OFFSET]     // vBB[vCC]<- w2
    .endif
    GET_INST_OPCODE ip                  // extract opcode from rINST
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_aput_boolean

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aput_byte: /* 0x4f */
    NAME_START nterp_op_aput_byte

/*
 * Array put.  vBB[vCC] <- vAA.
 *
 * for: aput, aput-boolean, aput-byte, aput-char, aput-short, aput-wide, aput-object
 *
 */
    FETCH_B w2, 1, 0                    // w2<- BB
    lsr     w9, wINST, #8               // w9<- AA
    FETCH_B w3, 1, 1                    // w3<- CC
    GET_VREG w0, w2                     // w0<- vBB (array object)
    GET_VREG w1, w3                     // w1<- vCC (requested index)
    cbz     w0, common_errNullObject    // bail if null
    ldr     w3, [x0, #MIRROR_ARRAY_LENGTH_OFFSET]     // w3<- arrayObj->length
    .if !0
    add     x0, x0, w1, uxtw #0    // w0<- arrayObj + index*width
    .endif
    cmp     w1, w3                      // compare unsigned index, length
    bcs     common_errArrayIndex        // index >= length, bail
    .if 0
    EXPORT_PC                           // Export PC before overwriting it.
    .endif
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
    .if 0
    GET_VREG_WIDE x2, w9                // x2<- vAA
    .else
    GET_VREG w2, w9                     // w2<- vAA
    .endif
    .if 0
    strb  x2, [x0, #MIRROR_BYTE_ARRAY_DATA_OFFSET]     // vBB[vCC]<- x2
    .elseif 0
    bl art_quick_aput_obj
    .else
    strb  w2, [x0, #MIRROR_BYTE_ARRAY_DATA_OFFSET]     // vBB[vCC]<- w2
    .endif
    GET_INST_OPCODE ip                  // extract opcode from rINST
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_aput_byte

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aput_char: /* 0x50 */
    NAME_START nterp_op_aput_char

/*
 * Array put.  vBB[vCC] <- vAA.
 *
 * for: aput, aput-boolean, aput-byte, aput-char, aput-short, aput-wide, aput-object
 *
 */
    FETCH_B w2, 1, 0                    // w2<- BB
    lsr     w9, wINST, #8               // w9<- AA
    FETCH_B w3, 1, 1                    // w3<- CC
    GET_VREG w0, w2                     // w0<- vBB (array object)
    GET_VREG w1, w3                     // w1<- vCC (requested index)
    cbz     w0, common_errNullObject    // bail if null
    ldr     w3, [x0, #MIRROR_ARRAY_LENGTH_OFFSET]     // w3<- arrayObj->length
    .if !0
    add     x0, x0, w1, uxtw #1    // w0<- arrayObj + index*width
    .endif
    cmp     w1, w3                      // compare unsigned index, length
    bcs     common_errArrayIndex        // index >= length, bail
    .if 0
    EXPORT_PC                           // Export PC before overwriting it.
    .endif
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
    .if 0
    GET_VREG_WIDE x2, w9                // x2<- vAA
    .else
    GET_VREG w2, w9                     // w2<- vAA
    .endif
    .if 0
    strh  x2, [x0, #MIRROR_CHAR_ARRAY_DATA_OFFSET]     // vBB[vCC]<- x2
    .elseif 0
    bl art_quick_aput_obj
    .else
    strh  w2, [x0, #MIRROR_CHAR_ARRAY_DATA_OFFSET]     // vBB[vCC]<- w2
    .endif
    GET_INST_OPCODE ip                  // extract opcode from rINST
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_aput_char

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_aput_short: /* 0x51 */
    NAME_START nterp_op_aput_short

/*
 * Array put.  vBB[vCC] <- vAA.
 *
 * for: aput, aput-boolean, aput-byte, aput-char, aput-short, aput-wide, aput-object
 *
 */
    FETCH_B w2, 1, 0                    // w2<- BB
    lsr     w9, wINST, #8               // w9<- AA
    FETCH_B w3, 1, 1                    // w3<- CC
    GET_VREG w0, w2                     // w0<- vBB (array object)
    GET_VREG w1, w3                     // w1<- vCC (requested index)
    cbz     w0, common_errNullObject    // bail if null
    ldr     w3, [x0, #MIRROR_ARRAY_LENGTH_OFFSET]     // w3<- arrayObj->length
    .if !0
    add     x0, x0, w1, uxtw #1    // w0<- arrayObj + index*width
    .endif
    cmp     w1, w3                      // compare unsigned index, length
    bcs     common_errArrayIndex        // index >= length, bail
    .if 0
    EXPORT_PC                           // Export PC before overwriting it.
    .endif
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
    .if 0
    GET_VREG_WIDE x2, w9                // x2<- vAA
    .else
    GET_VREG w2, w9                     // w2<- vAA
    .endif
    .if 0
    strh  x2, [x0, #MIRROR_SHORT_ARRAY_DATA_OFFSET]     // vBB[vCC]<- x2
    .elseif 0
    bl art_quick_aput_obj
    .else
    strh  w2, [x0, #MIRROR_SHORT_ARRAY_DATA_OFFSET]     // vBB[vCC]<- w2
    .endif
    GET_INST_OPCODE ip                  // extract opcode from rINST
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_aput_short

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iget: /* 0x52 */
    NAME_START nterp_op_iget

   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE x0, nterp_op_iget_helper
.Lop_iget_resume:
   lsr     w2, wINST, #12              // w2<- B
   GET_VREG w3, w2                     // w3<- object we're operating on
   ubfx    w2, wINST, #8, #4           // w2<- A
   cbz     w3, common_errNullObject    // object was null
   .if 0
   ldr   x0, [x3, x0]
   SET_VREG_WIDE x0, w2                // fp[A] <- value
   .elseif 0
   ldr   w0, [x3, x0]
   cbnz    wMR, .Lop_iget_read_barrier
.Lop_iget_resume_after_read_barrier:
   SET_VREG_OBJECT w0, w2              // fp[A] <- value
   .else
   ldr   w0, [x3, x0]
   SET_VREG w0, w2                     // fp[A] <- value
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
   .if 0
.Lop_iget_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_iget_resume_after_read_barrier
   .endif

    NAME_END nterp_op_iget

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iget_wide: /* 0x53 */
    NAME_START nterp_op_iget_wide

   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE x0, nterp_op_iget_wide_helper
.Lop_iget_wide_resume:
   lsr     w2, wINST, #12              // w2<- B
   GET_VREG w3, w2                     // w3<- object we're operating on
   ubfx    w2, wINST, #8, #4           // w2<- A
   cbz     w3, common_errNullObject    // object was null
   .if 1
   ldr   x0, [x3, x0]
   SET_VREG_WIDE x0, w2                // fp[A] <- value
   .elseif 0
   ldr   w0, [x3, x0]
   cbnz    wMR, .Lop_iget_wide_read_barrier
.Lop_iget_wide_resume_after_read_barrier:
   SET_VREG_OBJECT w0, w2              // fp[A] <- value
   .else
   ldr   w0, [x3, x0]
   SET_VREG w0, w2                     // fp[A] <- value
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
   .if 0
.Lop_iget_wide_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_iget_wide_resume_after_read_barrier
   .endif

    NAME_END nterp_op_iget_wide

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iget_object: /* 0x54 */
    NAME_START nterp_op_iget_object

   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE x0, nterp_op_iget_object_helper
.Lop_iget_object_resume:
   lsr     w2, wINST, #12              // w2<- B
   GET_VREG w3, w2                     // w3<- object we're operating on
   ubfx    w2, wINST, #8, #4           // w2<- A
   cbz     w3, common_errNullObject    // object was null
   .if 0
   ldr   x0, [x3, x0]
   SET_VREG_WIDE x0, w2                // fp[A] <- value
   .elseif 1
   ldr   w0, [x3, x0]
   cbnz    wMR, .Lop_iget_object_read_barrier
.Lop_iget_object_resume_after_read_barrier:
   SET_VREG_OBJECT w0, w2              // fp[A] <- value
   .else
   ldr   w0, [x3, x0]
   SET_VREG w0, w2                     // fp[A] <- value
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
   .if 1
.Lop_iget_object_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_iget_object_resume_after_read_barrier
   .endif

    NAME_END nterp_op_iget_object

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iget_boolean: /* 0x55 */
    NAME_START nterp_op_iget_boolean

   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE x0, nterp_op_iget_boolean_helper
.Lop_iget_boolean_resume:
   lsr     w2, wINST, #12              // w2<- B
   GET_VREG w3, w2                     // w3<- object we're operating on
   ubfx    w2, wINST, #8, #4           // w2<- A
   cbz     w3, common_errNullObject    // object was null
   .if 0
   ldrb   x0, [x3, x0]
   SET_VREG_WIDE x0, w2                // fp[A] <- value
   .elseif 0
   ldrb   w0, [x3, x0]
   cbnz    wMR, .Lop_iget_boolean_read_barrier
.Lop_iget_boolean_resume_after_read_barrier:
   SET_VREG_OBJECT w0, w2              // fp[A] <- value
   .else
   ldrb   w0, [x3, x0]
   SET_VREG w0, w2                     // fp[A] <- value
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
   .if 0
.Lop_iget_boolean_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_iget_boolean_resume_after_read_barrier
   .endif

    NAME_END nterp_op_iget_boolean

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iget_byte: /* 0x56 */
    NAME_START nterp_op_iget_byte

   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE x0, nterp_op_iget_byte_helper
.Lop_iget_byte_resume:
   lsr     w2, wINST, #12              // w2<- B
   GET_VREG w3, w2                     // w3<- object we're operating on
   ubfx    w2, wINST, #8, #4           // w2<- A
   cbz     w3, common_errNullObject    // object was null
   .if 0
   ldrsb   x0, [x3, x0]
   SET_VREG_WIDE x0, w2                // fp[A] <- value
   .elseif 0
   ldrsb   w0, [x3, x0]
   cbnz    wMR, .Lop_iget_byte_read_barrier
.Lop_iget_byte_resume_after_read_barrier:
   SET_VREG_OBJECT w0, w2              // fp[A] <- value
   .else
   ldrsb   w0, [x3, x0]
   SET_VREG w0, w2                     // fp[A] <- value
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
   .if 0
.Lop_iget_byte_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_iget_byte_resume_after_read_barrier
   .endif

    NAME_END nterp_op_iget_byte

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iget_char: /* 0x57 */
    NAME_START nterp_op_iget_char

   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE x0, nterp_op_iget_char_helper
.Lop_iget_char_resume:
   lsr     w2, wINST, #12              // w2<- B
   GET_VREG w3, w2                     // w3<- object we're operating on
   ubfx    w2, wINST, #8, #4           // w2<- A
   cbz     w3, common_errNullObject    // object was null
   .if 0
   ldrh   x0, [x3, x0]
   SET_VREG_WIDE x0, w2                // fp[A] <- value
   .elseif 0
   ldrh   w0, [x3, x0]
   cbnz    wMR, .Lop_iget_char_read_barrier
.Lop_iget_char_resume_after_read_barrier:
   SET_VREG_OBJECT w0, w2              // fp[A] <- value
   .else
   ldrh   w0, [x3, x0]
   SET_VREG w0, w2                     // fp[A] <- value
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
   .if 0
.Lop_iget_char_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_iget_char_resume_after_read_barrier
   .endif

    NAME_END nterp_op_iget_char

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iget_short: /* 0x58 */
    NAME_START nterp_op_iget_short

   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE x0, nterp_op_iget_short_helper
.Lop_iget_short_resume:
   lsr     w2, wINST, #12              // w2<- B
   GET_VREG w3, w2                     // w3<- object we're operating on
   ubfx    w2, wINST, #8, #4           // w2<- A
   cbz     w3, common_errNullObject    // object was null
   .if 0
   ldrsh   x0, [x3, x0]
   SET_VREG_WIDE x0, w2                // fp[A] <- value
   .elseif 0
   ldrsh   w0, [x3, x0]
   cbnz    wMR, .Lop_iget_short_read_barrier
.Lop_iget_short_resume_after_read_barrier:
   SET_VREG_OBJECT w0, w2              // fp[A] <- value
   .else
   ldrsh   w0, [x3, x0]
   SET_VREG w0, w2                     // fp[A] <- value
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
   .if 0
.Lop_iget_short_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_iget_short_resume_after_read_barrier
   .endif

    NAME_END nterp_op_iget_short

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iput: /* 0x59 */
    NAME_START nterp_op_iput

   // Share slow paths for boolean and byte (strb) and slow paths for char and short (strh).
   // It does not matter to which `.Lop_iput_resume` the slow path returns.
   ubfx    w1, wINST, #8, #4           // w1<- A
   .if 0
   GET_VREG_WIDE x26, w1               // x26<- fp[A]/fp[A+1]
   .else
   GET_VREG w26, w1                    // w26 <- v[A]
   .endif
   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE x0, nterp_op_iput_helper_str00
.Lop_iput_resume:
   lsr     w2, wINST, #12              // w2<- B
   GET_VREG w2, w2                     // vB (object we're operating on)
   cbz w2, common_errNullObject
   .if 0
   str  x26, [x2, x0]
   .else
   str  w26, [x2, x0]
   WRITE_BARRIER_IF_OBJECT 0, w26, w2, .Lop_iput_skip_write_barrier
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip

    NAME_END nterp_op_iput

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iput_wide: /* 0x5a */
    NAME_START nterp_op_iput_wide

   // Share slow paths for boolean and byte (strb) and slow paths for char and short (strh).
   // It does not matter to which `.Lop_iput_wide_resume` the slow path returns.
   ubfx    w1, wINST, #8, #4           // w1<- A
   .if 1
   GET_VREG_WIDE x26, w1               // x26<- fp[A]/fp[A+1]
   .else
   GET_VREG w26, w1                    // w26 <- v[A]
   .endif
   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE x0, nterp_op_iput_helper_str10
.Lop_iput_wide_resume:
   lsr     w2, wINST, #12              // w2<- B
   GET_VREG w2, w2                     // vB (object we're operating on)
   cbz w2, common_errNullObject
   .if 1
   str  x26, [x2, x0]
   .else
   str  w26, [x2, x0]
   WRITE_BARRIER_IF_OBJECT 0, w26, w2, .Lop_iput_wide_skip_write_barrier
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip

    NAME_END nterp_op_iput_wide

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iput_object: /* 0x5b */
    NAME_START nterp_op_iput_object

   // Share slow paths for boolean and byte (strb) and slow paths for char and short (strh).
   // It does not matter to which `.Lop_iput_object_resume` the slow path returns.
   ubfx    w1, wINST, #8, #4           // w1<- A
   .if 0
   GET_VREG_WIDE x26, w1               // x26<- fp[A]/fp[A+1]
   .else
   GET_VREG w26, w1                    // w26 <- v[A]
   .endif
   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE x0, nterp_op_iput_helper_str01
.Lop_iput_object_resume:
   lsr     w2, wINST, #12              // w2<- B
   GET_VREG w2, w2                     // vB (object we're operating on)
   cbz w2, common_errNullObject
   .if 0
   str  x26, [x2, x0]
   .else
   str  w26, [x2, x0]
   WRITE_BARRIER_IF_OBJECT 1, w26, w2, .Lop_iput_object_skip_write_barrier
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip

    NAME_END nterp_op_iput_object

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iput_boolean: /* 0x5c */
    NAME_START nterp_op_iput_boolean

   // Share slow paths for boolean and byte (strb) and slow paths for char and short (strh).
   // It does not matter to which `.Lop_iput_boolean_resume` the slow path returns.
   ubfx    w1, wINST, #8, #4           // w1<- A
   .if 0
   GET_VREG_WIDE x26, w1               // x26<- fp[A]/fp[A+1]
   .else
   GET_VREG w26, w1                    // w26 <- v[A]
   .endif
   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE x0, nterp_op_iput_helper_strb00
.Lop_iput_boolean_resume:
   lsr     w2, wINST, #12              // w2<- B
   GET_VREG w2, w2                     // vB (object we're operating on)
   cbz w2, common_errNullObject
   .if 0
   strb  x26, [x2, x0]
   .else
   strb  w26, [x2, x0]
   WRITE_BARRIER_IF_OBJECT 0, w26, w2, .Lop_iput_boolean_skip_write_barrier
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip

    NAME_END nterp_op_iput_boolean

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iput_byte: /* 0x5d */
    NAME_START nterp_op_iput_byte

   // Share slow paths for boolean and byte (strb) and slow paths for char and short (strh).
   // It does not matter to which `.Lop_iput_byte_resume` the slow path returns.
   ubfx    w1, wINST, #8, #4           // w1<- A
   .if 0
   GET_VREG_WIDE x26, w1               // x26<- fp[A]/fp[A+1]
   .else
   GET_VREG w26, w1                    // w26 <- v[A]
   .endif
   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE x0, nterp_op_iput_helper_strb00
.Lop_iput_byte_resume:
   lsr     w2, wINST, #12              // w2<- B
   GET_VREG w2, w2                     // vB (object we're operating on)
   cbz w2, common_errNullObject
   .if 0
   strb  x26, [x2, x0]
   .else
   strb  w26, [x2, x0]
   WRITE_BARRIER_IF_OBJECT 0, w26, w2, .Lop_iput_byte_skip_write_barrier
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip

    NAME_END nterp_op_iput_byte

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iput_char: /* 0x5e */
    NAME_START nterp_op_iput_char

   // Share slow paths for boolean and byte (strb) and slow paths for char and short (strh).
   // It does not matter to which `.Lop_iput_char_resume` the slow path returns.
   ubfx    w1, wINST, #8, #4           // w1<- A
   .if 0
   GET_VREG_WIDE x26, w1               // x26<- fp[A]/fp[A+1]
   .else
   GET_VREG w26, w1                    // w26 <- v[A]
   .endif
   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE x0, nterp_op_iput_helper_strh00
.Lop_iput_char_resume:
   lsr     w2, wINST, #12              // w2<- B
   GET_VREG w2, w2                     // vB (object we're operating on)
   cbz w2, common_errNullObject
   .if 0
   strh  x26, [x2, x0]
   .else
   strh  w26, [x2, x0]
   WRITE_BARRIER_IF_OBJECT 0, w26, w2, .Lop_iput_char_skip_write_barrier
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip

    NAME_END nterp_op_iput_char

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_iput_short: /* 0x5f */
    NAME_START nterp_op_iput_short

   // Share slow paths for boolean and byte (strb) and slow paths for char and short (strh).
   // It does not matter to which `.Lop_iput_short_resume` the slow path returns.
   ubfx    w1, wINST, #8, #4           // w1<- A
   .if 0
   GET_VREG_WIDE x26, w1               // x26<- fp[A]/fp[A+1]
   .else
   GET_VREG w26, w1                    // w26 <- v[A]
   .endif
   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE x0, nterp_op_iput_helper_strh00
.Lop_iput_short_resume:
   lsr     w2, wINST, #12              // w2<- B
   GET_VREG w2, w2                     // vB (object we're operating on)
   cbz w2, common_errNullObject
   .if 0
   strh  x26, [x2, x0]
   .else
   strh  w26, [x2, x0]
   WRITE_BARRIER_IF_OBJECT 0, w26, w2, .Lop_iput_short_skip_write_barrier
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip

    NAME_END nterp_op_iput_short

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sget: /* 0x60 */
    NAME_START nterp_op_sget

   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE x0, nterp_op_sget_helper
.Lop_sget_resume:
   ldr     w1, [x0, #ART_FIELD_OFFSET_OFFSET]
   lsr     w2, wINST, #8               // w2 <- A
   ldr     w0, [x0, #ART_FIELD_DECLARING_CLASS_OFFSET]
   cbnz    wMR, .Lop_sget_read_barrier
.Lop_sget_resume_after_read_barrier:
   .if 0
   ldr     x0, [x0, x1]
   SET_VREG_WIDE x0, w2                // fp[A] <- value
   .elseif 0
   ldr   w0, [x0, x1]
   // No need to check the marking register, we know it's not set here.
.Lop_sget_after_reference_load:
   SET_VREG_OBJECT w0, w2              // fp[A] <- value
   .else
   ldr   w0, [x0, x1]
   SET_VREG w0, w2                     // fp[A] <- value
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.Lop_sget_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   .if 0
   ldr   w0, [x0, x1]
.Lop_sget_mark_after_load:
   // Here, we know the marking register is set.
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_sget_after_reference_load
   .else
   b       .Lop_sget_resume_after_read_barrier
   .endif

    NAME_END nterp_op_sget

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sget_wide: /* 0x61 */
    NAME_START nterp_op_sget_wide

   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE x0, nterp_op_sget_wide_helper
.Lop_sget_wide_resume:
   ldr     w1, [x0, #ART_FIELD_OFFSET_OFFSET]
   lsr     w2, wINST, #8               // w2 <- A
   ldr     w0, [x0, #ART_FIELD_DECLARING_CLASS_OFFSET]
   cbnz    wMR, .Lop_sget_wide_read_barrier
.Lop_sget_wide_resume_after_read_barrier:
   .if 1
   ldr     x0, [x0, x1]
   SET_VREG_WIDE x0, w2                // fp[A] <- value
   .elseif 0
   ldr   w0, [x0, x1]
   // No need to check the marking register, we know it's not set here.
.Lop_sget_wide_after_reference_load:
   SET_VREG_OBJECT w0, w2              // fp[A] <- value
   .else
   ldr   w0, [x0, x1]
   SET_VREG w0, w2                     // fp[A] <- value
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.Lop_sget_wide_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   .if 0
   ldr   w0, [x0, x1]
.Lop_sget_wide_mark_after_load:
   // Here, we know the marking register is set.
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_sget_wide_after_reference_load
   .else
   b       .Lop_sget_wide_resume_after_read_barrier
   .endif

    NAME_END nterp_op_sget_wide

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sget_object: /* 0x62 */
    NAME_START nterp_op_sget_object

   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE x0, nterp_op_sget_object_helper
.Lop_sget_object_resume:
   ldr     w1, [x0, #ART_FIELD_OFFSET_OFFSET]
   lsr     w2, wINST, #8               // w2 <- A
   ldr     w0, [x0, #ART_FIELD_DECLARING_CLASS_OFFSET]
   cbnz    wMR, .Lop_sget_object_read_barrier
.Lop_sget_object_resume_after_read_barrier:
   .if 0
   ldr     x0, [x0, x1]
   SET_VREG_WIDE x0, w2                // fp[A] <- value
   .elseif 1
   ldr   w0, [x0, x1]
   // No need to check the marking register, we know it's not set here.
.Lop_sget_object_after_reference_load:
   SET_VREG_OBJECT w0, w2              // fp[A] <- value
   .else
   ldr   w0, [x0, x1]
   SET_VREG w0, w2                     // fp[A] <- value
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.Lop_sget_object_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   .if 1
   ldr   w0, [x0, x1]
.Lop_sget_object_mark_after_load:
   // Here, we know the marking register is set.
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_sget_object_after_reference_load
   .else
   b       .Lop_sget_object_resume_after_read_barrier
   .endif

    NAME_END nterp_op_sget_object

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sget_boolean: /* 0x63 */
    NAME_START nterp_op_sget_boolean

   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE x0, nterp_op_sget_boolean_helper
.Lop_sget_boolean_resume:
   ldr     w1, [x0, #ART_FIELD_OFFSET_OFFSET]
   lsr     w2, wINST, #8               // w2 <- A
   ldr     w0, [x0, #ART_FIELD_DECLARING_CLASS_OFFSET]
   cbnz    wMR, .Lop_sget_boolean_read_barrier
.Lop_sget_boolean_resume_after_read_barrier:
   .if 0
   ldr     x0, [x0, x1]
   SET_VREG_WIDE x0, w2                // fp[A] <- value
   .elseif 0
   ldrb   w0, [x0, x1]
   // No need to check the marking register, we know it's not set here.
.Lop_sget_boolean_after_reference_load:
   SET_VREG_OBJECT w0, w2              // fp[A] <- value
   .else
   ldrb   w0, [x0, x1]
   SET_VREG w0, w2                     // fp[A] <- value
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.Lop_sget_boolean_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   .if 0
   ldrb   w0, [x0, x1]
.Lop_sget_boolean_mark_after_load:
   // Here, we know the marking register is set.
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_sget_boolean_after_reference_load
   .else
   b       .Lop_sget_boolean_resume_after_read_barrier
   .endif

    NAME_END nterp_op_sget_boolean

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sget_byte: /* 0x64 */
    NAME_START nterp_op_sget_byte

   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE x0, nterp_op_sget_byte_helper
.Lop_sget_byte_resume:
   ldr     w1, [x0, #ART_FIELD_OFFSET_OFFSET]
   lsr     w2, wINST, #8               // w2 <- A
   ldr     w0, [x0, #ART_FIELD_DECLARING_CLASS_OFFSET]
   cbnz    wMR, .Lop_sget_byte_read_barrier
.Lop_sget_byte_resume_after_read_barrier:
   .if 0
   ldr     x0, [x0, x1]
   SET_VREG_WIDE x0, w2                // fp[A] <- value
   .elseif 0
   ldrsb   w0, [x0, x1]
   // No need to check the marking register, we know it's not set here.
.Lop_sget_byte_after_reference_load:
   SET_VREG_OBJECT w0, w2              // fp[A] <- value
   .else
   ldrsb   w0, [x0, x1]
   SET_VREG w0, w2                     // fp[A] <- value
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.Lop_sget_byte_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   .if 0
   ldrsb   w0, [x0, x1]
.Lop_sget_byte_mark_after_load:
   // Here, we know the marking register is set.
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_sget_byte_after_reference_load
   .else
   b       .Lop_sget_byte_resume_after_read_barrier
   .endif

    NAME_END nterp_op_sget_byte

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sget_char: /* 0x65 */
    NAME_START nterp_op_sget_char

   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE x0, nterp_op_sget_char_helper
.Lop_sget_char_resume:
   ldr     w1, [x0, #ART_FIELD_OFFSET_OFFSET]
   lsr     w2, wINST, #8               // w2 <- A
   ldr     w0, [x0, #ART_FIELD_DECLARING_CLASS_OFFSET]
   cbnz    wMR, .Lop_sget_char_read_barrier
.Lop_sget_char_resume_after_read_barrier:
   .if 0
   ldr     x0, [x0, x1]
   SET_VREG_WIDE x0, w2                // fp[A] <- value
   .elseif 0
   ldrh   w0, [x0, x1]
   // No need to check the marking register, we know it's not set here.
.Lop_sget_char_after_reference_load:
   SET_VREG_OBJECT w0, w2              // fp[A] <- value
   .else
   ldrh   w0, [x0, x1]
   SET_VREG w0, w2                     // fp[A] <- value
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.Lop_sget_char_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   .if 0
   ldrh   w0, [x0, x1]
.Lop_sget_char_mark_after_load:
   // Here, we know the marking register is set.
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_sget_char_after_reference_load
   .else
   b       .Lop_sget_char_resume_after_read_barrier
   .endif

    NAME_END nterp_op_sget_char

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sget_short: /* 0x66 */
    NAME_START nterp_op_sget_short

   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE x0, nterp_op_sget_short_helper
.Lop_sget_short_resume:
   ldr     w1, [x0, #ART_FIELD_OFFSET_OFFSET]
   lsr     w2, wINST, #8               // w2 <- A
   ldr     w0, [x0, #ART_FIELD_DECLARING_CLASS_OFFSET]
   cbnz    wMR, .Lop_sget_short_read_barrier
.Lop_sget_short_resume_after_read_barrier:
   .if 0
   ldr     x0, [x0, x1]
   SET_VREG_WIDE x0, w2                // fp[A] <- value
   .elseif 0
   ldrsh   w0, [x0, x1]
   // No need to check the marking register, we know it's not set here.
.Lop_sget_short_after_reference_load:
   SET_VREG_OBJECT w0, w2              // fp[A] <- value
   .else
   ldrsh   w0, [x0, x1]
   SET_VREG w0, w2                     // fp[A] <- value
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.Lop_sget_short_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   .if 0
   ldrsh   w0, [x0, x1]
.Lop_sget_short_mark_after_load:
   // Here, we know the marking register is set.
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_sget_short_after_reference_load
   .else
   b       .Lop_sget_short_resume_after_read_barrier
   .endif

    NAME_END nterp_op_sget_short

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sput: /* 0x67 */
    NAME_START nterp_op_sput

   // Share slow paths for boolean and byte (strb) and slow paths for char and short (strh).
   // It does not matter to which `.Lop_sput_resume` the slow path returns.
   lsr     w2, wINST, #8               // w2 <- A
   .if 0
   GET_VREG_WIDE x26, w2               // x26 <- v[A]
   .else
   GET_VREG w26, w2                    // w26 <- v[A]
   .endif
   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE x0, nterp_op_sput_helper_str00
.Lop_sput_resume:
   ldr     w1, [x0, #ART_FIELD_OFFSET_OFFSET]
   ldr     w0, [x0, #ART_FIELD_DECLARING_CLASS_OFFSET]
   cbnz    wMR, .Lop_sput_read_barrier
.Lop_sput_resume_after_read_barrier:
   .if 0
   str  x26, [x0, x1]
   .else
   str  w26, [x0, x1]
   WRITE_BARRIER_IF_OBJECT 0, w26, w0, .Lop_sput_skip_write_barrier
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.Lop_sput_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_sput_resume_after_read_barrier

    NAME_END nterp_op_sput

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sput_wide: /* 0x68 */
    NAME_START nterp_op_sput_wide

   // Share slow paths for boolean and byte (strb) and slow paths for char and short (strh).
   // It does not matter to which `.Lop_sput_wide_resume` the slow path returns.
   lsr     w2, wINST, #8               // w2 <- A
   .if 1
   GET_VREG_WIDE x26, w2               // x26 <- v[A]
   .else
   GET_VREG w26, w2                    // w26 <- v[A]
   .endif
   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE x0, nterp_op_sput_helper_str10
.Lop_sput_wide_resume:
   ldr     w1, [x0, #ART_FIELD_OFFSET_OFFSET]
   ldr     w0, [x0, #ART_FIELD_DECLARING_CLASS_OFFSET]
   cbnz    wMR, .Lop_sput_wide_read_barrier
.Lop_sput_wide_resume_after_read_barrier:
   .if 1
   str  x26, [x0, x1]
   .else
   str  w26, [x0, x1]
   WRITE_BARRIER_IF_OBJECT 0, w26, w0, .Lop_sput_wide_skip_write_barrier
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.Lop_sput_wide_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_sput_wide_resume_after_read_barrier

    NAME_END nterp_op_sput_wide

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sput_object: /* 0x69 */
    NAME_START nterp_op_sput_object

   // Share slow paths for boolean and byte (strb) and slow paths for char and short (strh).
   // It does not matter to which `.Lop_sput_object_resume` the slow path returns.
   lsr     w2, wINST, #8               // w2 <- A
   .if 0
   GET_VREG_WIDE x26, w2               // x26 <- v[A]
   .else
   GET_VREG w26, w2                    // w26 <- v[A]
   .endif
   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE x0, nterp_op_sput_helper_str01
.Lop_sput_object_resume:
   ldr     w1, [x0, #ART_FIELD_OFFSET_OFFSET]
   ldr     w0, [x0, #ART_FIELD_DECLARING_CLASS_OFFSET]
   cbnz    wMR, .Lop_sput_object_read_barrier
.Lop_sput_object_resume_after_read_barrier:
   .if 0
   str  x26, [x0, x1]
   .else
   str  w26, [x0, x1]
   WRITE_BARRIER_IF_OBJECT 1, w26, w0, .Lop_sput_object_skip_write_barrier
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.Lop_sput_object_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_sput_object_resume_after_read_barrier

    NAME_END nterp_op_sput_object

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sput_boolean: /* 0x6a */
    NAME_START nterp_op_sput_boolean

   // Share slow paths for boolean and byte (strb) and slow paths for char and short (strh).
   // It does not matter to which `.Lop_sput_boolean_resume` the slow path returns.
   lsr     w2, wINST, #8               // w2 <- A
   .if 0
   GET_VREG_WIDE x26, w2               // x26 <- v[A]
   .else
   GET_VREG w26, w2                    // w26 <- v[A]
   .endif
   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE x0, nterp_op_sput_helper_strb00
.Lop_sput_boolean_resume:
   ldr     w1, [x0, #ART_FIELD_OFFSET_OFFSET]
   ldr     w0, [x0, #ART_FIELD_DECLARING_CLASS_OFFSET]
   cbnz    wMR, .Lop_sput_boolean_read_barrier
.Lop_sput_boolean_resume_after_read_barrier:
   .if 0
   strb  x26, [x0, x1]
   .else
   strb  w26, [x0, x1]
   WRITE_BARRIER_IF_OBJECT 0, w26, w0, .Lop_sput_boolean_skip_write_barrier
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.Lop_sput_boolean_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_sput_boolean_resume_after_read_barrier

    NAME_END nterp_op_sput_boolean

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sput_byte: /* 0x6b */
    NAME_START nterp_op_sput_byte

   // Share slow paths for boolean and byte (strb) and slow paths for char and short (strh).
   // It does not matter to which `.Lop_sput_byte_resume` the slow path returns.
   lsr     w2, wINST, #8               // w2 <- A
   .if 0
   GET_VREG_WIDE x26, w2               // x26 <- v[A]
   .else
   GET_VREG w26, w2                    // w26 <- v[A]
   .endif
   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE x0, nterp_op_sput_helper_strb00
.Lop_sput_byte_resume:
   ldr     w1, [x0, #ART_FIELD_OFFSET_OFFSET]
   ldr     w0, [x0, #ART_FIELD_DECLARING_CLASS_OFFSET]
   cbnz    wMR, .Lop_sput_byte_read_barrier
.Lop_sput_byte_resume_after_read_barrier:
   .if 0
   strb  x26, [x0, x1]
   .else
   strb  w26, [x0, x1]
   WRITE_BARRIER_IF_OBJECT 0, w26, w0, .Lop_sput_byte_skip_write_barrier
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.Lop_sput_byte_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_sput_byte_resume_after_read_barrier

    NAME_END nterp_op_sput_byte

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sput_char: /* 0x6c */
    NAME_START nterp_op_sput_char

   // Share slow paths for boolean and byte (strb) and slow paths for char and short (strh).
   // It does not matter to which `.Lop_sput_char_resume` the slow path returns.
   lsr     w2, wINST, #8               // w2 <- A
   .if 0
   GET_VREG_WIDE x26, w2               // x26 <- v[A]
   .else
   GET_VREG w26, w2                    // w26 <- v[A]
   .endif
   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE x0, nterp_op_sput_helper_strh00
.Lop_sput_char_resume:
   ldr     w1, [x0, #ART_FIELD_OFFSET_OFFSET]
   ldr     w0, [x0, #ART_FIELD_DECLARING_CLASS_OFFSET]
   cbnz    wMR, .Lop_sput_char_read_barrier
.Lop_sput_char_resume_after_read_barrier:
   .if 0
   strh  x26, [x0, x1]
   .else
   strh  w26, [x0, x1]
   WRITE_BARRIER_IF_OBJECT 0, w26, w0, .Lop_sput_char_skip_write_barrier
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.Lop_sput_char_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_sput_char_resume_after_read_barrier

    NAME_END nterp_op_sput_char

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sput_short: /* 0x6d */
    NAME_START nterp_op_sput_short

   // Share slow paths for boolean and byte (strb) and slow paths for char and short (strh).
   // It does not matter to which `.Lop_sput_short_resume` the slow path returns.
   lsr     w2, wINST, #8               // w2 <- A
   .if 0
   GET_VREG_WIDE x26, w2               // x26 <- v[A]
   .else
   GET_VREG w26, w2                    // w26 <- v[A]
   .endif
   // Fast-path which gets the field from thread-local cache.
   FETCH_FROM_THREAD_CACHE x0, nterp_op_sput_helper_strh00
.Lop_sput_short_resume:
   ldr     w1, [x0, #ART_FIELD_OFFSET_OFFSET]
   ldr     w0, [x0, #ART_FIELD_DECLARING_CLASS_OFFSET]
   cbnz    wMR, .Lop_sput_short_read_barrier
.Lop_sput_short_resume_after_read_barrier:
   .if 0
   strh  x26, [x0, x1]
   .else
   strh  w26, [x0, x1]
   WRITE_BARRIER_IF_OBJECT 0, w26, w0, .Lop_sput_short_skip_write_barrier
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.Lop_sput_short_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_sput_short_resume_after_read_barrier

    NAME_END nterp_op_sput_short

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_virtual: /* 0x6e */
    NAME_START nterp_op_invoke_virtual

   EXPORT_PC
   // Fast-path which gets the method from thread-local cache.
   FETCH_FROM_THREAD_CACHE x2, 2f
1:
   FETCH w1, 2
   .if !0
   and w1, w1, #0xf
   .endif
   GET_VREG w1, w1
   // Note: if w1 is null, this will be handled by our SIGSEGV handler.
   ldr w0, [x1, #MIRROR_OBJECT_CLASS_OFFSET]
   add w0, w0, #MIRROR_CLASS_VTABLE_OFFSET_64
   ldr x0, [x0, w2, uxtw #3]
   b NterpCommonInvokeInstance
2:
   mov x0, xSELF
   ldr x1, [sp]
   mov x2, xPC
   bl nterp_get_method
   mov x2, x0
   b 1b

    NAME_END nterp_op_invoke_virtual

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_super: /* 0x6f */
    NAME_START nterp_op_invoke_super

   EXPORT_PC
   // Fast-path which gets the method from thread-local cache.
   FETCH_FROM_THREAD_CACHE x0, 2f
1:
   // Load the first argument (the 'this' pointer).
   FETCH w1, 2
   .if !0
   and w1, w1, #0xf
   .endif
   GET_VREG w1, w1
   cbz w1, common_errNullObject    // bail if null
   b NterpCommonInvokeInstance
2:
   mov x0, xSELF
   ldr x1, [sp]
   mov x2, xPC
   bl nterp_get_method
   .if 1
   b 1b
   .else
   tbz x0, #0, 1b
   and x0, x0, #-2 // Remove the extra bit that marks it's a String.<init> method.
   .if 0
   b NterpHandleStringInitRange
   .else
   b NterpHandleStringInit
   .endif
   .endif

    NAME_END nterp_op_invoke_super

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_direct: /* 0x70 */
    NAME_START nterp_op_invoke_direct

   EXPORT_PC
   // Fast-path which gets the method from thread-local cache.
   FETCH_FROM_THREAD_CACHE x0, 2f
1:
   // Load the first argument (the 'this' pointer).
   FETCH w1, 2
   .if !0
   and w1, w1, #0xf
   .endif
   GET_VREG w1, w1
   cbz w1, common_errNullObject    // bail if null
   b NterpCommonInvokeInstance
2:
   mov x0, xSELF
   ldr x1, [sp]
   mov x2, xPC
   bl nterp_get_method
   .if 0
   b 1b
   .else
   tbz x0, #0, 1b
   and x0, x0, #-2 // Remove the extra bit that marks it's a String.<init> method.
   .if 0
   b NterpHandleStringInitRange
   .else
   b NterpHandleStringInit
   .endif
   .endif

    NAME_END nterp_op_invoke_direct

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_static: /* 0x71 */
    NAME_START nterp_op_invoke_static

   EXPORT_PC
   // Fast-path which gets the method from thread-local cache.
   FETCH_FROM_THREAD_CACHE x0, 1f
   b NterpCommonInvokeStatic
1:
   mov x0, xSELF
   ldr x1, [sp]
   mov x2, xPC
   bl nterp_get_method
   b NterpCommonInvokeStatic

    NAME_END nterp_op_invoke_static

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_interface: /* 0x72 */
    NAME_START nterp_op_invoke_interface

   EXPORT_PC
   // Fast-path which gets the method from thread-local cache.
   FETCH_FROM_THREAD_CACHE x26, 5f
1:
   // First argument is the 'this' pointer.
   FETCH w1, 2
   .if !0
   and w1, w1, #0xf
   .endif
   GET_VREG w1, w1
   // Note: if w1 is null, this will be handled by our SIGSEGV handler.
   ldr w2, [x1, #MIRROR_OBJECT_CLASS_OFFSET]
   // Test the first two bits of the fetched ArtMethod:
   // - If the first bit is set, this is a method on j.l.Object
   // - If the second bit is set, this is a default method.
   tst w26, #0x3
   b.ne 3f
   ldrh w3, [x26, #ART_METHOD_IMT_INDEX_OFFSET]
2:
   ldr x2, [x2, #MIRROR_CLASS_IMT_PTR_OFFSET_64]
   ldr x0, [x2, w3, uxtw #3]
   .if 0
   b NterpCommonInvokeInterfaceRange
   .else
   b NterpCommonInvokeInterface
   .endif
3:
   tbnz w26, #0, 4f
   and x26, x26, #-4
   ldrh w3, [x26, #ART_METHOD_METHOD_INDEX_OFFSET]
   and w3, w3, #ART_METHOD_IMT_MASK
   b 2b
4:
   lsr w26, w26, #16
   add w2, w2, #MIRROR_CLASS_VTABLE_OFFSET_64
   ldr x0, [x2, w26, uxtw #3]
   .if 0
   b NterpCommonInvokeInstanceRange
   .else
   b NterpCommonInvokeInstance
   .endif
5:
   mov x0, xSELF
   ldr x1, [sp]
   mov x2, xPC
   bl nterp_get_method
   mov x26, x0
   b 1b

    NAME_END nterp_op_invoke_interface

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_73: /* 0x73 */
    NAME_START nterp_op_unused_73

    brk 42

    NAME_END nterp_op_unused_73

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_virtual_range: /* 0x74 */
    NAME_START nterp_op_invoke_virtual_range

   EXPORT_PC
   // Fast-path which gets the method from thread-local cache.
   FETCH_FROM_THREAD_CACHE x2, 2f
1:
   FETCH w1, 2
   .if !1
   and w1, w1, #0xf
   .endif
   GET_VREG w1, w1
   // Note: if w1 is null, this will be handled by our SIGSEGV handler.
   ldr w0, [x1, #MIRROR_OBJECT_CLASS_OFFSET]
   add w0, w0, #MIRROR_CLASS_VTABLE_OFFSET_64
   ldr x0, [x0, w2, uxtw #3]
   b NterpCommonInvokeInstanceRange
2:
   mov x0, xSELF
   ldr x1, [sp]
   mov x2, xPC
   bl nterp_get_method
   mov x2, x0
   b 1b

    NAME_END nterp_op_invoke_virtual_range

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_super_range: /* 0x75 */
    NAME_START nterp_op_invoke_super_range

   EXPORT_PC
   // Fast-path which gets the method from thread-local cache.
   FETCH_FROM_THREAD_CACHE x0, 2f
1:
   // Load the first argument (the 'this' pointer).
   FETCH w1, 2
   .if !1
   and w1, w1, #0xf
   .endif
   GET_VREG w1, w1
   cbz w1, common_errNullObject    // bail if null
   b NterpCommonInvokeInstanceRange
2:
   mov x0, xSELF
   ldr x1, [sp]
   mov x2, xPC
   bl nterp_get_method
   .if 1
   b 1b
   .else
   tbz x0, #0, 1b
   and x0, x0, #-2 // Remove the extra bit that marks it's a String.<init> method.
   .if 1
   b NterpHandleStringInitRange
   .else
   b NterpHandleStringInit
   .endif
   .endif

    NAME_END nterp_op_invoke_super_range

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_direct_range: /* 0x76 */
    NAME_START nterp_op_invoke_direct_range

   EXPORT_PC
   // Fast-path which gets the method from thread-local cache.
   FETCH_FROM_THREAD_CACHE x0, 2f
1:
   // Load the first argument (the 'this' pointer).
   FETCH w1, 2
   .if !1
   and w1, w1, #0xf
   .endif
   GET_VREG w1, w1
   cbz w1, common_errNullObject    // bail if null
   b NterpCommonInvokeInstanceRange
2:
   mov x0, xSELF
   ldr x1, [sp]
   mov x2, xPC
   bl nterp_get_method
   .if 0
   b 1b
   .else
   tbz x0, #0, 1b
   and x0, x0, #-2 // Remove the extra bit that marks it's a String.<init> method.
   .if 1
   b NterpHandleStringInitRange
   .else
   b NterpHandleStringInit
   .endif
   .endif

    NAME_END nterp_op_invoke_direct_range

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_static_range: /* 0x77 */
    NAME_START nterp_op_invoke_static_range

   EXPORT_PC
   // Fast-path which gets the method from thread-local cache.
   FETCH_FROM_THREAD_CACHE x0, 1f
   b NterpCommonInvokeStaticRange
1:
   mov x0, xSELF
   ldr x1, [sp]
   mov x2, xPC
   bl nterp_get_method
   b NterpCommonInvokeStaticRange

    NAME_END nterp_op_invoke_static_range

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_interface_range: /* 0x78 */
    NAME_START nterp_op_invoke_interface_range

   EXPORT_PC
   // Fast-path which gets the method from thread-local cache.
   FETCH_FROM_THREAD_CACHE x26, 5f
1:
   // First argument is the 'this' pointer.
   FETCH w1, 2
   .if !1
   and w1, w1, #0xf
   .endif
   GET_VREG w1, w1
   // Note: if w1 is null, this will be handled by our SIGSEGV handler.
   ldr w2, [x1, #MIRROR_OBJECT_CLASS_OFFSET]
   // Test the first two bits of the fetched ArtMethod:
   // - If the first bit is set, this is a method on j.l.Object
   // - If the second bit is set, this is a default method.
   tst w26, #0x3
   b.ne 3f
   ldrh w3, [x26, #ART_METHOD_IMT_INDEX_OFFSET]
2:
   ldr x2, [x2, #MIRROR_CLASS_IMT_PTR_OFFSET_64]
   ldr x0, [x2, w3, uxtw #3]
   .if 1
   b NterpCommonInvokeInterfaceRange
   .else
   b NterpCommonInvokeInterface
   .endif
3:
   tbnz w26, #0, 4f
   and x26, x26, #-4
   ldrh w3, [x26, #ART_METHOD_METHOD_INDEX_OFFSET]
   and w3, w3, #ART_METHOD_IMT_MASK
   b 2b
4:
   lsr w26, w26, #16
   add w2, w2, #MIRROR_CLASS_VTABLE_OFFSET_64
   ldr x0, [x2, w26, uxtw #3]
   .if 1
   b NterpCommonInvokeInstanceRange
   .else
   b NterpCommonInvokeInstance
   .endif
5:
   mov x0, xSELF
   ldr x1, [sp]
   mov x2, xPC
   bl nterp_get_method
   mov x26, x0
   b 1b

    NAME_END nterp_op_invoke_interface_range

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_79: /* 0x79 */
    NAME_START nterp_op_unused_79

    brk 42

    NAME_END nterp_op_unused_79

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_7a: /* 0x7a */
    NAME_START nterp_op_unused_7a

    brk 42

    NAME_END nterp_op_unused_7a

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_neg_int: /* 0x7b */
    NAME_START nterp_op_neg_int

    /*
     * Generic 32-bit unary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = op w0".
     * This could be an ARM instruction or a function call.
     *
     * for: neg-int, not-int, neg-float, int-to-float, float-to-int,
     *      int-to-byte, int-to-char, int-to-short
     */
    /* unop vA, vB */
    lsr     w3, wINST, #12              // w3<- B
    GET_VREG w0, w3                     // w0<- vB
    ubfx    w9, wINST, #8, #4           // w9<- A
    FETCH_ADVANCE_INST 1                // advance rPC, load rINST
    sub     w0, wzr, w0                              // w0<- op, w0-w3 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG w0, w9                     // vAA<- w0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 8-9 instructions */

    NAME_END nterp_op_neg_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_not_int: /* 0x7c */
    NAME_START nterp_op_not_int

    /*
     * Generic 32-bit unary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = op w0".
     * This could be an ARM instruction or a function call.
     *
     * for: neg-int, not-int, neg-float, int-to-float, float-to-int,
     *      int-to-byte, int-to-char, int-to-short
     */
    /* unop vA, vB */
    lsr     w3, wINST, #12              // w3<- B
    GET_VREG w0, w3                     // w0<- vB
    ubfx    w9, wINST, #8, #4           // w9<- A
    FETCH_ADVANCE_INST 1                // advance rPC, load rINST
    mvn     w0, w0                              // w0<- op, w0-w3 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG w0, w9                     // vAA<- w0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 8-9 instructions */

    NAME_END nterp_op_not_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_neg_long: /* 0x7d */
    NAME_START nterp_op_neg_long

    /*
     * Generic 64-bit unary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = op x0".
     *
     * For: neg-long, not-long
     */
    /* unop vA, vB */
    lsr     w3, wINST, #12              // w3<- B
    ubfx    w4, wINST, #8, #4           // w4<- A
    GET_VREG_WIDE x0, w3
    FETCH_ADVANCE_INST 1                // advance rPC, load wINST
    sub x0, xzr, x0
    GET_INST_OPCODE ip                  // extract opcode from wINST
    SET_VREG_WIDE x0, w4
    GOTO_OPCODE ip                      // jump to next instruction
    /* 10-11 instructions */

    NAME_END nterp_op_neg_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_not_long: /* 0x7e */
    NAME_START nterp_op_not_long

    /*
     * Generic 64-bit unary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = op x0".
     *
     * For: neg-long, not-long
     */
    /* unop vA, vB */
    lsr     w3, wINST, #12              // w3<- B
    ubfx    w4, wINST, #8, #4           // w4<- A
    GET_VREG_WIDE x0, w3
    FETCH_ADVANCE_INST 1                // advance rPC, load wINST
    mvn     x0, x0
    GET_INST_OPCODE ip                  // extract opcode from wINST
    SET_VREG_WIDE x0, w4
    GOTO_OPCODE ip                      // jump to next instruction
    /* 10-11 instructions */

    NAME_END nterp_op_not_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_neg_float: /* 0x7f */
    NAME_START nterp_op_neg_float

    /*
     * Generic 32-bit unary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = op w0".
     * This could be an ARM instruction or a function call.
     *
     * for: neg-int, not-int, neg-float, int-to-float, float-to-int,
     *      int-to-byte, int-to-char, int-to-short
     */
    /* unop vA, vB */
    lsr     w3, wINST, #12              // w3<- B
    GET_VREG w0, w3                     // w0<- vB
    ubfx    w9, wINST, #8, #4           // w9<- A
    FETCH_ADVANCE_INST 1                // advance rPC, load rINST
    eor     w0, w0, #0x80000000                              // w0<- op, w0-w3 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG w0, w9                     // vAA<- w0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 8-9 instructions */

    NAME_END nterp_op_neg_float

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_neg_double: /* 0x80 */
    NAME_START nterp_op_neg_double

    /*
     * Generic 64-bit unary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = op x0".
     *
     * For: neg-long, not-long
     */
    /* unop vA, vB */
    lsr     w3, wINST, #12              // w3<- B
    ubfx    w4, wINST, #8, #4           // w4<- A
    GET_VREG_WIDE x0, w3
    FETCH_ADVANCE_INST 1                // advance rPC, load wINST
    eor     x0, x0, #0x8000000000000000
    GET_INST_OPCODE ip                  // extract opcode from wINST
    SET_VREG_WIDE x0, w4
    GOTO_OPCODE ip                      // jump to next instruction
    /* 10-11 instructions */

    NAME_END nterp_op_neg_double

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_int_to_long: /* 0x81 */
    NAME_START nterp_op_int_to_long

    /* int-to-long vA, vB */
    lsr     w3, wINST, #12              // w3<- B
    ubfx    w4, wINST, #8, #4           // w4<- A
    GET_VREG_S x0, w3                   // x0<- sign_extend(fp[B])
    FETCH_ADVANCE_INST 1                // advance rPC, load wINST
    GET_INST_OPCODE ip                  // extract opcode from wINST
    SET_VREG_WIDE x0, w4                // fp[A]<- x0
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_int_to_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_int_to_float: /* 0x82 */
    NAME_START nterp_op_int_to_float

    /*
     * Generic 32bit-to-32bit floating point unary operation.  Provide an
     * "instr" line that specifies an instruction that performs "s0 = op w0".
     *
     * For: int-to-float, float-to-int
     * TODO: refactor all of the conversions - parameterize width and use same template.
     */
    /* unop vA, vB */
    lsr     w3, wINST, #12              // w3<- B
    ubfx    w4, wINST, #8, #4           // w4<- A
    GET_VREG w0, w3
    FETCH_ADVANCE_INST 1                // advance rPC, load wINST
    scvtf s0, w0                              // d0<- op
    GET_INST_OPCODE ip                  // extract opcode from wINST
    SET_VREG_FLOAT s0, w4          // vA<- d0
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_int_to_float

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_int_to_double: /* 0x83 */
    NAME_START nterp_op_int_to_double

    /*
     * Generic 32bit-to-64bit floating point unary operation.  Provide an
     * "instr" line that specifies an instruction that performs "d0 = op w0".
     *
     * For: int-to-double, float-to-double, float-to-long
     */
    /* unop vA, vB */
    lsr     w3, wINST, #12              // w3<- B
    ubfx    w4, wINST, #8, #4           // w4<- A
    GET_VREG w0, w3
    FETCH_ADVANCE_INST 1                // advance rPC, load wINST
    scvtf d0, w0                              // d0<- op
    GET_INST_OPCODE ip                  // extract opcode from wINST
    SET_VREG_WIDE d0, w4           // vA<- d0
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_int_to_double

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_long_to_int: /* 0x84 */
    NAME_START nterp_op_long_to_int

/* we ignore the high word, making this equivalent to a 32-bit reg move */
    /* for move, move-object, long-to-int */
    /* op vA, vB */
    lsr     w1, wINST, #12              // x1<- B from 15:12
    ubfx    w0, wINST, #8, #4           // x0<- A from 11:8
    FETCH_ADVANCE_INST 1                // advance rPC, load wINST
    GET_VREG w2, w1                     // x2<- fp[B]
    GET_INST_OPCODE ip                  // ip<- opcode from wINST
    .if 0
    SET_VREG_OBJECT w2, w0              // fp[A]<- x2
    .else
    SET_VREG w2, w0                     // fp[A]<- x2
    .endif
    GOTO_OPCODE ip                      // execute next instruction

    NAME_END nterp_op_long_to_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_long_to_float: /* 0x85 */
    NAME_START nterp_op_long_to_float

    /*
     * Generic 64bit-to-32bit floating point unary operation.  Provide an
     * "instr" line that specifies an instruction that performs "s0 = op x0".
     *
     * For: int-to-double, float-to-double, float-to-long
     */
    /* unop vA, vB */
    lsr     w3, wINST, #12              // w3<- B
    ubfx    w4, wINST, #8, #4           // w4<- A
    GET_VREG_WIDE x0, w3
    FETCH_ADVANCE_INST 1                // advance rPC, load wINST
    scvtf s0, x0                              // d0<- op
    GET_INST_OPCODE ip                  // extract opcode from wINST
    SET_VREG_FLOAT s0, w4          // vA<- d0
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_long_to_float

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_long_to_double: /* 0x86 */
    NAME_START nterp_op_long_to_double

    /*
     * Generic 64bit-to-64bit floating point unary operation.  Provide an
     * "instr" line that specifies an instruction that performs "d0 = op x0".
     *
     * For: long-to-double, double-to-long
     */
    /* unop vA, vB */
    lsr     w3, wINST, #12              // w3<- B
    ubfx    w4, wINST, #8, #4           // w4<- A
    GET_VREG_WIDE x0, w3
    FETCH_ADVANCE_INST 1                // advance rPC, load wINST
    scvtf d0, x0                              // d0<- op
    GET_INST_OPCODE ip                  // extract opcode from wINST
    SET_VREG_DOUBLE d0, w4         // vA<- d0
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_long_to_double

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_float_to_int: /* 0x87 */
    NAME_START nterp_op_float_to_int

    /*
     * Generic 32bit-to-32bit floating point unary operation.  Provide an
     * "instr" line that specifies an instruction that performs "w0 = op s0".
     *
     * For: int-to-float, float-to-int
     * TODO: refactor all of the conversions - parameterize width and use same template.
     */
    /* unop vA, vB */
    lsr     w3, wINST, #12              // w3<- B
    ubfx    w4, wINST, #8, #4           // w4<- A
    GET_VREG s0, w3
    FETCH_ADVANCE_INST 1                // advance rPC, load wINST
    fcvtzs w0, s0                              // d0<- op
    GET_INST_OPCODE ip                  // extract opcode from wINST
    SET_VREG_FLOAT w0, w4          // vA<- d0
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_float_to_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_float_to_long: /* 0x88 */
    NAME_START nterp_op_float_to_long

    /*
     * Generic 32bit-to-64bit floating point unary operation.  Provide an
     * "instr" line that specifies an instruction that performs "x0 = op s0".
     *
     * For: int-to-double, float-to-double, float-to-long
     */
    /* unop vA, vB */
    lsr     w3, wINST, #12              // w3<- B
    ubfx    w4, wINST, #8, #4           // w4<- A
    GET_VREG s0, w3
    FETCH_ADVANCE_INST 1                // advance rPC, load wINST
    fcvtzs x0, s0                              // d0<- op
    GET_INST_OPCODE ip                  // extract opcode from wINST
    SET_VREG_WIDE x0, w4           // vA<- d0
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_float_to_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_float_to_double: /* 0x89 */
    NAME_START nterp_op_float_to_double

    /*
     * Generic 32bit-to-64bit floating point unary operation.  Provide an
     * "instr" line that specifies an instruction that performs "d0 = op s0".
     *
     * For: int-to-double, float-to-double, float-to-long
     */
    /* unop vA, vB */
    lsr     w3, wINST, #12              // w3<- B
    ubfx    w4, wINST, #8, #4           // w4<- A
    GET_VREG s0, w3
    FETCH_ADVANCE_INST 1                // advance rPC, load wINST
    fcvt  d0, s0                              // d0<- op
    GET_INST_OPCODE ip                  // extract opcode from wINST
    SET_VREG_WIDE d0, w4           // vA<- d0
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_float_to_double

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_double_to_int: /* 0x8a */
    NAME_START nterp_op_double_to_int

    /*
     * Generic 64bit-to-32bit floating point unary operation.  Provide an
     * "instr" line that specifies an instruction that performs "w0 = op d0".
     *
     * For: int-to-double, float-to-double, float-to-long
     */
    /* unop vA, vB */
    lsr     w3, wINST, #12              // w3<- B
    ubfx    w4, wINST, #8, #4           // w4<- A
    GET_VREG_DOUBLE d0, w3
    FETCH_ADVANCE_INST 1                // advance rPC, load wINST
    fcvtzs w0, d0                              // d0<- op
    GET_INST_OPCODE ip                  // extract opcode from wINST
    SET_VREG_FLOAT w0, w4          // vA<- d0
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_double_to_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_double_to_long: /* 0x8b */
    NAME_START nterp_op_double_to_long

    /*
     * Generic 64bit-to-64bit floating point unary operation.  Provide an
     * "instr" line that specifies an instruction that performs "x0 = op d0".
     *
     * For: long-to-double, double-to-long
     */
    /* unop vA, vB */
    lsr     w3, wINST, #12              // w3<- B
    ubfx    w4, wINST, #8, #4           // w4<- A
    GET_VREG_DOUBLE d0, w3
    FETCH_ADVANCE_INST 1                // advance rPC, load wINST
    fcvtzs x0, d0                              // d0<- op
    GET_INST_OPCODE ip                  // extract opcode from wINST
    SET_VREG_WIDE x0, w4           // vA<- d0
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_double_to_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_double_to_float: /* 0x8c */
    NAME_START nterp_op_double_to_float

    /*
     * Generic 64bit-to-32bit floating point unary operation.  Provide an
     * "instr" line that specifies an instruction that performs "s0 = op d0".
     *
     * For: int-to-double, float-to-double, float-to-long
     */
    /* unop vA, vB */
    lsr     w3, wINST, #12              // w3<- B
    ubfx    w4, wINST, #8, #4           // w4<- A
    GET_VREG_DOUBLE d0, w3
    FETCH_ADVANCE_INST 1                // advance rPC, load wINST
    fcvt s0, d0                              // d0<- op
    GET_INST_OPCODE ip                  // extract opcode from wINST
    SET_VREG_FLOAT s0, w4          // vA<- d0
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_double_to_float

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_int_to_byte: /* 0x8d */
    NAME_START nterp_op_int_to_byte

    /*
     * Generic 32-bit unary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = op w0".
     * This could be an ARM instruction or a function call.
     *
     * for: neg-int, not-int, neg-float, int-to-float, float-to-int,
     *      int-to-byte, int-to-char, int-to-short
     */
    /* unop vA, vB */
    lsr     w3, wINST, #12              // w3<- B
    GET_VREG w0, w3                     // w0<- vB
    ubfx    w9, wINST, #8, #4           // w9<- A
    FETCH_ADVANCE_INST 1                // advance rPC, load rINST
    sxtb    w0, w0                              // w0<- op, w0-w3 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG w0, w9                     // vAA<- w0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 8-9 instructions */

    NAME_END nterp_op_int_to_byte

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_int_to_char: /* 0x8e */
    NAME_START nterp_op_int_to_char

    /*
     * Generic 32-bit unary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = op w0".
     * This could be an ARM instruction or a function call.
     *
     * for: neg-int, not-int, neg-float, int-to-float, float-to-int,
     *      int-to-byte, int-to-char, int-to-short
     */
    /* unop vA, vB */
    lsr     w3, wINST, #12              // w3<- B
    GET_VREG w0, w3                     // w0<- vB
    ubfx    w9, wINST, #8, #4           // w9<- A
    FETCH_ADVANCE_INST 1                // advance rPC, load rINST
    uxth    w0, w0                              // w0<- op, w0-w3 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG w0, w9                     // vAA<- w0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 8-9 instructions */

    NAME_END nterp_op_int_to_char

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_int_to_short: /* 0x8f */
    NAME_START nterp_op_int_to_short

    /*
     * Generic 32-bit unary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = op w0".
     * This could be an ARM instruction or a function call.
     *
     * for: neg-int, not-int, neg-float, int-to-float, float-to-int,
     *      int-to-byte, int-to-char, int-to-short
     */
    /* unop vA, vB */
    lsr     w3, wINST, #12              // w3<- B
    GET_VREG w0, w3                     // w0<- vB
    ubfx    w9, wINST, #8, #4           // w9<- A
    FETCH_ADVANCE_INST 1                // advance rPC, load rINST
    sxth    w0, w0                              // w0<- op, w0-w3 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG w0, w9                     // vAA<- w0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 8-9 instructions */

    NAME_END nterp_op_int_to_short

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_add_int: /* 0x90 */
    NAME_START nterp_op_add_int

    /*
     * Generic 32-bit binary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = w0 op w1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than w0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.  Note that we
     * *don't* check for (INT_MIN / -1) here, because the ARM math lib
     * handles it correctly.
     *
     * For: add-int, sub-int, mul-int, div-int, rem-int, and-int, or-int,
     *      xor-int, shl-int, shr-int, ushr-int, add-float, sub-float,
     *      mul-float, div-float, rem-float
     */
    /* binop vAA, vBB, vCC */
    FETCH w0, 1                         // w0<- CCBB
    lsr     w9, wINST, #8               // w9<- AA
    lsr     w3, w0, #8                  // w3<- CC
    and     w2, w0, #255                // w2<- BB
    GET_VREG w1, w3                     // w1<- vCC
    GET_VREG w0, w2                     // w0<- vBB
    .if 0
    cbz     w1, common_errDivideByZero  // is second operand zero?
    .endif
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
                               // optional op; may set condition codes
    add     w0, w0, w1                              // w0<- op, w0-w3 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG w0, w9                // vAA<- w0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 11-14 instructions */

    NAME_END nterp_op_add_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sub_int: /* 0x91 */
    NAME_START nterp_op_sub_int

    /*
     * Generic 32-bit binary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = w0 op w1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than w0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.  Note that we
     * *don't* check for (INT_MIN / -1) here, because the ARM math lib
     * handles it correctly.
     *
     * For: add-int, sub-int, mul-int, div-int, rem-int, and-int, or-int,
     *      xor-int, shl-int, shr-int, ushr-int, add-float, sub-float,
     *      mul-float, div-float, rem-float
     */
    /* binop vAA, vBB, vCC */
    FETCH w0, 1                         // w0<- CCBB
    lsr     w9, wINST, #8               // w9<- AA
    lsr     w3, w0, #8                  // w3<- CC
    and     w2, w0, #255                // w2<- BB
    GET_VREG w1, w3                     // w1<- vCC
    GET_VREG w0, w2                     // w0<- vBB
    .if 0
    cbz     w1, common_errDivideByZero  // is second operand zero?
    .endif
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
                               // optional op; may set condition codes
    sub     w0, w0, w1                              // w0<- op, w0-w3 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG w0, w9                // vAA<- w0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 11-14 instructions */

    NAME_END nterp_op_sub_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_mul_int: /* 0x92 */
    NAME_START nterp_op_mul_int

/* must be "mul w0, w1, w0" -- "w0, w0, w1" is illegal */
    /*
     * Generic 32-bit binary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = w0 op w1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than w0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.  Note that we
     * *don't* check for (INT_MIN / -1) here, because the ARM math lib
     * handles it correctly.
     *
     * For: add-int, sub-int, mul-int, div-int, rem-int, and-int, or-int,
     *      xor-int, shl-int, shr-int, ushr-int, add-float, sub-float,
     *      mul-float, div-float, rem-float
     */
    /* binop vAA, vBB, vCC */
    FETCH w0, 1                         // w0<- CCBB
    lsr     w9, wINST, #8               // w9<- AA
    lsr     w3, w0, #8                  // w3<- CC
    and     w2, w0, #255                // w2<- BB
    GET_VREG w1, w3                     // w1<- vCC
    GET_VREG w0, w2                     // w0<- vBB
    .if 0
    cbz     w1, common_errDivideByZero  // is second operand zero?
    .endif
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
                               // optional op; may set condition codes
    mul     w0, w1, w0                              // w0<- op, w0-w3 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG w0, w9                // vAA<- w0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 11-14 instructions */

    NAME_END nterp_op_mul_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_div_int: /* 0x93 */
    NAME_START nterp_op_div_int

    /*
     * Generic 32-bit binary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = w0 op w1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than w0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.  Note that we
     * *don't* check for (INT_MIN / -1) here, because the ARM math lib
     * handles it correctly.
     *
     * For: add-int, sub-int, mul-int, div-int, rem-int, and-int, or-int,
     *      xor-int, shl-int, shr-int, ushr-int, add-float, sub-float,
     *      mul-float, div-float, rem-float
     */
    /* binop vAA, vBB, vCC */
    FETCH w0, 1                         // w0<- CCBB
    lsr     w9, wINST, #8               // w9<- AA
    lsr     w3, w0, #8                  // w3<- CC
    and     w2, w0, #255                // w2<- BB
    GET_VREG w1, w3                     // w1<- vCC
    GET_VREG w0, w2                     // w0<- vBB
    .if 1
    cbz     w1, common_errDivideByZero  // is second operand zero?
    .endif
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
                               // optional op; may set condition codes
    sdiv     w0, w0, w1                              // w0<- op, w0-w3 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG w0, w9                // vAA<- w0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 11-14 instructions */

    NAME_END nterp_op_div_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_rem_int: /* 0x94 */
    NAME_START nterp_op_rem_int

    /*
     * Generic 32-bit binary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = w0 op w1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than w0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.  Note that we
     * *don't* check for (INT_MIN / -1) here, because the ARM math lib
     * handles it correctly.
     *
     * For: add-int, sub-int, mul-int, div-int, rem-int, and-int, or-int,
     *      xor-int, shl-int, shr-int, ushr-int, add-float, sub-float,
     *      mul-float, div-float, rem-float
     */
    /* binop vAA, vBB, vCC */
    FETCH w0, 1                         // w0<- CCBB
    lsr     w9, wINST, #8               // w9<- AA
    lsr     w3, w0, #8                  // w3<- CC
    and     w2, w0, #255                // w2<- BB
    GET_VREG w1, w3                     // w1<- vCC
    GET_VREG w0, w2                     // w0<- vBB
    .if 1
    cbz     w1, common_errDivideByZero  // is second operand zero?
    .endif
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
    sdiv     w2, w0, w1                           // optional op; may set condition codes
    msub w0, w2, w1, w0                              // w0<- op, w0-w3 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG w0, w9                // vAA<- w0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 11-14 instructions */

    NAME_END nterp_op_rem_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_and_int: /* 0x95 */
    NAME_START nterp_op_and_int

    /*
     * Generic 32-bit binary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = w0 op w1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than w0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.  Note that we
     * *don't* check for (INT_MIN / -1) here, because the ARM math lib
     * handles it correctly.
     *
     * For: add-int, sub-int, mul-int, div-int, rem-int, and-int, or-int,
     *      xor-int, shl-int, shr-int, ushr-int, add-float, sub-float,
     *      mul-float, div-float, rem-float
     */
    /* binop vAA, vBB, vCC */
    FETCH w0, 1                         // w0<- CCBB
    lsr     w9, wINST, #8               // w9<- AA
    lsr     w3, w0, #8                  // w3<- CC
    and     w2, w0, #255                // w2<- BB
    GET_VREG w1, w3                     // w1<- vCC
    GET_VREG w0, w2                     // w0<- vBB
    .if 0
    cbz     w1, common_errDivideByZero  // is second operand zero?
    .endif
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
                               // optional op; may set condition codes
    and     w0, w0, w1                              // w0<- op, w0-w3 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG w0, w9                // vAA<- w0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 11-14 instructions */

    NAME_END nterp_op_and_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_or_int: /* 0x96 */
    NAME_START nterp_op_or_int

    /*
     * Generic 32-bit binary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = w0 op w1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than w0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.  Note that we
     * *don't* check for (INT_MIN / -1) here, because the ARM math lib
     * handles it correctly.
     *
     * For: add-int, sub-int, mul-int, div-int, rem-int, and-int, or-int,
     *      xor-int, shl-int, shr-int, ushr-int, add-float, sub-float,
     *      mul-float, div-float, rem-float
     */
    /* binop vAA, vBB, vCC */
    FETCH w0, 1                         // w0<- CCBB
    lsr     w9, wINST, #8               // w9<- AA
    lsr     w3, w0, #8                  // w3<- CC
    and     w2, w0, #255                // w2<- BB
    GET_VREG w1, w3                     // w1<- vCC
    GET_VREG w0, w2                     // w0<- vBB
    .if 0
    cbz     w1, common_errDivideByZero  // is second operand zero?
    .endif
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
                               // optional op; may set condition codes
    orr     w0, w0, w1                              // w0<- op, w0-w3 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG w0, w9                // vAA<- w0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 11-14 instructions */

    NAME_END nterp_op_or_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_xor_int: /* 0x97 */
    NAME_START nterp_op_xor_int

    /*
     * Generic 32-bit binary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = w0 op w1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than w0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.  Note that we
     * *don't* check for (INT_MIN / -1) here, because the ARM math lib
     * handles it correctly.
     *
     * For: add-int, sub-int, mul-int, div-int, rem-int, and-int, or-int,
     *      xor-int, shl-int, shr-int, ushr-int, add-float, sub-float,
     *      mul-float, div-float, rem-float
     */
    /* binop vAA, vBB, vCC */
    FETCH w0, 1                         // w0<- CCBB
    lsr     w9, wINST, #8               // w9<- AA
    lsr     w3, w0, #8                  // w3<- CC
    and     w2, w0, #255                // w2<- BB
    GET_VREG w1, w3                     // w1<- vCC
    GET_VREG w0, w2                     // w0<- vBB
    .if 0
    cbz     w1, common_errDivideByZero  // is second operand zero?
    .endif
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
                               // optional op; may set condition codes
    eor     w0, w0, w1                              // w0<- op, w0-w3 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG w0, w9                // vAA<- w0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 11-14 instructions */

    NAME_END nterp_op_xor_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_shl_int: /* 0x98 */
    NAME_START nterp_op_shl_int

    /*
     * Generic 32-bit binary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = w0 op w1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than w0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.  Note that we
     * *don't* check for (INT_MIN / -1) here, because the ARM math lib
     * handles it correctly.
     *
     * For: add-int, sub-int, mul-int, div-int, rem-int, and-int, or-int,
     *      xor-int, shl-int, shr-int, ushr-int, add-float, sub-float,
     *      mul-float, div-float, rem-float
     */
    /* binop vAA, vBB, vCC */
    FETCH w0, 1                         // w0<- CCBB
    lsr     w9, wINST, #8               // w9<- AA
    lsr     w3, w0, #8                  // w3<- CC
    and     w2, w0, #255                // w2<- BB
    GET_VREG w1, w3                     // w1<- vCC
    GET_VREG w0, w2                     // w0<- vBB
    .if 0
    cbz     w1, common_errDivideByZero  // is second operand zero?
    .endif
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
                               // optional op; may set condition codes
    lsl     w0, w0, w1                              // w0<- op, w0-w3 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG w0, w9                // vAA<- w0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 11-14 instructions */

    NAME_END nterp_op_shl_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_shr_int: /* 0x99 */
    NAME_START nterp_op_shr_int

    /*
     * Generic 32-bit binary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = w0 op w1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than w0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.  Note that we
     * *don't* check for (INT_MIN / -1) here, because the ARM math lib
     * handles it correctly.
     *
     * For: add-int, sub-int, mul-int, div-int, rem-int, and-int, or-int,
     *      xor-int, shl-int, shr-int, ushr-int, add-float, sub-float,
     *      mul-float, div-float, rem-float
     */
    /* binop vAA, vBB, vCC */
    FETCH w0, 1                         // w0<- CCBB
    lsr     w9, wINST, #8               // w9<- AA
    lsr     w3, w0, #8                  // w3<- CC
    and     w2, w0, #255                // w2<- BB
    GET_VREG w1, w3                     // w1<- vCC
    GET_VREG w0, w2                     // w0<- vBB
    .if 0
    cbz     w1, common_errDivideByZero  // is second operand zero?
    .endif
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
                               // optional op; may set condition codes
    asr     w0, w0, w1                              // w0<- op, w0-w3 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG w0, w9                // vAA<- w0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 11-14 instructions */

    NAME_END nterp_op_shr_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_ushr_int: /* 0x9a */
    NAME_START nterp_op_ushr_int

    /*
     * Generic 32-bit binary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = w0 op w1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than w0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.  Note that we
     * *don't* check for (INT_MIN / -1) here, because the ARM math lib
     * handles it correctly.
     *
     * For: add-int, sub-int, mul-int, div-int, rem-int, and-int, or-int,
     *      xor-int, shl-int, shr-int, ushr-int, add-float, sub-float,
     *      mul-float, div-float, rem-float
     */
    /* binop vAA, vBB, vCC */
    FETCH w0, 1                         // w0<- CCBB
    lsr     w9, wINST, #8               // w9<- AA
    lsr     w3, w0, #8                  // w3<- CC
    and     w2, w0, #255                // w2<- BB
    GET_VREG w1, w3                     // w1<- vCC
    GET_VREG w0, w2                     // w0<- vBB
    .if 0
    cbz     w1, common_errDivideByZero  // is second operand zero?
    .endif
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
                               // optional op; may set condition codes
    lsr     w0, w0, w1                              // w0<- op, w0-w3 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG w0, w9                // vAA<- w0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 11-14 instructions */

    NAME_END nterp_op_ushr_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_add_long: /* 0x9b */
    NAME_START nterp_op_add_long

    /*
     * Generic 64-bit binary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = x1 op x2".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than x0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.
     *
     * For: add-long, sub-long, mul-long, div-long, rem-long, and-long, or-long,
     *      xor-long, add-double, sub-double, mul-double, div-double, rem-double
     */
    /* binop vAA, vBB, vCC */
    FETCH w0, 1                         // w0<- CCBB
    lsr     w4, wINST, #8               // w4<- AA
    lsr     w2, w0, #8                  // w2<- CC
    and     w1, w0, #255                // w1<- BB
    GET_VREG_WIDE x2, w2               // w2<- vCC
    GET_VREG_WIDE x1, w1               // w1<- vBB
    .if 0
    cbz     x2, common_errDivideByZero  // is second operand zero?
    .endif
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
    
    add x0, x1, x2                              // x0<- op, w0-w4 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG_WIDE x0, w4           // vAA<- x0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 11-14 instructions */

    NAME_END nterp_op_add_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sub_long: /* 0x9c */
    NAME_START nterp_op_sub_long

    /*
     * Generic 64-bit binary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = x1 op x2".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than x0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.
     *
     * For: add-long, sub-long, mul-long, div-long, rem-long, and-long, or-long,
     *      xor-long, add-double, sub-double, mul-double, div-double, rem-double
     */
    /* binop vAA, vBB, vCC */
    FETCH w0, 1                         // w0<- CCBB
    lsr     w4, wINST, #8               // w4<- AA
    lsr     w2, w0, #8                  // w2<- CC
    and     w1, w0, #255                // w1<- BB
    GET_VREG_WIDE x2, w2               // w2<- vCC
    GET_VREG_WIDE x1, w1               // w1<- vBB
    .if 0
    cbz     x2, common_errDivideByZero  // is second operand zero?
    .endif
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
    
    sub x0, x1, x2                              // x0<- op, w0-w4 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG_WIDE x0, w4           // vAA<- x0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 11-14 instructions */

    NAME_END nterp_op_sub_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_mul_long: /* 0x9d */
    NAME_START nterp_op_mul_long

    /*
     * Generic 64-bit binary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = x1 op x2".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than x0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.
     *
     * For: add-long, sub-long, mul-long, div-long, rem-long, and-long, or-long,
     *      xor-long, add-double, sub-double, mul-double, div-double, rem-double
     */
    /* binop vAA, vBB, vCC */
    FETCH w0, 1                         // w0<- CCBB
    lsr     w4, wINST, #8               // w4<- AA
    lsr     w2, w0, #8                  // w2<- CC
    and     w1, w0, #255                // w1<- BB
    GET_VREG_WIDE x2, w2               // w2<- vCC
    GET_VREG_WIDE x1, w1               // w1<- vBB
    .if 0
    cbz     x2, common_errDivideByZero  // is second operand zero?
    .endif
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
    
    mul x0, x1, x2                              // x0<- op, w0-w4 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG_WIDE x0, w4           // vAA<- x0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 11-14 instructions */

    NAME_END nterp_op_mul_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_div_long: /* 0x9e */
    NAME_START nterp_op_div_long

    /*
     * Generic 64-bit binary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = x1 op x2".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than x0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.
     *
     * For: add-long, sub-long, mul-long, div-long, rem-long, and-long, or-long,
     *      xor-long, add-double, sub-double, mul-double, div-double, rem-double
     */
    /* binop vAA, vBB, vCC */
    FETCH w0, 1                         // w0<- CCBB
    lsr     w4, wINST, #8               // w4<- AA
    lsr     w2, w0, #8                  // w2<- CC
    and     w1, w0, #255                // w1<- BB
    GET_VREG_WIDE x2, w2               // w2<- vCC
    GET_VREG_WIDE x1, w1               // w1<- vBB
    .if 1
    cbz     x2, common_errDivideByZero  // is second operand zero?
    .endif
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
    
    sdiv x0, x1, x2                              // x0<- op, w0-w4 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG_WIDE x0, w4           // vAA<- x0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 11-14 instructions */

    NAME_END nterp_op_div_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_rem_long: /* 0x9f */
    NAME_START nterp_op_rem_long

    /*
     * Generic 64-bit binary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = x1 op x2".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than x0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.
     *
     * For: add-long, sub-long, mul-long, div-long, rem-long, and-long, or-long,
     *      xor-long, add-double, sub-double, mul-double, div-double, rem-double
     */
    /* binop vAA, vBB, vCC */
    FETCH w0, 1                         // w0<- CCBB
    lsr     w4, wINST, #8               // w4<- AA
    lsr     w2, w0, #8                  // w2<- CC
    and     w1, w0, #255                // w1<- BB
    GET_VREG_WIDE x2, w2               // w2<- vCC
    GET_VREG_WIDE x1, w1               // w1<- vBB
    .if 1
    cbz     x2, common_errDivideByZero  // is second operand zero?
    .endif
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
    sdiv x3, x1, x2
    msub x0, x3, x2, x1                              // x0<- op, w0-w4 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG_WIDE x0, w4           // vAA<- x0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 11-14 instructions */

    NAME_END nterp_op_rem_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_and_long: /* 0xa0 */
    NAME_START nterp_op_and_long

    /*
     * Generic 64-bit binary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = x1 op x2".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than x0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.
     *
     * For: add-long, sub-long, mul-long, div-long, rem-long, and-long, or-long,
     *      xor-long, add-double, sub-double, mul-double, div-double, rem-double
     */
    /* binop vAA, vBB, vCC */
    FETCH w0, 1                         // w0<- CCBB
    lsr     w4, wINST, #8               // w4<- AA
    lsr     w2, w0, #8                  // w2<- CC
    and     w1, w0, #255                // w1<- BB
    GET_VREG_WIDE x2, w2               // w2<- vCC
    GET_VREG_WIDE x1, w1               // w1<- vBB
    .if 0
    cbz     x2, common_errDivideByZero  // is second operand zero?
    .endif
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
    
    and x0, x1, x2                              // x0<- op, w0-w4 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG_WIDE x0, w4           // vAA<- x0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 11-14 instructions */

    NAME_END nterp_op_and_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_or_long: /* 0xa1 */
    NAME_START nterp_op_or_long

    /*
     * Generic 64-bit binary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = x1 op x2".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than x0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.
     *
     * For: add-long, sub-long, mul-long, div-long, rem-long, and-long, or-long,
     *      xor-long, add-double, sub-double, mul-double, div-double, rem-double
     */
    /* binop vAA, vBB, vCC */
    FETCH w0, 1                         // w0<- CCBB
    lsr     w4, wINST, #8               // w4<- AA
    lsr     w2, w0, #8                  // w2<- CC
    and     w1, w0, #255                // w1<- BB
    GET_VREG_WIDE x2, w2               // w2<- vCC
    GET_VREG_WIDE x1, w1               // w1<- vBB
    .if 0
    cbz     x2, common_errDivideByZero  // is second operand zero?
    .endif
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
    
    orr x0, x1, x2                              // x0<- op, w0-w4 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG_WIDE x0, w4           // vAA<- x0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 11-14 instructions */

    NAME_END nterp_op_or_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_xor_long: /* 0xa2 */
    NAME_START nterp_op_xor_long

    /*
     * Generic 64-bit binary operation.  Provide an "instr" line that
     * specifies an instruction that performs "result = x1 op x2".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than x0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.
     *
     * For: add-long, sub-long, mul-long, div-long, rem-long, and-long, or-long,
     *      xor-long, add-double, sub-double, mul-double, div-double, rem-double
     */
    /* binop vAA, vBB, vCC */
    FETCH w0, 1                         // w0<- CCBB
    lsr     w4, wINST, #8               // w4<- AA
    lsr     w2, w0, #8                  // w2<- CC
    and     w1, w0, #255                // w1<- BB
    GET_VREG_WIDE x2, w2               // w2<- vCC
    GET_VREG_WIDE x1, w1               // w1<- vBB
    .if 0
    cbz     x2, common_errDivideByZero  // is second operand zero?
    .endif
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
    
    eor x0, x1, x2                              // x0<- op, w0-w4 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG_WIDE x0, w4           // vAA<- x0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 11-14 instructions */

    NAME_END nterp_op_xor_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_shl_long: /* 0xa3 */
    NAME_START nterp_op_shl_long

    /*
     * 64-bit shift operation.
     *
     * For: shl-long, shr-long, ushr-long
     */
    /* binop vAA, vBB, vCC */
    FETCH w0, 1                         // w0<- CCBB
    lsr      w3, wINST, #8               // w3<- AA
    lsr      w2, w0, #8                  // w2<- CC
    GET_VREG w2, w2                     // w2<- vCC (shift count)
    and      w1, w0, #255                // w1<- BB
    GET_VREG_WIDE x1, w1                // x1<- vBB
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
    lsl  x0, x1, x2                 // Do the shift. Only low 6 bits of x2 are used.
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG_WIDE x0, w3                // vAA<- x0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 11-14 instructions */

    NAME_END nterp_op_shl_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_shr_long: /* 0xa4 */
    NAME_START nterp_op_shr_long

    /*
     * 64-bit shift operation.
     *
     * For: shl-long, shr-long, ushr-long
     */
    /* binop vAA, vBB, vCC */
    FETCH w0, 1                         // w0<- CCBB
    lsr      w3, wINST, #8               // w3<- AA
    lsr      w2, w0, #8                  // w2<- CC
    GET_VREG w2, w2                     // w2<- vCC (shift count)
    and      w1, w0, #255                // w1<- BB
    GET_VREG_WIDE x1, w1                // x1<- vBB
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
    asr  x0, x1, x2                 // Do the shift. Only low 6 bits of x2 are used.
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG_WIDE x0, w3                // vAA<- x0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 11-14 instructions */

    NAME_END nterp_op_shr_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_ushr_long: /* 0xa5 */
    NAME_START nterp_op_ushr_long

    /*
     * 64-bit shift operation.
     *
     * For: shl-long, shr-long, ushr-long
     */
    /* binop vAA, vBB, vCC */
    FETCH w0, 1                         // w0<- CCBB
    lsr      w3, wINST, #8               // w3<- AA
    lsr      w2, w0, #8                  // w2<- CC
    GET_VREG w2, w2                     // w2<- vCC (shift count)
    and      w1, w0, #255                // w1<- BB
    GET_VREG_WIDE x1, w1                // x1<- vBB
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
    lsr  x0, x1, x2                 // Do the shift. Only low 6 bits of x2 are used.
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG_WIDE x0, w3                // vAA<- x0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 11-14 instructions */

    NAME_END nterp_op_ushr_long

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_add_float: /* 0xa6 */
    NAME_START nterp_op_add_float

    /*
     * Generic 32-bit floating-point operation.
     *
     * For: add-float, sub-float, mul-float, div-float
     * form: <op> s0, s0, s1
     */
    /* floatop vAA, vBB, vCC */
    FETCH w0, 1                         // r0<- CCBB
    lsr     w1, w0, #8                  // r2<- CC
    and     w0, w0, #255                // r1<- BB
    GET_VREG  s1, w1
    GET_VREG  s0, w0
    fadd   s0, s0, s1                              // s0<- op
    lsr     w1, wINST, #8               // r1<- AA
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG_FLOAT s0, w1
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_add_float

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sub_float: /* 0xa7 */
    NAME_START nterp_op_sub_float

    /*
     * Generic 32-bit floating-point operation.
     *
     * For: add-float, sub-float, mul-float, div-float
     * form: <op> s0, s0, s1
     */
    /* floatop vAA, vBB, vCC */
    FETCH w0, 1                         // r0<- CCBB
    lsr     w1, w0, #8                  // r2<- CC
    and     w0, w0, #255                // r1<- BB
    GET_VREG  s1, w1
    GET_VREG  s0, w0
    fsub   s0, s0, s1                              // s0<- op
    lsr     w1, wINST, #8               // r1<- AA
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG_FLOAT s0, w1
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_sub_float

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_mul_float: /* 0xa8 */
    NAME_START nterp_op_mul_float

    /*
     * Generic 32-bit floating-point operation.
     *
     * For: add-float, sub-float, mul-float, div-float
     * form: <op> s0, s0, s1
     */
    /* floatop vAA, vBB, vCC */
    FETCH w0, 1                         // r0<- CCBB
    lsr     w1, w0, #8                  // r2<- CC
    and     w0, w0, #255                // r1<- BB
    GET_VREG  s1, w1
    GET_VREG  s0, w0
    fmul   s0, s0, s1                              // s0<- op
    lsr     w1, wINST, #8               // r1<- AA
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG_FLOAT s0, w1
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_mul_float

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_div_float: /* 0xa9 */
    NAME_START nterp_op_div_float

    /*
     * Generic 32-bit floating-point operation.
     *
     * For: add-float, sub-float, mul-float, div-float
     * form: <op> s0, s0, s1
     */
    /* floatop vAA, vBB, vCC */
    FETCH w0, 1                         // r0<- CCBB
    lsr     w1, w0, #8                  // r2<- CC
    and     w0, w0, #255                // r1<- BB
    GET_VREG  s1, w1
    GET_VREG  s0, w0
    fdiv   s0, s0, s1                              // s0<- op
    lsr     w1, wINST, #8               // r1<- AA
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG_FLOAT s0, w1
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_div_float

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_rem_float: /* 0xaa */
    NAME_START nterp_op_rem_float

/* EABI doesn't define a float remainder function, but libm does */
    /*
     * Generic 32-bit floating-point operation.
     *
     * For: add-float, sub-float, mul-float, div-float
     * form: <op> s0, s0, s1
     */
    /* floatop vAA, vBB, vCC */
    FETCH w0, 1                         // r0<- CCBB
    lsr     w1, w0, #8                  // r2<- CC
    and     w0, w0, #255                // r1<- BB
    GET_VREG  s1, w1
    GET_VREG  s0, w0
    bl      fmodf                              // s0<- op
    lsr     w1, wINST, #8               // r1<- AA
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG_FLOAT s0, w1
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_rem_float

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_add_double: /* 0xab */
    NAME_START nterp_op_add_double

    /*
     * Generic 64-bit floating-point operation.
     */
    /* binop vAA, vBB, vCC */
    FETCH w0, 1                         // w0<- CCBB
    lsr     w4, wINST, #8               // w4<- AA
    lsr     w2, w0, #8                  // w2<- CC
    and     w1, w0, #255                // w1<- BB
    GET_VREG_DOUBLE d2, w2             // w2<- vCC
    GET_VREG_DOUBLE d1, w1             // w1<- vBB
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
    fadd d0, d1, d2                              // d0<- op, w0-w4 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG_DOUBLE d0, w4         // vAA<- d0
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_add_double

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sub_double: /* 0xac */
    NAME_START nterp_op_sub_double

    /*
     * Generic 64-bit floating-point operation.
     */
    /* binop vAA, vBB, vCC */
    FETCH w0, 1                         // w0<- CCBB
    lsr     w4, wINST, #8               // w4<- AA
    lsr     w2, w0, #8                  // w2<- CC
    and     w1, w0, #255                // w1<- BB
    GET_VREG_DOUBLE d2, w2             // w2<- vCC
    GET_VREG_DOUBLE d1, w1             // w1<- vBB
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
    fsub d0, d1, d2                              // d0<- op, w0-w4 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG_DOUBLE d0, w4         // vAA<- d0
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_sub_double

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_mul_double: /* 0xad */
    NAME_START nterp_op_mul_double

    /*
     * Generic 64-bit floating-point operation.
     */
    /* binop vAA, vBB, vCC */
    FETCH w0, 1                         // w0<- CCBB
    lsr     w4, wINST, #8               // w4<- AA
    lsr     w2, w0, #8                  // w2<- CC
    and     w1, w0, #255                // w1<- BB
    GET_VREG_DOUBLE d2, w2             // w2<- vCC
    GET_VREG_DOUBLE d1, w1             // w1<- vBB
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
    fmul d0, d1, d2                              // d0<- op, w0-w4 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG_DOUBLE d0, w4         // vAA<- d0
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_mul_double

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_div_double: /* 0xae */
    NAME_START nterp_op_div_double

    /*
     * Generic 64-bit floating-point operation.
     */
    /* binop vAA, vBB, vCC */
    FETCH w0, 1                         // w0<- CCBB
    lsr     w4, wINST, #8               // w4<- AA
    lsr     w2, w0, #8                  // w2<- CC
    and     w1, w0, #255                // w1<- BB
    GET_VREG_DOUBLE d2, w2             // w2<- vCC
    GET_VREG_DOUBLE d1, w1             // w1<- vBB
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
    fdiv d0, d1, d2                              // d0<- op, w0-w4 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG_DOUBLE d0, w4         // vAA<- d0
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_div_double

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_rem_double: /* 0xaf */
    NAME_START nterp_op_rem_double

    /* rem vAA, vBB, vCC */
    FETCH w0, 1                         // w0<- CCBB
    lsr     w2, w0, #8                  // w2<- CC
    and     w1, w0, #255                // w1<- BB
    GET_VREG_DOUBLE d1, w2              // d1<- vCC
    GET_VREG_DOUBLE d0, w1              // d0<- vBB
    bl  fmod
    lsr     w4, wINST, #8               // w4<- AA
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG_WIDE d0, w4                // vAA<- result
    GOTO_OPCODE ip                      // jump to next instruction
    /* 11-14 instructions */

    NAME_END nterp_op_rem_double

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_add_int_2addr: /* 0xb0 */
    NAME_START nterp_op_add_int_2addr

    /*
     * Generic 32-bit "/2addr" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = w0 op w1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than w0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.
     *
     * For: add-int/2addr, sub-int/2addr, mul-int/2addr, div-int/2addr,
     *      rem-int/2addr, and-int/2addr, or-int/2addr, xor-int/2addr,
     *      shl-int/2addr, shr-int/2addr, ushr-int/2addr, add-float/2addr,
     *      sub-float/2addr, mul-float/2addr, div-float/2addr, rem-float/2addr
     */
    /* binop/2addr vA, vB */
    lsr     w3, wINST, #12              // w3<- B
    ubfx    w9, wINST, #8, #4           // w9<- A
    GET_VREG w1, w3                     // w1<- vB
    GET_VREG w0, w9                     // w0<- vA
    .if 0
    cbz     w1, common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 1                // advance rPC, load rINST
                               // optional op; may set condition codes
    add     w0, w0, w1                              // w0<- op, w0-w3 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG w0, w9                // vAA<- w0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 10-13 instructions */

    NAME_END nterp_op_add_int_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sub_int_2addr: /* 0xb1 */
    NAME_START nterp_op_sub_int_2addr

    /*
     * Generic 32-bit "/2addr" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = w0 op w1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than w0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.
     *
     * For: add-int/2addr, sub-int/2addr, mul-int/2addr, div-int/2addr,
     *      rem-int/2addr, and-int/2addr, or-int/2addr, xor-int/2addr,
     *      shl-int/2addr, shr-int/2addr, ushr-int/2addr, add-float/2addr,
     *      sub-float/2addr, mul-float/2addr, div-float/2addr, rem-float/2addr
     */
    /* binop/2addr vA, vB */
    lsr     w3, wINST, #12              // w3<- B
    ubfx    w9, wINST, #8, #4           // w9<- A
    GET_VREG w1, w3                     // w1<- vB
    GET_VREG w0, w9                     // w0<- vA
    .if 0
    cbz     w1, common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 1                // advance rPC, load rINST
                               // optional op; may set condition codes
    sub     w0, w0, w1                              // w0<- op, w0-w3 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG w0, w9                // vAA<- w0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 10-13 instructions */

    NAME_END nterp_op_sub_int_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_mul_int_2addr: /* 0xb2 */
    NAME_START nterp_op_mul_int_2addr

/* must be "mul w0, w1, w0" -- "w0, w0, w1" is illegal */
    /*
     * Generic 32-bit "/2addr" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = w0 op w1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than w0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.
     *
     * For: add-int/2addr, sub-int/2addr, mul-int/2addr, div-int/2addr,
     *      rem-int/2addr, and-int/2addr, or-int/2addr, xor-int/2addr,
     *      shl-int/2addr, shr-int/2addr, ushr-int/2addr, add-float/2addr,
     *      sub-float/2addr, mul-float/2addr, div-float/2addr, rem-float/2addr
     */
    /* binop/2addr vA, vB */
    lsr     w3, wINST, #12              // w3<- B
    ubfx    w9, wINST, #8, #4           // w9<- A
    GET_VREG w1, w3                     // w1<- vB
    GET_VREG w0, w9                     // w0<- vA
    .if 0
    cbz     w1, common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 1                // advance rPC, load rINST
                               // optional op; may set condition codes
    mul     w0, w1, w0                              // w0<- op, w0-w3 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG w0, w9                // vAA<- w0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 10-13 instructions */

    NAME_END nterp_op_mul_int_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_div_int_2addr: /* 0xb3 */
    NAME_START nterp_op_div_int_2addr

    /*
     * Generic 32-bit "/2addr" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = w0 op w1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than w0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.
     *
     * For: add-int/2addr, sub-int/2addr, mul-int/2addr, div-int/2addr,
     *      rem-int/2addr, and-int/2addr, or-int/2addr, xor-int/2addr,
     *      shl-int/2addr, shr-int/2addr, ushr-int/2addr, add-float/2addr,
     *      sub-float/2addr, mul-float/2addr, div-float/2addr, rem-float/2addr
     */
    /* binop/2addr vA, vB */
    lsr     w3, wINST, #12              // w3<- B
    ubfx    w9, wINST, #8, #4           // w9<- A
    GET_VREG w1, w3                     // w1<- vB
    GET_VREG w0, w9                     // w0<- vA
    .if 1
    cbz     w1, common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 1                // advance rPC, load rINST
                               // optional op; may set condition codes
    sdiv     w0, w0, w1                              // w0<- op, w0-w3 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG w0, w9                // vAA<- w0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 10-13 instructions */

    NAME_END nterp_op_div_int_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_rem_int_2addr: /* 0xb4 */
    NAME_START nterp_op_rem_int_2addr

    /*
     * Generic 32-bit "/2addr" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = w0 op w1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than w0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.
     *
     * For: add-int/2addr, sub-int/2addr, mul-int/2addr, div-int/2addr,
     *      rem-int/2addr, and-int/2addr, or-int/2addr, xor-int/2addr,
     *      shl-int/2addr, shr-int/2addr, ushr-int/2addr, add-float/2addr,
     *      sub-float/2addr, mul-float/2addr, div-float/2addr, rem-float/2addr
     */
    /* binop/2addr vA, vB */
    lsr     w3, wINST, #12              // w3<- B
    ubfx    w9, wINST, #8, #4           // w9<- A
    GET_VREG w1, w3                     // w1<- vB
    GET_VREG w0, w9                     // w0<- vA
    .if 1
    cbz     w1, common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 1                // advance rPC, load rINST
    sdiv     w2, w0, w1                           // optional op; may set condition codes
    msub w0, w2, w1, w0                              // w0<- op, w0-w3 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG w0, w9                // vAA<- w0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 10-13 instructions */

    NAME_END nterp_op_rem_int_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_and_int_2addr: /* 0xb5 */
    NAME_START nterp_op_and_int_2addr

    /*
     * Generic 32-bit "/2addr" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = w0 op w1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than w0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.
     *
     * For: add-int/2addr, sub-int/2addr, mul-int/2addr, div-int/2addr,
     *      rem-int/2addr, and-int/2addr, or-int/2addr, xor-int/2addr,
     *      shl-int/2addr, shr-int/2addr, ushr-int/2addr, add-float/2addr,
     *      sub-float/2addr, mul-float/2addr, div-float/2addr, rem-float/2addr
     */
    /* binop/2addr vA, vB */
    lsr     w3, wINST, #12              // w3<- B
    ubfx    w9, wINST, #8, #4           // w9<- A
    GET_VREG w1, w3                     // w1<- vB
    GET_VREG w0, w9                     // w0<- vA
    .if 0
    cbz     w1, common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 1                // advance rPC, load rINST
                               // optional op; may set condition codes
    and     w0, w0, w1                              // w0<- op, w0-w3 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG w0, w9                // vAA<- w0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 10-13 instructions */

    NAME_END nterp_op_and_int_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_or_int_2addr: /* 0xb6 */
    NAME_START nterp_op_or_int_2addr

    /*
     * Generic 32-bit "/2addr" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = w0 op w1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than w0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.
     *
     * For: add-int/2addr, sub-int/2addr, mul-int/2addr, div-int/2addr,
     *      rem-int/2addr, and-int/2addr, or-int/2addr, xor-int/2addr,
     *      shl-int/2addr, shr-int/2addr, ushr-int/2addr, add-float/2addr,
     *      sub-float/2addr, mul-float/2addr, div-float/2addr, rem-float/2addr
     */
    /* binop/2addr vA, vB */
    lsr     w3, wINST, #12              // w3<- B
    ubfx    w9, wINST, #8, #4           // w9<- A
    GET_VREG w1, w3                     // w1<- vB
    GET_VREG w0, w9                     // w0<- vA
    .if 0
    cbz     w1, common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 1                // advance rPC, load rINST
                               // optional op; may set condition codes
    orr     w0, w0, w1                              // w0<- op, w0-w3 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG w0, w9                // vAA<- w0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 10-13 instructions */

    NAME_END nterp_op_or_int_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_xor_int_2addr: /* 0xb7 */
    NAME_START nterp_op_xor_int_2addr

    /*
     * Generic 32-bit "/2addr" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = w0 op w1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than w0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.
     *
     * For: add-int/2addr, sub-int/2addr, mul-int/2addr, div-int/2addr,
     *      rem-int/2addr, and-int/2addr, or-int/2addr, xor-int/2addr,
     *      shl-int/2addr, shr-int/2addr, ushr-int/2addr, add-float/2addr,
     *      sub-float/2addr, mul-float/2addr, div-float/2addr, rem-float/2addr
     */
    /* binop/2addr vA, vB */
    lsr     w3, wINST, #12              // w3<- B
    ubfx    w9, wINST, #8, #4           // w9<- A
    GET_VREG w1, w3                     // w1<- vB
    GET_VREG w0, w9                     // w0<- vA
    .if 0
    cbz     w1, common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 1                // advance rPC, load rINST
                               // optional op; may set condition codes
    eor     w0, w0, w1                              // w0<- op, w0-w3 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG w0, w9                // vAA<- w0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 10-13 instructions */

    NAME_END nterp_op_xor_int_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_shl_int_2addr: /* 0xb8 */
    NAME_START nterp_op_shl_int_2addr

    /*
     * Generic 32-bit "/2addr" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = w0 op w1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than w0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.
     *
     * For: add-int/2addr, sub-int/2addr, mul-int/2addr, div-int/2addr,
     *      rem-int/2addr, and-int/2addr, or-int/2addr, xor-int/2addr,
     *      shl-int/2addr, shr-int/2addr, ushr-int/2addr, add-float/2addr,
     *      sub-float/2addr, mul-float/2addr, div-float/2addr, rem-float/2addr
     */
    /* binop/2addr vA, vB */
    lsr     w3, wINST, #12              // w3<- B
    ubfx    w9, wINST, #8, #4           // w9<- A
    GET_VREG w1, w3                     // w1<- vB
    GET_VREG w0, w9                     // w0<- vA
    .if 0
    cbz     w1, common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 1                // advance rPC, load rINST
                               // optional op; may set condition codes
    lsl     w0, w0, w1                              // w0<- op, w0-w3 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG w0, w9                // vAA<- w0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 10-13 instructions */

    NAME_END nterp_op_shl_int_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_shr_int_2addr: /* 0xb9 */
    NAME_START nterp_op_shr_int_2addr

    /*
     * Generic 32-bit "/2addr" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = w0 op w1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than w0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.
     *
     * For: add-int/2addr, sub-int/2addr, mul-int/2addr, div-int/2addr,
     *      rem-int/2addr, and-int/2addr, or-int/2addr, xor-int/2addr,
     *      shl-int/2addr, shr-int/2addr, ushr-int/2addr, add-float/2addr,
     *      sub-float/2addr, mul-float/2addr, div-float/2addr, rem-float/2addr
     */
    /* binop/2addr vA, vB */
    lsr     w3, wINST, #12              // w3<- B
    ubfx    w9, wINST, #8, #4           // w9<- A
    GET_VREG w1, w3                     // w1<- vB
    GET_VREG w0, w9                     // w0<- vA
    .if 0
    cbz     w1, common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 1                // advance rPC, load rINST
                               // optional op; may set condition codes
    asr     w0, w0, w1                              // w0<- op, w0-w3 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG w0, w9                // vAA<- w0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 10-13 instructions */

    NAME_END nterp_op_shr_int_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_ushr_int_2addr: /* 0xba */
    NAME_START nterp_op_ushr_int_2addr

    /*
     * Generic 32-bit "/2addr" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = w0 op w1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than w0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.
     *
     * For: add-int/2addr, sub-int/2addr, mul-int/2addr, div-int/2addr,
     *      rem-int/2addr, and-int/2addr, or-int/2addr, xor-int/2addr,
     *      shl-int/2addr, shr-int/2addr, ushr-int/2addr, add-float/2addr,
     *      sub-float/2addr, mul-float/2addr, div-float/2addr, rem-float/2addr
     */
    /* binop/2addr vA, vB */
    lsr     w3, wINST, #12              // w3<- B
    ubfx    w9, wINST, #8, #4           // w9<- A
    GET_VREG w1, w3                     // w1<- vB
    GET_VREG w0, w9                     // w0<- vA
    .if 0
    cbz     w1, common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 1                // advance rPC, load rINST
                               // optional op; may set condition codes
    lsr     w0, w0, w1                              // w0<- op, w0-w3 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG w0, w9                // vAA<- w0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 10-13 instructions */

    NAME_END nterp_op_ushr_int_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_add_long_2addr: /* 0xbb */
    NAME_START nterp_op_add_long_2addr

    /*
     * Generic 64-bit "/2addr" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "x0 = x0 op x1".
     * This must not be a function call, as we keep w2 live across it.
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.
     *
     * For: add-long/2addr, sub-long/2addr, mul-long/2addr, div-long/2addr,
     *      and-long/2addr, or-long/2addr, xor-long/2addr,
     *      shl-long/2addr, shr-long/2addr, ushr-long/2addr, add-double/2addr,
     *      sub-double/2addr, mul-double/2addr, div-double/2addr, rem-double/2addr
     */
    /* binop/2addr vA, vB */
    lsr     w1, wINST, #12              // w1<- B
    ubfx    w2, wINST, #8, #4           // w2<- A
    GET_VREG_WIDE x1, w1               // x1<- vB
    GET_VREG_WIDE x0, w2               // x0<- vA
    .if 0
    cbz     x1, common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 1                // advance rPC, load rINST
    
    add     x0, x0, x1                              // result<- op
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG_WIDE x0, w2               // vAA<- result
    GOTO_OPCODE ip                      // jump to next instruction
    /* 10-13 instructions */

    NAME_END nterp_op_add_long_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sub_long_2addr: /* 0xbc */
    NAME_START nterp_op_sub_long_2addr

    /*
     * Generic 64-bit "/2addr" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "x0 = x0 op x1".
     * This must not be a function call, as we keep w2 live across it.
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.
     *
     * For: add-long/2addr, sub-long/2addr, mul-long/2addr, div-long/2addr,
     *      and-long/2addr, or-long/2addr, xor-long/2addr,
     *      shl-long/2addr, shr-long/2addr, ushr-long/2addr, add-double/2addr,
     *      sub-double/2addr, mul-double/2addr, div-double/2addr, rem-double/2addr
     */
    /* binop/2addr vA, vB */
    lsr     w1, wINST, #12              // w1<- B
    ubfx    w2, wINST, #8, #4           // w2<- A
    GET_VREG_WIDE x1, w1               // x1<- vB
    GET_VREG_WIDE x0, w2               // x0<- vA
    .if 0
    cbz     x1, common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 1                // advance rPC, load rINST
    
    sub     x0, x0, x1                              // result<- op
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG_WIDE x0, w2               // vAA<- result
    GOTO_OPCODE ip                      // jump to next instruction
    /* 10-13 instructions */

    NAME_END nterp_op_sub_long_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_mul_long_2addr: /* 0xbd */
    NAME_START nterp_op_mul_long_2addr

    /*
     * Generic 64-bit "/2addr" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "x0 = x0 op x1".
     * This must not be a function call, as we keep w2 live across it.
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.
     *
     * For: add-long/2addr, sub-long/2addr, mul-long/2addr, div-long/2addr,
     *      and-long/2addr, or-long/2addr, xor-long/2addr,
     *      shl-long/2addr, shr-long/2addr, ushr-long/2addr, add-double/2addr,
     *      sub-double/2addr, mul-double/2addr, div-double/2addr, rem-double/2addr
     */
    /* binop/2addr vA, vB */
    lsr     w1, wINST, #12              // w1<- B
    ubfx    w2, wINST, #8, #4           // w2<- A
    GET_VREG_WIDE x1, w1               // x1<- vB
    GET_VREG_WIDE x0, w2               // x0<- vA
    .if 0
    cbz     x1, common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 1                // advance rPC, load rINST
    
    mul     x0, x0, x1                              // result<- op
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG_WIDE x0, w2               // vAA<- result
    GOTO_OPCODE ip                      // jump to next instruction
    /* 10-13 instructions */

    NAME_END nterp_op_mul_long_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_div_long_2addr: /* 0xbe */
    NAME_START nterp_op_div_long_2addr

    /*
     * Generic 64-bit "/2addr" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "x0 = x0 op x1".
     * This must not be a function call, as we keep w2 live across it.
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.
     *
     * For: add-long/2addr, sub-long/2addr, mul-long/2addr, div-long/2addr,
     *      and-long/2addr, or-long/2addr, xor-long/2addr,
     *      shl-long/2addr, shr-long/2addr, ushr-long/2addr, add-double/2addr,
     *      sub-double/2addr, mul-double/2addr, div-double/2addr, rem-double/2addr
     */
    /* binop/2addr vA, vB */
    lsr     w1, wINST, #12              // w1<- B
    ubfx    w2, wINST, #8, #4           // w2<- A
    GET_VREG_WIDE x1, w1               // x1<- vB
    GET_VREG_WIDE x0, w2               // x0<- vA
    .if 1
    cbz     x1, common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 1                // advance rPC, load rINST
    
    sdiv     x0, x0, x1                              // result<- op
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG_WIDE x0, w2               // vAA<- result
    GOTO_OPCODE ip                      // jump to next instruction
    /* 10-13 instructions */

    NAME_END nterp_op_div_long_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_rem_long_2addr: /* 0xbf */
    NAME_START nterp_op_rem_long_2addr

    /*
     * Generic 64-bit "/2addr" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "x0 = x0 op x1".
     * This must not be a function call, as we keep w2 live across it.
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.
     *
     * For: add-long/2addr, sub-long/2addr, mul-long/2addr, div-long/2addr,
     *      and-long/2addr, or-long/2addr, xor-long/2addr,
     *      shl-long/2addr, shr-long/2addr, ushr-long/2addr, add-double/2addr,
     *      sub-double/2addr, mul-double/2addr, div-double/2addr, rem-double/2addr
     */
    /* binop/2addr vA, vB */
    lsr     w1, wINST, #12              // w1<- B
    ubfx    w2, wINST, #8, #4           // w2<- A
    GET_VREG_WIDE x1, w1               // x1<- vB
    GET_VREG_WIDE x0, w2               // x0<- vA
    .if 1
    cbz     x1, common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 1                // advance rPC, load rINST
    sdiv x3, x0, x1
    msub x0, x3, x1, x0                              // result<- op
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG_WIDE x0, w2               // vAA<- result
    GOTO_OPCODE ip                      // jump to next instruction
    /* 10-13 instructions */

    NAME_END nterp_op_rem_long_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_and_long_2addr: /* 0xc0 */
    NAME_START nterp_op_and_long_2addr

    /*
     * Generic 64-bit "/2addr" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "x0 = x0 op x1".
     * This must not be a function call, as we keep w2 live across it.
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.
     *
     * For: add-long/2addr, sub-long/2addr, mul-long/2addr, div-long/2addr,
     *      and-long/2addr, or-long/2addr, xor-long/2addr,
     *      shl-long/2addr, shr-long/2addr, ushr-long/2addr, add-double/2addr,
     *      sub-double/2addr, mul-double/2addr, div-double/2addr, rem-double/2addr
     */
    /* binop/2addr vA, vB */
    lsr     w1, wINST, #12              // w1<- B
    ubfx    w2, wINST, #8, #4           // w2<- A
    GET_VREG_WIDE x1, w1               // x1<- vB
    GET_VREG_WIDE x0, w2               // x0<- vA
    .if 0
    cbz     x1, common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 1                // advance rPC, load rINST
    
    and     x0, x0, x1                              // result<- op
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG_WIDE x0, w2               // vAA<- result
    GOTO_OPCODE ip                      // jump to next instruction
    /* 10-13 instructions */

    NAME_END nterp_op_and_long_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_or_long_2addr: /* 0xc1 */
    NAME_START nterp_op_or_long_2addr

    /*
     * Generic 64-bit "/2addr" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "x0 = x0 op x1".
     * This must not be a function call, as we keep w2 live across it.
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.
     *
     * For: add-long/2addr, sub-long/2addr, mul-long/2addr, div-long/2addr,
     *      and-long/2addr, or-long/2addr, xor-long/2addr,
     *      shl-long/2addr, shr-long/2addr, ushr-long/2addr, add-double/2addr,
     *      sub-double/2addr, mul-double/2addr, div-double/2addr, rem-double/2addr
     */
    /* binop/2addr vA, vB */
    lsr     w1, wINST, #12              // w1<- B
    ubfx    w2, wINST, #8, #4           // w2<- A
    GET_VREG_WIDE x1, w1               // x1<- vB
    GET_VREG_WIDE x0, w2               // x0<- vA
    .if 0
    cbz     x1, common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 1                // advance rPC, load rINST
    
    orr     x0, x0, x1                              // result<- op
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG_WIDE x0, w2               // vAA<- result
    GOTO_OPCODE ip                      // jump to next instruction
    /* 10-13 instructions */

    NAME_END nterp_op_or_long_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_xor_long_2addr: /* 0xc2 */
    NAME_START nterp_op_xor_long_2addr

    /*
     * Generic 64-bit "/2addr" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "x0 = x0 op x1".
     * This must not be a function call, as we keep w2 live across it.
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.
     *
     * For: add-long/2addr, sub-long/2addr, mul-long/2addr, div-long/2addr,
     *      and-long/2addr, or-long/2addr, xor-long/2addr,
     *      shl-long/2addr, shr-long/2addr, ushr-long/2addr, add-double/2addr,
     *      sub-double/2addr, mul-double/2addr, div-double/2addr, rem-double/2addr
     */
    /* binop/2addr vA, vB */
    lsr     w1, wINST, #12              // w1<- B
    ubfx    w2, wINST, #8, #4           // w2<- A
    GET_VREG_WIDE x1, w1               // x1<- vB
    GET_VREG_WIDE x0, w2               // x0<- vA
    .if 0
    cbz     x1, common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 1                // advance rPC, load rINST
    
    eor     x0, x0, x1                              // result<- op
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG_WIDE x0, w2               // vAA<- result
    GOTO_OPCODE ip                      // jump to next instruction
    /* 10-13 instructions */

    NAME_END nterp_op_xor_long_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_shl_long_2addr: /* 0xc3 */
    NAME_START nterp_op_shl_long_2addr

    /*
     * Generic 64-bit shift operation.
     */
    /* binop/2addr vA, vB */
    lsr     w1, wINST, #12              // w1<- B
    ubfx    w2, wINST, #8, #4           // w2<- A
    GET_VREG w1, w1                     // x1<- vB
    GET_VREG_WIDE x0, w2                // x0<- vA
    FETCH_ADVANCE_INST 1                // advance rPC, load rINST
    lsl x0, x0, x1                  // Do the shift. Only low 6 bits of x1 are used.
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG_WIDE x0, w2               // vAA<- result
    GOTO_OPCODE ip                      // jump to next instruction
    /* 10-13 instructions */

    NAME_END nterp_op_shl_long_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_shr_long_2addr: /* 0xc4 */
    NAME_START nterp_op_shr_long_2addr

    /*
     * Generic 64-bit shift operation.
     */
    /* binop/2addr vA, vB */
    lsr     w1, wINST, #12              // w1<- B
    ubfx    w2, wINST, #8, #4           // w2<- A
    GET_VREG w1, w1                     // x1<- vB
    GET_VREG_WIDE x0, w2                // x0<- vA
    FETCH_ADVANCE_INST 1                // advance rPC, load rINST
    asr x0, x0, x1                  // Do the shift. Only low 6 bits of x1 are used.
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG_WIDE x0, w2               // vAA<- result
    GOTO_OPCODE ip                      // jump to next instruction
    /* 10-13 instructions */

    NAME_END nterp_op_shr_long_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_ushr_long_2addr: /* 0xc5 */
    NAME_START nterp_op_ushr_long_2addr

    /*
     * Generic 64-bit shift operation.
     */
    /* binop/2addr vA, vB */
    lsr     w1, wINST, #12              // w1<- B
    ubfx    w2, wINST, #8, #4           // w2<- A
    GET_VREG w1, w1                     // x1<- vB
    GET_VREG_WIDE x0, w2                // x0<- vA
    FETCH_ADVANCE_INST 1                // advance rPC, load rINST
    lsr x0, x0, x1                  // Do the shift. Only low 6 bits of x1 are used.
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG_WIDE x0, w2               // vAA<- result
    GOTO_OPCODE ip                      // jump to next instruction
    /* 10-13 instructions */

    NAME_END nterp_op_ushr_long_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_add_float_2addr: /* 0xc6 */
    NAME_START nterp_op_add_float_2addr

    /*
     * Generic 32-bit floating point "/2addr" binary operation.  Provide
     * an "instr" line that specifies an instruction that performs
     * "s2 = s0 op s1".
     *
     * For: add-float/2addr, sub-float/2addr, mul-float/2addr, div-float/2addr
     */
    /* binop/2addr vA, vB */
    lsr     w3, wINST, #12              // w3<- B
    ubfx    w9, wINST, #8, #4           // w9<- A
    GET_VREG s1, w3
    GET_VREG s0, w9
    fadd   s2, s0, s1                              // s2<- op
    FETCH_ADVANCE_INST 1                // advance rPC, load rINST
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG_FLOAT s2, w9
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_add_float_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sub_float_2addr: /* 0xc7 */
    NAME_START nterp_op_sub_float_2addr

    /*
     * Generic 32-bit floating point "/2addr" binary operation.  Provide
     * an "instr" line that specifies an instruction that performs
     * "s2 = s0 op s1".
     *
     * For: add-float/2addr, sub-float/2addr, mul-float/2addr, div-float/2addr
     */
    /* binop/2addr vA, vB */
    lsr     w3, wINST, #12              // w3<- B
    ubfx    w9, wINST, #8, #4           // w9<- A
    GET_VREG s1, w3
    GET_VREG s0, w9
    fsub   s2, s0, s1                              // s2<- op
    FETCH_ADVANCE_INST 1                // advance rPC, load rINST
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG_FLOAT s2, w9
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_sub_float_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_mul_float_2addr: /* 0xc8 */
    NAME_START nterp_op_mul_float_2addr

    /*
     * Generic 32-bit floating point "/2addr" binary operation.  Provide
     * an "instr" line that specifies an instruction that performs
     * "s2 = s0 op s1".
     *
     * For: add-float/2addr, sub-float/2addr, mul-float/2addr, div-float/2addr
     */
    /* binop/2addr vA, vB */
    lsr     w3, wINST, #12              // w3<- B
    ubfx    w9, wINST, #8, #4           // w9<- A
    GET_VREG s1, w3
    GET_VREG s0, w9
    fmul   s2, s0, s1                              // s2<- op
    FETCH_ADVANCE_INST 1                // advance rPC, load rINST
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG_FLOAT s2, w9
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_mul_float_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_div_float_2addr: /* 0xc9 */
    NAME_START nterp_op_div_float_2addr

    /*
     * Generic 32-bit floating point "/2addr" binary operation.  Provide
     * an "instr" line that specifies an instruction that performs
     * "s2 = s0 op s1".
     *
     * For: add-float/2addr, sub-float/2addr, mul-float/2addr, div-float/2addr
     */
    /* binop/2addr vA, vB */
    lsr     w3, wINST, #12              // w3<- B
    ubfx    w9, wINST, #8, #4           // w9<- A
    GET_VREG s1, w3
    GET_VREG s0, w9
    fdiv   s2, s0, s1                              // s2<- op
    FETCH_ADVANCE_INST 1                // advance rPC, load rINST
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG_FLOAT s2, w9
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_div_float_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_rem_float_2addr: /* 0xca */
    NAME_START nterp_op_rem_float_2addr

    /* rem vA, vB */
    lsr     w3, wINST, #12              // w3<- B
    ubfx    w9, wINST, #8, #4           // w9<- A
    GET_VREG s1, w3
    GET_VREG s0, w9
    bl  fmodf
    ubfx    w9, wINST, #8, #4           // w9<- A
    FETCH_ADVANCE_INST 1                // advance rPC, load rINST
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG_FLOAT s0, w9
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_rem_float_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_add_double_2addr: /* 0xcb */
    NAME_START nterp_op_add_double_2addr

    /*
     * Generic 64-bit floating point "/2addr" binary operation.
     */
    /* binop/2addr vA, vB */
    lsr     w1, wINST, #12              // w1<- B
    ubfx    w2, wINST, #8, #4           // w2<- A
    GET_VREG_DOUBLE d1, w1             // x1<- vB
    GET_VREG_DOUBLE d0, w2             // x0<- vA
    FETCH_ADVANCE_INST 1                // advance rPC, load rINST
    fadd     d0, d0, d1                              // result<- op
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG_DOUBLE d0, w2             // vAA<- result
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_add_double_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_sub_double_2addr: /* 0xcc */
    NAME_START nterp_op_sub_double_2addr

    /*
     * Generic 64-bit floating point "/2addr" binary operation.
     */
    /* binop/2addr vA, vB */
    lsr     w1, wINST, #12              // w1<- B
    ubfx    w2, wINST, #8, #4           // w2<- A
    GET_VREG_DOUBLE d1, w1             // x1<- vB
    GET_VREG_DOUBLE d0, w2             // x0<- vA
    FETCH_ADVANCE_INST 1                // advance rPC, load rINST
    fsub     d0, d0, d1                              // result<- op
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG_DOUBLE d0, w2             // vAA<- result
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_sub_double_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_mul_double_2addr: /* 0xcd */
    NAME_START nterp_op_mul_double_2addr

    /*
     * Generic 64-bit floating point "/2addr" binary operation.
     */
    /* binop/2addr vA, vB */
    lsr     w1, wINST, #12              // w1<- B
    ubfx    w2, wINST, #8, #4           // w2<- A
    GET_VREG_DOUBLE d1, w1             // x1<- vB
    GET_VREG_DOUBLE d0, w2             // x0<- vA
    FETCH_ADVANCE_INST 1                // advance rPC, load rINST
    fmul     d0, d0, d1                              // result<- op
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG_DOUBLE d0, w2             // vAA<- result
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_mul_double_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_div_double_2addr: /* 0xce */
    NAME_START nterp_op_div_double_2addr

    /*
     * Generic 64-bit floating point "/2addr" binary operation.
     */
    /* binop/2addr vA, vB */
    lsr     w1, wINST, #12              // w1<- B
    ubfx    w2, wINST, #8, #4           // w2<- A
    GET_VREG_DOUBLE d1, w1             // x1<- vB
    GET_VREG_DOUBLE d0, w2             // x0<- vA
    FETCH_ADVANCE_INST 1                // advance rPC, load rINST
    fdiv     d0, d0, d1                              // result<- op
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG_DOUBLE d0, w2             // vAA<- result
    GOTO_OPCODE ip                      // jump to next instruction

    NAME_END nterp_op_div_double_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_rem_double_2addr: /* 0xcf */
    NAME_START nterp_op_rem_double_2addr

    /* rem vA, vB */
    lsr     w1, wINST, #12              // w1<- B
    ubfx    w2, wINST, #8, #4           // w2<- A
    GET_VREG_DOUBLE d1, w1              // d1<- vB
    GET_VREG_DOUBLE d0, w2              // d0<- vA
    bl fmod
    ubfx    w2, wINST, #8, #4           // w2<- A (need to reload - killed across call)
    FETCH_ADVANCE_INST 1                // advance rPC, load rINST
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG_WIDE d0, w2                // vAA<- result
    GOTO_OPCODE ip                      // jump to next instruction
    /* 10-13 instructions */

    NAME_END nterp_op_rem_double_2addr

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_add_int_lit16: /* 0xd0 */
    NAME_START nterp_op_add_int_lit16

    /*
     * Generic 32-bit "lit16" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = w0 op w1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than w0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.
     *
     * For: add-int/lit16, rsub-int, mul-int/lit16, div-int/lit16,
     *      rem-int/lit16, and-int/lit16, or-int/lit16, xor-int/lit16
     */
    /* binop/lit16 vA, vB, #+CCCC */
    FETCH_S w1, 1                       // w1<- ssssCCCC (sign-extended)
    lsr     w2, wINST, #12              // w2<- B
    ubfx    w9, wINST, #8, #4           // w9<- A
    GET_VREG w0, w2                     // w0<- vB
    .if 0
    cbz     w1, common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
    
    add     w0, w0, w1                              // w0<- op, w0-w3 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG w0, w9                // vAA<- w0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 10-13 instructions */

    NAME_END nterp_op_add_int_lit16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_rsub_int: /* 0xd1 */
    NAME_START nterp_op_rsub_int

/* this op is "rsub-int", but can be thought of as "rsub-int/lit16" */
    /*
     * Generic 32-bit "lit16" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = w0 op w1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than w0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.
     *
     * For: add-int/lit16, rsub-int, mul-int/lit16, div-int/lit16,
     *      rem-int/lit16, and-int/lit16, or-int/lit16, xor-int/lit16
     */
    /* binop/lit16 vA, vB, #+CCCC */
    FETCH_S w1, 1                       // w1<- ssssCCCC (sign-extended)
    lsr     w2, wINST, #12              // w2<- B
    ubfx    w9, wINST, #8, #4           // w9<- A
    GET_VREG w0, w2                     // w0<- vB
    .if 0
    cbz     w1, common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
    
    sub     w0, w1, w0                              // w0<- op, w0-w3 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG w0, w9                // vAA<- w0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 10-13 instructions */

    NAME_END nterp_op_rsub_int

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_mul_int_lit16: /* 0xd2 */
    NAME_START nterp_op_mul_int_lit16

/* must be "mul w0, w1, w0" -- "w0, w0, w1" is illegal */
    /*
     * Generic 32-bit "lit16" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = w0 op w1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than w0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.
     *
     * For: add-int/lit16, rsub-int, mul-int/lit16, div-int/lit16,
     *      rem-int/lit16, and-int/lit16, or-int/lit16, xor-int/lit16
     */
    /* binop/lit16 vA, vB, #+CCCC */
    FETCH_S w1, 1                       // w1<- ssssCCCC (sign-extended)
    lsr     w2, wINST, #12              // w2<- B
    ubfx    w9, wINST, #8, #4           // w9<- A
    GET_VREG w0, w2                     // w0<- vB
    .if 0
    cbz     w1, common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
    
    mul     w0, w1, w0                              // w0<- op, w0-w3 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG w0, w9                // vAA<- w0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 10-13 instructions */

    NAME_END nterp_op_mul_int_lit16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_div_int_lit16: /* 0xd3 */
    NAME_START nterp_op_div_int_lit16

    /*
     * Generic 32-bit "lit16" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = w0 op w1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than w0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.
     *
     * For: add-int/lit16, rsub-int, mul-int/lit16, div-int/lit16,
     *      rem-int/lit16, and-int/lit16, or-int/lit16, xor-int/lit16
     */
    /* binop/lit16 vA, vB, #+CCCC */
    FETCH_S w1, 1                       // w1<- ssssCCCC (sign-extended)
    lsr     w2, wINST, #12              // w2<- B
    ubfx    w9, wINST, #8, #4           // w9<- A
    GET_VREG w0, w2                     // w0<- vB
    .if 1
    cbz     w1, common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
    
    sdiv w0, w0, w1                              // w0<- op, w0-w3 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG w0, w9                // vAA<- w0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 10-13 instructions */

    NAME_END nterp_op_div_int_lit16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_rem_int_lit16: /* 0xd4 */
    NAME_START nterp_op_rem_int_lit16

    /*
     * Generic 32-bit "lit16" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = w0 op w1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than w0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.
     *
     * For: add-int/lit16, rsub-int, mul-int/lit16, div-int/lit16,
     *      rem-int/lit16, and-int/lit16, or-int/lit16, xor-int/lit16
     */
    /* binop/lit16 vA, vB, #+CCCC */
    FETCH_S w1, 1                       // w1<- ssssCCCC (sign-extended)
    lsr     w2, wINST, #12              // w2<- B
    ubfx    w9, wINST, #8, #4           // w9<- A
    GET_VREG w0, w2                     // w0<- vB
    .if 1
    cbz     w1, common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
    sdiv w3, w0, w1
    msub w0, w3, w1, w0                              // w0<- op, w0-w3 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG w0, w9                // vAA<- w0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 10-13 instructions */

    NAME_END nterp_op_rem_int_lit16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_and_int_lit16: /* 0xd5 */
    NAME_START nterp_op_and_int_lit16

    /*
     * Generic 32-bit "lit16" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = w0 op w1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than w0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.
     *
     * For: add-int/lit16, rsub-int, mul-int/lit16, div-int/lit16,
     *      rem-int/lit16, and-int/lit16, or-int/lit16, xor-int/lit16
     */
    /* binop/lit16 vA, vB, #+CCCC */
    FETCH_S w1, 1                       // w1<- ssssCCCC (sign-extended)
    lsr     w2, wINST, #12              // w2<- B
    ubfx    w9, wINST, #8, #4           // w9<- A
    GET_VREG w0, w2                     // w0<- vB
    .if 0
    cbz     w1, common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
    
    and     w0, w0, w1                              // w0<- op, w0-w3 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG w0, w9                // vAA<- w0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 10-13 instructions */

    NAME_END nterp_op_and_int_lit16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_or_int_lit16: /* 0xd6 */
    NAME_START nterp_op_or_int_lit16

    /*
     * Generic 32-bit "lit16" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = w0 op w1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than w0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.
     *
     * For: add-int/lit16, rsub-int, mul-int/lit16, div-int/lit16,
     *      rem-int/lit16, and-int/lit16, or-int/lit16, xor-int/lit16
     */
    /* binop/lit16 vA, vB, #+CCCC */
    FETCH_S w1, 1                       // w1<- ssssCCCC (sign-extended)
    lsr     w2, wINST, #12              // w2<- B
    ubfx    w9, wINST, #8, #4           // w9<- A
    GET_VREG w0, w2                     // w0<- vB
    .if 0
    cbz     w1, common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
    
    orr     w0, w0, w1                              // w0<- op, w0-w3 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG w0, w9                // vAA<- w0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 10-13 instructions */

    NAME_END nterp_op_or_int_lit16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_xor_int_lit16: /* 0xd7 */
    NAME_START nterp_op_xor_int_lit16

    /*
     * Generic 32-bit "lit16" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = w0 op w1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than w0, you can override "result".)
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.
     *
     * For: add-int/lit16, rsub-int, mul-int/lit16, div-int/lit16,
     *      rem-int/lit16, and-int/lit16, or-int/lit16, xor-int/lit16
     */
    /* binop/lit16 vA, vB, #+CCCC */
    FETCH_S w1, 1                       // w1<- ssssCCCC (sign-extended)
    lsr     w2, wINST, #12              // w2<- B
    ubfx    w9, wINST, #8, #4           // w9<- A
    GET_VREG w0, w2                     // w0<- vB
    .if 0
    cbz     w1, common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
    
    eor     w0, w0, w1                              // w0<- op, w0-w3 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG w0, w9                // vAA<- w0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 10-13 instructions */

    NAME_END nterp_op_xor_int_lit16

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_add_int_lit8: /* 0xd8 */
    NAME_START nterp_op_add_int_lit8

    /*
     * Generic 32-bit "lit8" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = w0 op w1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than w0, you can override "result".)
     *
     * You can override "extract" if the extraction of the literal value
     * from w3 to w1 is not the default "asr w1, w3, #8". The extraction
     * can be omitted completely if the shift is embedded in "instr".
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.
     *
     * For: add-int/lit8, rsub-int/lit8, mul-int/lit8, div-int/lit8,
     *      rem-int/lit8, and-int/lit8, or-int/lit8, xor-int/lit8,
     *      shl-int/lit8, shr-int/lit8, ushr-int/lit8
     */
    /* binop/lit8 vAA, vBB, #+CC */
    FETCH_S w3, 1                       // w3<- ssssCCBB (sign-extended for CC)
    lsr     w9, wINST, #8               // w9<- AA
    and     w2, w3, #255                // w2<- BB
    GET_VREG w0, w2                     // w0<- vBB
                                // optional; typically w1<- ssssssCC (sign extended)
    .if 0
    cbz     w1, common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
                               // optional op; may set condition codes
    add     w0, w0, w3, asr #8                              // w0<- op, w0-w3 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG w0, w9                // vAA<- w0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 10-12 instructions */

    NAME_END nterp_op_add_int_lit8

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_rsub_int_lit8: /* 0xd9 */
    NAME_START nterp_op_rsub_int_lit8

    /*
     * Generic 32-bit "lit8" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = w0 op w1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than w0, you can override "result".)
     *
     * You can override "extract" if the extraction of the literal value
     * from w3 to w1 is not the default "asr w1, w3, #8". The extraction
     * can be omitted completely if the shift is embedded in "instr".
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.
     *
     * For: add-int/lit8, rsub-int/lit8, mul-int/lit8, div-int/lit8,
     *      rem-int/lit8, and-int/lit8, or-int/lit8, xor-int/lit8,
     *      shl-int/lit8, shr-int/lit8, ushr-int/lit8
     */
    /* binop/lit8 vAA, vBB, #+CC */
    FETCH_S w3, 1                       // w3<- ssssCCBB (sign-extended for CC)
    lsr     w9, wINST, #8               // w9<- AA
    and     w2, w3, #255                // w2<- BB
    GET_VREG w0, w2                     // w0<- vBB
    asr     w1, w3, #8                            // optional; typically w1<- ssssssCC (sign extended)
    .if 0
    cbz     w1, common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
                               // optional op; may set condition codes
    sub     w0, w1, w0                              // w0<- op, w0-w3 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG w0, w9                // vAA<- w0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 10-12 instructions */

    NAME_END nterp_op_rsub_int_lit8

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_mul_int_lit8: /* 0xda */
    NAME_START nterp_op_mul_int_lit8

/* must be "mul w0, w1, w0" -- "w0, w0, w1" is illegal */
    /*
     * Generic 32-bit "lit8" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = w0 op w1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than w0, you can override "result".)
     *
     * You can override "extract" if the extraction of the literal value
     * from w3 to w1 is not the default "asr w1, w3, #8". The extraction
     * can be omitted completely if the shift is embedded in "instr".
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.
     *
     * For: add-int/lit8, rsub-int/lit8, mul-int/lit8, div-int/lit8,
     *      rem-int/lit8, and-int/lit8, or-int/lit8, xor-int/lit8,
     *      shl-int/lit8, shr-int/lit8, ushr-int/lit8
     */
    /* binop/lit8 vAA, vBB, #+CC */
    FETCH_S w3, 1                       // w3<- ssssCCBB (sign-extended for CC)
    lsr     w9, wINST, #8               // w9<- AA
    and     w2, w3, #255                // w2<- BB
    GET_VREG w0, w2                     // w0<- vBB
    asr     w1, w3, #8                            // optional; typically w1<- ssssssCC (sign extended)
    .if 0
    cbz     w1, common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
                               // optional op; may set condition codes
    mul     w0, w1, w0                              // w0<- op, w0-w3 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG w0, w9                // vAA<- w0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 10-12 instructions */

    NAME_END nterp_op_mul_int_lit8

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_div_int_lit8: /* 0xdb */
    NAME_START nterp_op_div_int_lit8

    /*
     * Generic 32-bit "lit8" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = w0 op w1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than w0, you can override "result".)
     *
     * You can override "extract" if the extraction of the literal value
     * from w3 to w1 is not the default "asr w1, w3, #8". The extraction
     * can be omitted completely if the shift is embedded in "instr".
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.
     *
     * For: add-int/lit8, rsub-int/lit8, mul-int/lit8, div-int/lit8,
     *      rem-int/lit8, and-int/lit8, or-int/lit8, xor-int/lit8,
     *      shl-int/lit8, shr-int/lit8, ushr-int/lit8
     */
    /* binop/lit8 vAA, vBB, #+CC */
    FETCH_S w3, 1                       // w3<- ssssCCBB (sign-extended for CC)
    lsr     w9, wINST, #8               // w9<- AA
    and     w2, w3, #255                // w2<- BB
    GET_VREG w0, w2                     // w0<- vBB
    asr     w1, w3, #8                            // optional; typically w1<- ssssssCC (sign extended)
    .if 1
    cbz     w1, common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
                               // optional op; may set condition codes
    sdiv     w0, w0, w1                              // w0<- op, w0-w3 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG w0, w9                // vAA<- w0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 10-12 instructions */

    NAME_END nterp_op_div_int_lit8

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_rem_int_lit8: /* 0xdc */
    NAME_START nterp_op_rem_int_lit8

    /*
     * Generic 32-bit "lit8" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = w0 op w1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than w0, you can override "result".)
     *
     * You can override "extract" if the extraction of the literal value
     * from w3 to w1 is not the default "asr w1, w3, #8". The extraction
     * can be omitted completely if the shift is embedded in "instr".
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.
     *
     * For: add-int/lit8, rsub-int/lit8, mul-int/lit8, div-int/lit8,
     *      rem-int/lit8, and-int/lit8, or-int/lit8, xor-int/lit8,
     *      shl-int/lit8, shr-int/lit8, ushr-int/lit8
     */
    /* binop/lit8 vAA, vBB, #+CC */
    FETCH_S w3, 1                       // w3<- ssssCCBB (sign-extended for CC)
    lsr     w9, wINST, #8               // w9<- AA
    and     w2, w3, #255                // w2<- BB
    GET_VREG w0, w2                     // w0<- vBB
    asr     w1, w3, #8                            // optional; typically w1<- ssssssCC (sign extended)
    .if 1
    cbz     w1, common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
    sdiv w3, w0, w1                           // optional op; may set condition codes
    msub w0, w3, w1, w0                              // w0<- op, w0-w3 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG w0, w9                // vAA<- w0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 10-12 instructions */

    NAME_END nterp_op_rem_int_lit8

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_and_int_lit8: /* 0xdd */
    NAME_START nterp_op_and_int_lit8

    /*
     * Generic 32-bit "lit8" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = w0 op w1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than w0, you can override "result".)
     *
     * You can override "extract" if the extraction of the literal value
     * from w3 to w1 is not the default "asr w1, w3, #8". The extraction
     * can be omitted completely if the shift is embedded in "instr".
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.
     *
     * For: add-int/lit8, rsub-int/lit8, mul-int/lit8, div-int/lit8,
     *      rem-int/lit8, and-int/lit8, or-int/lit8, xor-int/lit8,
     *      shl-int/lit8, shr-int/lit8, ushr-int/lit8
     */
    /* binop/lit8 vAA, vBB, #+CC */
    FETCH_S w3, 1                       // w3<- ssssCCBB (sign-extended for CC)
    lsr     w9, wINST, #8               // w9<- AA
    and     w2, w3, #255                // w2<- BB
    GET_VREG w0, w2                     // w0<- vBB
                                // optional; typically w1<- ssssssCC (sign extended)
    .if 0
    cbz     w1, common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
                               // optional op; may set condition codes
    and     w0, w0, w3, asr #8                              // w0<- op, w0-w3 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG w0, w9                // vAA<- w0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 10-12 instructions */

    NAME_END nterp_op_and_int_lit8

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_or_int_lit8: /* 0xde */
    NAME_START nterp_op_or_int_lit8

    /*
     * Generic 32-bit "lit8" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = w0 op w1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than w0, you can override "result".)
     *
     * You can override "extract" if the extraction of the literal value
     * from w3 to w1 is not the default "asr w1, w3, #8". The extraction
     * can be omitted completely if the shift is embedded in "instr".
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.
     *
     * For: add-int/lit8, rsub-int/lit8, mul-int/lit8, div-int/lit8,
     *      rem-int/lit8, and-int/lit8, or-int/lit8, xor-int/lit8,
     *      shl-int/lit8, shr-int/lit8, ushr-int/lit8
     */
    /* binop/lit8 vAA, vBB, #+CC */
    FETCH_S w3, 1                       // w3<- ssssCCBB (sign-extended for CC)
    lsr     w9, wINST, #8               // w9<- AA
    and     w2, w3, #255                // w2<- BB
    GET_VREG w0, w2                     // w0<- vBB
                                // optional; typically w1<- ssssssCC (sign extended)
    .if 0
    cbz     w1, common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
                               // optional op; may set condition codes
    orr     w0, w0, w3, asr #8                              // w0<- op, w0-w3 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG w0, w9                // vAA<- w0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 10-12 instructions */

    NAME_END nterp_op_or_int_lit8

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_xor_int_lit8: /* 0xdf */
    NAME_START nterp_op_xor_int_lit8

    /*
     * Generic 32-bit "lit8" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = w0 op w1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than w0, you can override "result".)
     *
     * You can override "extract" if the extraction of the literal value
     * from w3 to w1 is not the default "asr w1, w3, #8". The extraction
     * can be omitted completely if the shift is embedded in "instr".
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.
     *
     * For: add-int/lit8, rsub-int/lit8, mul-int/lit8, div-int/lit8,
     *      rem-int/lit8, and-int/lit8, or-int/lit8, xor-int/lit8,
     *      shl-int/lit8, shr-int/lit8, ushr-int/lit8
     */
    /* binop/lit8 vAA, vBB, #+CC */
    FETCH_S w3, 1                       // w3<- ssssCCBB (sign-extended for CC)
    lsr     w9, wINST, #8               // w9<- AA
    and     w2, w3, #255                // w2<- BB
    GET_VREG w0, w2                     // w0<- vBB
                                // optional; typically w1<- ssssssCC (sign extended)
    .if 0
    cbz     w1, common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
                               // optional op; may set condition codes
    eor     w0, w0, w3, asr #8                              // w0<- op, w0-w3 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG w0, w9                // vAA<- w0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 10-12 instructions */

    NAME_END nterp_op_xor_int_lit8

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_shl_int_lit8: /* 0xe0 */
    NAME_START nterp_op_shl_int_lit8

    /*
     * Generic 32-bit "lit8" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = w0 op w1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than w0, you can override "result".)
     *
     * You can override "extract" if the extraction of the literal value
     * from w3 to w1 is not the default "asr w1, w3, #8". The extraction
     * can be omitted completely if the shift is embedded in "instr".
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.
     *
     * For: add-int/lit8, rsub-int/lit8, mul-int/lit8, div-int/lit8,
     *      rem-int/lit8, and-int/lit8, or-int/lit8, xor-int/lit8,
     *      shl-int/lit8, shr-int/lit8, ushr-int/lit8
     */
    /* binop/lit8 vAA, vBB, #+CC */
    FETCH_S w3, 1                       // w3<- ssssCCBB (sign-extended for CC)
    lsr     w9, wINST, #8               // w9<- AA
    and     w2, w3, #255                // w2<- BB
    GET_VREG w0, w2                     // w0<- vBB
    ubfx    w1, w3, #8, #5                            // optional; typically w1<- ssssssCC (sign extended)
    .if 0
    cbz     w1, common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
                               // optional op; may set condition codes
    lsl     w0, w0, w1                              // w0<- op, w0-w3 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG w0, w9                // vAA<- w0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 10-12 instructions */

    NAME_END nterp_op_shl_int_lit8

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_shr_int_lit8: /* 0xe1 */
    NAME_START nterp_op_shr_int_lit8

    /*
     * Generic 32-bit "lit8" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = w0 op w1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than w0, you can override "result".)
     *
     * You can override "extract" if the extraction of the literal value
     * from w3 to w1 is not the default "asr w1, w3, #8". The extraction
     * can be omitted completely if the shift is embedded in "instr".
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.
     *
     * For: add-int/lit8, rsub-int/lit8, mul-int/lit8, div-int/lit8,
     *      rem-int/lit8, and-int/lit8, or-int/lit8, xor-int/lit8,
     *      shl-int/lit8, shr-int/lit8, ushr-int/lit8
     */
    /* binop/lit8 vAA, vBB, #+CC */
    FETCH_S w3, 1                       // w3<- ssssCCBB (sign-extended for CC)
    lsr     w9, wINST, #8               // w9<- AA
    and     w2, w3, #255                // w2<- BB
    GET_VREG w0, w2                     // w0<- vBB
    ubfx    w1, w3, #8, #5                            // optional; typically w1<- ssssssCC (sign extended)
    .if 0
    cbz     w1, common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
                               // optional op; may set condition codes
    asr     w0, w0, w1                              // w0<- op, w0-w3 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG w0, w9                // vAA<- w0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 10-12 instructions */

    NAME_END nterp_op_shr_int_lit8

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_ushr_int_lit8: /* 0xe2 */
    NAME_START nterp_op_ushr_int_lit8

    /*
     * Generic 32-bit "lit8" binary operation.  Provide an "instr" line
     * that specifies an instruction that performs "result = w0 op w1".
     * This could be an ARM instruction or a function call.  (If the result
     * comes back in a register other than w0, you can override "result".)
     *
     * You can override "extract" if the extraction of the literal value
     * from w3 to w1 is not the default "asr w1, w3, #8". The extraction
     * can be omitted completely if the shift is embedded in "instr".
     *
     * If "chkzero" is set to 1, we perform a divide-by-zero check on
     * vCC (w1).  Useful for integer division and modulus.
     *
     * For: add-int/lit8, rsub-int/lit8, mul-int/lit8, div-int/lit8,
     *      rem-int/lit8, and-int/lit8, or-int/lit8, xor-int/lit8,
     *      shl-int/lit8, shr-int/lit8, ushr-int/lit8
     */
    /* binop/lit8 vAA, vBB, #+CC */
    FETCH_S w3, 1                       // w3<- ssssCCBB (sign-extended for CC)
    lsr     w9, wINST, #8               // w9<- AA
    and     w2, w3, #255                // w2<- BB
    GET_VREG w0, w2                     // w0<- vBB
    ubfx    w1, w3, #8, #5                            // optional; typically w1<- ssssssCC (sign extended)
    .if 0
    cbz     w1, common_errDivideByZero
    .endif
    FETCH_ADVANCE_INST 2                // advance rPC, load rINST
                               // optional op; may set condition codes
    lsr     w0, w0, w1                              // w0<- op, w0-w3 changed
    GET_INST_OPCODE ip                  // extract opcode from rINST
    SET_VREG w0, w9                // vAA<- w0
    GOTO_OPCODE ip                      // jump to next instruction
    /* 10-12 instructions */

    NAME_END nterp_op_ushr_int_lit8

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_e3: /* 0xe3 */
    NAME_START nterp_op_unused_e3

    brk 42

    NAME_END nterp_op_unused_e3

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_e4: /* 0xe4 */
    NAME_START nterp_op_unused_e4

    brk 42

    NAME_END nterp_op_unused_e4

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_e5: /* 0xe5 */
    NAME_START nterp_op_unused_e5

    brk 42

    NAME_END nterp_op_unused_e5

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_e6: /* 0xe6 */
    NAME_START nterp_op_unused_e6

    brk 42

    NAME_END nterp_op_unused_e6

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_e7: /* 0xe7 */
    NAME_START nterp_op_unused_e7

    brk 42

    NAME_END nterp_op_unused_e7

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_e8: /* 0xe8 */
    NAME_START nterp_op_unused_e8

    brk 42

    NAME_END nterp_op_unused_e8

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_e9: /* 0xe9 */
    NAME_START nterp_op_unused_e9

    brk 42

    NAME_END nterp_op_unused_e9

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_ea: /* 0xea */
    NAME_START nterp_op_unused_ea

    brk 42

    NAME_END nterp_op_unused_ea

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_eb: /* 0xeb */
    NAME_START nterp_op_unused_eb

    brk 42

    NAME_END nterp_op_unused_eb

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_ec: /* 0xec */
    NAME_START nterp_op_unused_ec

    brk 42

    NAME_END nterp_op_unused_ec

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_ed: /* 0xed */
    NAME_START nterp_op_unused_ed

    brk 42

    NAME_END nterp_op_unused_ed

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_ee: /* 0xee */
    NAME_START nterp_op_unused_ee

    brk 42

    NAME_END nterp_op_unused_ee

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_ef: /* 0xef */
    NAME_START nterp_op_unused_ef

    brk 42

    NAME_END nterp_op_unused_ef

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_f0: /* 0xf0 */
    NAME_START nterp_op_unused_f0

    brk 42

    NAME_END nterp_op_unused_f0

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_f1: /* 0xf1 */
    NAME_START nterp_op_unused_f1

    brk 42

    NAME_END nterp_op_unused_f1

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_f2: /* 0xf2 */
    NAME_START nterp_op_unused_f2

    brk 42

    NAME_END nterp_op_unused_f2

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_f3: /* 0xf3 */
    NAME_START nterp_op_unused_f3

    brk 42

    NAME_END nterp_op_unused_f3

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_f4: /* 0xf4 */
    NAME_START nterp_op_unused_f4

    brk 42

    NAME_END nterp_op_unused_f4

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_f5: /* 0xf5 */
    NAME_START nterp_op_unused_f5

    brk 42

    NAME_END nterp_op_unused_f5

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_f6: /* 0xf6 */
    NAME_START nterp_op_unused_f6

    brk 42

    NAME_END nterp_op_unused_f6

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_f7: /* 0xf7 */
    NAME_START nterp_op_unused_f7

    brk 42

    NAME_END nterp_op_unused_f7

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_f8: /* 0xf8 */
    NAME_START nterp_op_unused_f8

    brk 42

    NAME_END nterp_op_unused_f8

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_unused_f9: /* 0xf9 */
    NAME_START nterp_op_unused_f9

    brk 42

    NAME_END nterp_op_unused_f9

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_polymorphic: /* 0xfa */
    NAME_START nterp_op_invoke_polymorphic

   EXPORT_PC
   // No need to fetch the target method.
   // Load the first argument (the 'this' pointer).
   FETCH w1, 2
   and w1, w1, #0xf
   GET_VREG w1, w1
   cbz w1, common_errNullObject    // bail if null
   b NterpCommonInvokePolymorphic

    NAME_END nterp_op_invoke_polymorphic

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_polymorphic_range: /* 0xfb */
    NAME_START nterp_op_invoke_polymorphic_range

   EXPORT_PC
   // No need to fetch the target method.
   // Load the first argument (the 'this' pointer).
   FETCH w1, 2
   GET_VREG w1, w1
   cbz w1, common_errNullObject    // bail if null
   b NterpCommonInvokePolymorphicRange

    NAME_END nterp_op_invoke_polymorphic_range

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_custom: /* 0xfc */
    NAME_START nterp_op_invoke_custom

   EXPORT_PC
   FETCH w0, 1 // call_site index, first argument of runtime call.
   b NterpCommonInvokeCustom

    NAME_END nterp_op_invoke_custom

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_invoke_custom_range: /* 0xfd */
    NAME_START nterp_op_invoke_custom_range

   EXPORT_PC
   FETCH w0, 1 // call_site index, first argument of runtime call.
   b NterpCommonInvokeCustomRange

    NAME_END nterp_op_invoke_custom_range

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_const_method_handle: /* 0xfe */
    NAME_START nterp_op_const_method_handle

   // Fast-path which gets the object from thread-local cache.
   FETCH_FROM_THREAD_CACHE x0, 2f
   cbnz wMR, 3f
1:
   lsr     w1, wINST, #8               // w1<- AA
   .if 0
   FETCH_ADVANCE_INST 3                // advance rPC, load wINST
   .else
   FETCH_ADVANCE_INST 2                // advance rPC, load wINST
   .endif
   GET_INST_OPCODE ip                  // extract opcode from wINST
   SET_VREG_OBJECT w0, w1              // vAA <- value
   GOTO_OPCODE ip                      // jump to next instruction
2:
   EXPORT_PC
   mov x0, xSELF
   ldr x1, [sp]
   mov x2, xPC
   bl nterp_load_object
   b 1b
3:
   bl art_quick_read_barrier_mark_reg00
   b 1b

    NAME_END nterp_op_const_method_handle

/* ------------------------------ */
    .balign MTERP_HANDLER_SIZE
.L_op_const_method_type: /* 0xff */
    NAME_START nterp_op_const_method_type

   // Fast-path which gets the object from thread-local cache.
   FETCH_FROM_THREAD_CACHE x0, 2f
   cbnz wMR, 3f
1:
   lsr     w1, wINST, #8               // w1<- AA
   .if 0
   FETCH_ADVANCE_INST 3                // advance rPC, load wINST
   .else
   FETCH_ADVANCE_INST 2                // advance rPC, load wINST
   .endif
   GET_INST_OPCODE ip                  // extract opcode from wINST
   SET_VREG_OBJECT w0, w1              // vAA <- value
   GOTO_OPCODE ip                      // jump to next instruction
2:
   EXPORT_PC
   mov x0, xSELF
   ldr x1, [sp]
   mov x2, xPC
   bl nterp_load_object
   b 1b
3:
   bl art_quick_read_barrier_mark_reg00
   b 1b

    NAME_END nterp_op_const_method_type

    .balign MTERP_HANDLER_SIZE

    .type artNterpAsmInstructionEnd, #function
    .hidden artNterpAsmInstructionEnd
    .global artNterpAsmInstructionEnd
artNterpAsmInstructionEnd:
    // artNterpAsmInstructionEnd is used as landing pad for exception handling.
    FETCH_INST
    GET_INST_OPCODE ip
    GOTO_OPCODE ip

    NAME_START nterp_op_iget_boolean_helper
   mov     x0, xSELF
   ldr     x1, [sp]
   mov     x2, xPC
   mov     x3, #0
   EXPORT_PC
   bl      nterp_get_instance_field_offset
   tbz     w0, #31, .Lop_iget_boolean_resume
   CLEAR_INSTANCE_VOLATILE_MARKER w0
   lsr     w2, wINST, #12              // w2<- B
   GET_VREG w3, w2                     // w3<- object we're operating on
   ubfx    w2, wINST, #8, #4           // w2<- A
   cbz     w3, common_errNullObject    // object was null
   add     x3, x3, x0
   .if 0
   ldarb x0, [x3]
   SET_VREG_WIDE x0, w2                // fp[A] <- value
   .elseif 0
   ldarb w0, [x3]
   cbnz wMR, .Lop_iget_boolean_read_barrier
   SET_VREG_OBJECT w0, w2              // fp[A] <- value
   .else
   ldarb w0, [x3]
   
   SET_VREG w0, w2                     // fp[A] <- value
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip

    NAME_END nterp_op_iget_boolean_helper
    NAME_START nterp_op_iget_byte_helper
   mov     x0, xSELF
   ldr     x1, [sp]
   mov     x2, xPC
   mov     x3, #0
   EXPORT_PC
   bl      nterp_get_instance_field_offset
   tbz     w0, #31, .Lop_iget_byte_resume
   CLEAR_INSTANCE_VOLATILE_MARKER w0
   lsr     w2, wINST, #12              // w2<- B
   GET_VREG w3, w2                     // w3<- object we're operating on
   ubfx    w2, wINST, #8, #4           // w2<- A
   cbz     w3, common_errNullObject    // object was null
   add     x3, x3, x0
   .if 0
   ldarb x0, [x3]
   SET_VREG_WIDE x0, w2                // fp[A] <- value
   .elseif 0
   ldarb w0, [x3]
   cbnz wMR, .Lop_iget_byte_read_barrier
   SET_VREG_OBJECT w0, w2              // fp[A] <- value
   .else
   ldarb w0, [x3]
   sxtb w0, w0
   SET_VREG w0, w2                     // fp[A] <- value
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip

    NAME_END nterp_op_iget_byte_helper
    NAME_START nterp_op_iget_char_helper
   mov     x0, xSELF
   ldr     x1, [sp]
   mov     x2, xPC
   mov     x3, #0
   EXPORT_PC
   bl      nterp_get_instance_field_offset
   tbz     w0, #31, .Lop_iget_char_resume
   CLEAR_INSTANCE_VOLATILE_MARKER w0
   lsr     w2, wINST, #12              // w2<- B
   GET_VREG w3, w2                     // w3<- object we're operating on
   ubfx    w2, wINST, #8, #4           // w2<- A
   cbz     w3, common_errNullObject    // object was null
   add     x3, x3, x0
   .if 0
   ldarh x0, [x3]
   SET_VREG_WIDE x0, w2                // fp[A] <- value
   .elseif 0
   ldarh w0, [x3]
   cbnz wMR, .Lop_iget_char_read_barrier
   SET_VREG_OBJECT w0, w2              // fp[A] <- value
   .else
   ldarh w0, [x3]
   
   SET_VREG w0, w2                     // fp[A] <- value
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip

    NAME_END nterp_op_iget_char_helper
    NAME_START nterp_op_iget_helper
   mov     x0, xSELF
   ldr     x1, [sp]
   mov     x2, xPC
   mov     x3, #0
   EXPORT_PC
   bl      nterp_get_instance_field_offset
   tbz     w0, #31, .Lop_iget_resume
   CLEAR_INSTANCE_VOLATILE_MARKER w0
   lsr     w2, wINST, #12              // w2<- B
   GET_VREG w3, w2                     // w3<- object we're operating on
   ubfx    w2, wINST, #8, #4           // w2<- A
   cbz     w3, common_errNullObject    // object was null
   add     x3, x3, x0
   .if 0
   ldar x0, [x3]
   SET_VREG_WIDE x0, w2                // fp[A] <- value
   .elseif 0
   ldar w0, [x3]
   cbnz wMR, .Lop_iget_read_barrier
   SET_VREG_OBJECT w0, w2              // fp[A] <- value
   .else
   ldar w0, [x3]
   
   SET_VREG w0, w2                     // fp[A] <- value
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip

    NAME_END nterp_op_iget_helper
    NAME_START nterp_op_iget_object_helper
   mov     x0, xSELF
   ldr     x1, [sp]
   mov     x2, xPC
   mov     x3, #0
   EXPORT_PC
   bl      nterp_get_instance_field_offset
   tbz     w0, #31, .Lop_iget_object_resume
   CLEAR_INSTANCE_VOLATILE_MARKER w0
   lsr     w2, wINST, #12              // w2<- B
   GET_VREG w3, w2                     // w3<- object we're operating on
   ubfx    w2, wINST, #8, #4           // w2<- A
   cbz     w3, common_errNullObject    // object was null
   add     x3, x3, x0
   .if 0
   ldar x0, [x3]
   SET_VREG_WIDE x0, w2                // fp[A] <- value
   .elseif 1
   ldar w0, [x3]
   cbnz wMR, .Lop_iget_object_read_barrier
   SET_VREG_OBJECT w0, w2              // fp[A] <- value
   .else
   ldar w0, [x3]
   
   SET_VREG w0, w2                     // fp[A] <- value
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip

    NAME_END nterp_op_iget_object_helper
    NAME_START nterp_op_iget_short_helper
   mov     x0, xSELF
   ldr     x1, [sp]
   mov     x2, xPC
   mov     x3, #0
   EXPORT_PC
   bl      nterp_get_instance_field_offset
   tbz     w0, #31, .Lop_iget_short_resume
   CLEAR_INSTANCE_VOLATILE_MARKER w0
   lsr     w2, wINST, #12              // w2<- B
   GET_VREG w3, w2                     // w3<- object we're operating on
   ubfx    w2, wINST, #8, #4           // w2<- A
   cbz     w3, common_errNullObject    // object was null
   add     x3, x3, x0
   .if 0
   ldarh x0, [x3]
   SET_VREG_WIDE x0, w2                // fp[A] <- value
   .elseif 0
   ldarh w0, [x3]
   cbnz wMR, .Lop_iget_short_read_barrier
   SET_VREG_OBJECT w0, w2              // fp[A] <- value
   .else
   ldarh w0, [x3]
   sxth w0, w0
   SET_VREG w0, w2                     // fp[A] <- value
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip

    NAME_END nterp_op_iget_short_helper
    NAME_START nterp_op_iget_wide_helper
   mov     x0, xSELF
   ldr     x1, [sp]
   mov     x2, xPC
   mov     x3, #0
   EXPORT_PC
   bl      nterp_get_instance_field_offset
   tbz     w0, #31, .Lop_iget_wide_resume
   CLEAR_INSTANCE_VOLATILE_MARKER w0
   lsr     w2, wINST, #12              // w2<- B
   GET_VREG w3, w2                     // w3<- object we're operating on
   ubfx    w2, wINST, #8, #4           // w2<- A
   cbz     w3, common_errNullObject    // object was null
   add     x3, x3, x0
   .if 1
   ldar x0, [x3]
   SET_VREG_WIDE x0, w2                // fp[A] <- value
   .elseif 0
   ldar w0, [x3]
   cbnz wMR, .Lop_iget_wide_read_barrier
   SET_VREG_OBJECT w0, w2              // fp[A] <- value
   .else
   ldar w0, [x3]
   
   SET_VREG w0, w2                     // fp[A] <- value
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip

    NAME_END nterp_op_iget_wide_helper
    NAME_START nterp_op_iput_helper_str00
   mov     x0, xSELF
   ldr     x1, [sp]
   mov     x2, xPC
   .if 0
   mov     x3, x26
   .else
   mov     x3, #0
   .endif
   EXPORT_PC
   bl      nterp_get_instance_field_offset
   .if 0
   // Reload the value as it may have moved.
   ubfx    w1, wINST, #8, #4           // w1<- A
   GET_VREG w26, w1                    // w26 <- v[A]
   .endif
   tbz     w0, #31, .Lop_iput_resume
   CLEAR_INSTANCE_VOLATILE_MARKER w0
   lsr     w2, wINST, #12              // w2<- B
   GET_VREG w2, w2                     // vB (object we're operating on)
   cbz     w2, common_errNullObject
   add     x3, x2, x0
   .if 0
   stlr x26, [x3]
   .else
   stlr w26, [x3]
   WRITE_BARRIER_IF_OBJECT 0, w26, w2, .Lop_iput_slow_path_skip_write_barrier
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip

    NAME_END nterp_op_iput_helper_str00
    NAME_START nterp_op_iput_helper_str01
   mov     x0, xSELF
   ldr     x1, [sp]
   mov     x2, xPC
   .if 1
   mov     x3, x26
   .else
   mov     x3, #0
   .endif
   EXPORT_PC
   bl      nterp_get_instance_field_offset
   .if 1
   // Reload the value as it may have moved.
   ubfx    w1, wINST, #8, #4           // w1<- A
   GET_VREG w26, w1                    // w26 <- v[A]
   .endif
   tbz     w0, #31, .Lop_iput_object_resume
   CLEAR_INSTANCE_VOLATILE_MARKER w0
   lsr     w2, wINST, #12              // w2<- B
   GET_VREG w2, w2                     // vB (object we're operating on)
   cbz     w2, common_errNullObject
   add     x3, x2, x0
   .if 0
   stlr x26, [x3]
   .else
   stlr w26, [x3]
   WRITE_BARRIER_IF_OBJECT 1, w26, w2, .Lop_iput_object_slow_path_skip_write_barrier
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip

    NAME_END nterp_op_iput_helper_str01
    NAME_START nterp_op_iput_helper_str10
   mov     x0, xSELF
   ldr     x1, [sp]
   mov     x2, xPC
   .if 0
   mov     x3, x26
   .else
   mov     x3, #0
   .endif
   EXPORT_PC
   bl      nterp_get_instance_field_offset
   .if 0
   // Reload the value as it may have moved.
   ubfx    w1, wINST, #8, #4           // w1<- A
   GET_VREG w26, w1                    // w26 <- v[A]
   .endif
   tbz     w0, #31, .Lop_iput_wide_resume
   CLEAR_INSTANCE_VOLATILE_MARKER w0
   lsr     w2, wINST, #12              // w2<- B
   GET_VREG w2, w2                     // vB (object we're operating on)
   cbz     w2, common_errNullObject
   add     x3, x2, x0
   .if 1
   stlr x26, [x3]
   .else
   stlr w26, [x3]
   WRITE_BARRIER_IF_OBJECT 0, w26, w2, .Lop_iput_wide_slow_path_skip_write_barrier
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip

    NAME_END nterp_op_iput_helper_str10
    NAME_START nterp_op_iput_helper_strb00
   mov     x0, xSELF
   ldr     x1, [sp]
   mov     x2, xPC
   .if 0
   mov     x3, x26
   .else
   mov     x3, #0
   .endif
   EXPORT_PC
   bl      nterp_get_instance_field_offset
   .if 0
   // Reload the value as it may have moved.
   ubfx    w1, wINST, #8, #4           // w1<- A
   GET_VREG w26, w1                    // w26 <- v[A]
   .endif
   tbz     w0, #31, .Lop_iput_byte_resume
   CLEAR_INSTANCE_VOLATILE_MARKER w0
   lsr     w2, wINST, #12              // w2<- B
   GET_VREG w2, w2                     // vB (object we're operating on)
   cbz     w2, common_errNullObject
   add     x3, x2, x0
   .if 0
   stlrb x26, [x3]
   .else
   stlrb w26, [x3]
   WRITE_BARRIER_IF_OBJECT 0, w26, w2, .Lop_iput_byte_slow_path_skip_write_barrier
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip

    NAME_END nterp_op_iput_helper_strb00
    NAME_START nterp_op_iput_helper_strh00
   mov     x0, xSELF
   ldr     x1, [sp]
   mov     x2, xPC
   .if 0
   mov     x3, x26
   .else
   mov     x3, #0
   .endif
   EXPORT_PC
   bl      nterp_get_instance_field_offset
   .if 0
   // Reload the value as it may have moved.
   ubfx    w1, wINST, #8, #4           // w1<- A
   GET_VREG w26, w1                    // w26 <- v[A]
   .endif
   tbz     w0, #31, .Lop_iput_short_resume
   CLEAR_INSTANCE_VOLATILE_MARKER w0
   lsr     w2, wINST, #12              // w2<- B
   GET_VREG w2, w2                     // vB (object we're operating on)
   cbz     w2, common_errNullObject
   add     x3, x2, x0
   .if 0
   stlrh x26, [x3]
   .else
   stlrh w26, [x3]
   WRITE_BARRIER_IF_OBJECT 0, w26, w2, .Lop_iput_short_slow_path_skip_write_barrier
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip

    NAME_END nterp_op_iput_helper_strh00
    NAME_START nterp_op_sget_boolean_helper
   mov     x0, xSELF
   ldr     x1, [sp]
   mov     x2, xPC
   mov     x3, #0
   EXPORT_PC
   bl      nterp_get_static_field
   tbz     x0, #0, .Lop_sget_boolean_resume
   CLEAR_STATIC_VOLATILE_MARKER x0
   ldr     w1, [x0, #ART_FIELD_OFFSET_OFFSET]
   lsr     w2, wINST, #8               // w2 <- A
   ldr     w0, [x0, #ART_FIELD_DECLARING_CLASS_OFFSET]
   cbnz    wMR, .Lop_sget_boolean_slow_path_read_barrier
.Lop_sget_boolean_slow_path_resume_after_read_barrier:
   add     x0, x0, x1
   .if 0
   ldar    x0, [x0]
   SET_VREG_WIDE x0, w2                // fp[A] <- value
   .elseif 0
   ldarb w0, [x0]
   cbnz    wMR, .Lop_sget_boolean_mark_after_load
   SET_VREG_OBJECT w0, w2              // fp[A] <- value
   .else
   ldarb w0, [x0]
   
   SET_VREG w0, w2                     // fp[A] <- value
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.Lop_sget_boolean_slow_path_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_sget_boolean_slow_path_resume_after_read_barrier

    NAME_END nterp_op_sget_boolean_helper
    NAME_START nterp_op_sget_byte_helper
   mov     x0, xSELF
   ldr     x1, [sp]
   mov     x2, xPC
   mov     x3, #0
   EXPORT_PC
   bl      nterp_get_static_field
   tbz     x0, #0, .Lop_sget_byte_resume
   CLEAR_STATIC_VOLATILE_MARKER x0
   ldr     w1, [x0, #ART_FIELD_OFFSET_OFFSET]
   lsr     w2, wINST, #8               // w2 <- A
   ldr     w0, [x0, #ART_FIELD_DECLARING_CLASS_OFFSET]
   cbnz    wMR, .Lop_sget_byte_slow_path_read_barrier
.Lop_sget_byte_slow_path_resume_after_read_barrier:
   add     x0, x0, x1
   .if 0
   ldar    x0, [x0]
   SET_VREG_WIDE x0, w2                // fp[A] <- value
   .elseif 0
   ldarb w0, [x0]
   cbnz    wMR, .Lop_sget_byte_mark_after_load
   SET_VREG_OBJECT w0, w2              // fp[A] <- value
   .else
   ldarb w0, [x0]
   sxtb w0, w0
   SET_VREG w0, w2                     // fp[A] <- value
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.Lop_sget_byte_slow_path_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_sget_byte_slow_path_resume_after_read_barrier

    NAME_END nterp_op_sget_byte_helper
    NAME_START nterp_op_sget_char_helper
   mov     x0, xSELF
   ldr     x1, [sp]
   mov     x2, xPC
   mov     x3, #0
   EXPORT_PC
   bl      nterp_get_static_field
   tbz     x0, #0, .Lop_sget_char_resume
   CLEAR_STATIC_VOLATILE_MARKER x0
   ldr     w1, [x0, #ART_FIELD_OFFSET_OFFSET]
   lsr     w2, wINST, #8               // w2 <- A
   ldr     w0, [x0, #ART_FIELD_DECLARING_CLASS_OFFSET]
   cbnz    wMR, .Lop_sget_char_slow_path_read_barrier
.Lop_sget_char_slow_path_resume_after_read_barrier:
   add     x0, x0, x1
   .if 0
   ldar    x0, [x0]
   SET_VREG_WIDE x0, w2                // fp[A] <- value
   .elseif 0
   ldarh w0, [x0]
   cbnz    wMR, .Lop_sget_char_mark_after_load
   SET_VREG_OBJECT w0, w2              // fp[A] <- value
   .else
   ldarh w0, [x0]
   
   SET_VREG w0, w2                     // fp[A] <- value
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.Lop_sget_char_slow_path_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_sget_char_slow_path_resume_after_read_barrier

    NAME_END nterp_op_sget_char_helper
    NAME_START nterp_op_sget_helper
   mov     x0, xSELF
   ldr     x1, [sp]
   mov     x2, xPC
   mov     x3, #0
   EXPORT_PC
   bl      nterp_get_static_field
   tbz     x0, #0, .Lop_sget_resume
   CLEAR_STATIC_VOLATILE_MARKER x0
   ldr     w1, [x0, #ART_FIELD_OFFSET_OFFSET]
   lsr     w2, wINST, #8               // w2 <- A
   ldr     w0, [x0, #ART_FIELD_DECLARING_CLASS_OFFSET]
   cbnz    wMR, .Lop_sget_slow_path_read_barrier
.Lop_sget_slow_path_resume_after_read_barrier:
   add     x0, x0, x1
   .if 0
   ldar    x0, [x0]
   SET_VREG_WIDE x0, w2                // fp[A] <- value
   .elseif 0
   ldar w0, [x0]
   cbnz    wMR, .Lop_sget_mark_after_load
   SET_VREG_OBJECT w0, w2              // fp[A] <- value
   .else
   ldar w0, [x0]
   
   SET_VREG w0, w2                     // fp[A] <- value
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.Lop_sget_slow_path_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_sget_slow_path_resume_after_read_barrier

    NAME_END nterp_op_sget_helper
    NAME_START nterp_op_sget_object_helper
   mov     x0, xSELF
   ldr     x1, [sp]
   mov     x2, xPC
   mov     x3, #0
   EXPORT_PC
   bl      nterp_get_static_field
   tbz     x0, #0, .Lop_sget_object_resume
   CLEAR_STATIC_VOLATILE_MARKER x0
   ldr     w1, [x0, #ART_FIELD_OFFSET_OFFSET]
   lsr     w2, wINST, #8               // w2 <- A
   ldr     w0, [x0, #ART_FIELD_DECLARING_CLASS_OFFSET]
   cbnz    wMR, .Lop_sget_object_slow_path_read_barrier
.Lop_sget_object_slow_path_resume_after_read_barrier:
   add     x0, x0, x1
   .if 0
   ldar    x0, [x0]
   SET_VREG_WIDE x0, w2                // fp[A] <- value
   .elseif 1
   ldar w0, [x0]
   cbnz    wMR, .Lop_sget_object_mark_after_load
   SET_VREG_OBJECT w0, w2              // fp[A] <- value
   .else
   ldar w0, [x0]
   
   SET_VREG w0, w2                     // fp[A] <- value
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.Lop_sget_object_slow_path_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_sget_object_slow_path_resume_after_read_barrier

    NAME_END nterp_op_sget_object_helper
    NAME_START nterp_op_sget_short_helper
   mov     x0, xSELF
   ldr     x1, [sp]
   mov     x2, xPC
   mov     x3, #0
   EXPORT_PC
   bl      nterp_get_static_field
   tbz     x0, #0, .Lop_sget_short_resume
   CLEAR_STATIC_VOLATILE_MARKER x0
   ldr     w1, [x0, #ART_FIELD_OFFSET_OFFSET]
   lsr     w2, wINST, #8               // w2 <- A
   ldr     w0, [x0, #ART_FIELD_DECLARING_CLASS_OFFSET]
   cbnz    wMR, .Lop_sget_short_slow_path_read_barrier
.Lop_sget_short_slow_path_resume_after_read_barrier:
   add     x0, x0, x1
   .if 0
   ldar    x0, [x0]
   SET_VREG_WIDE x0, w2                // fp[A] <- value
   .elseif 0
   ldarh w0, [x0]
   cbnz    wMR, .Lop_sget_short_mark_after_load
   SET_VREG_OBJECT w0, w2              // fp[A] <- value
   .else
   ldarh w0, [x0]
   sxth w0, w0
   SET_VREG w0, w2                     // fp[A] <- value
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.Lop_sget_short_slow_path_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_sget_short_slow_path_resume_after_read_barrier

    NAME_END nterp_op_sget_short_helper
    NAME_START nterp_op_sget_wide_helper
   mov     x0, xSELF
   ldr     x1, [sp]
   mov     x2, xPC
   mov     x3, #0
   EXPORT_PC
   bl      nterp_get_static_field
   tbz     x0, #0, .Lop_sget_wide_resume
   CLEAR_STATIC_VOLATILE_MARKER x0
   ldr     w1, [x0, #ART_FIELD_OFFSET_OFFSET]
   lsr     w2, wINST, #8               // w2 <- A
   ldr     w0, [x0, #ART_FIELD_DECLARING_CLASS_OFFSET]
   cbnz    wMR, .Lop_sget_wide_slow_path_read_barrier
.Lop_sget_wide_slow_path_resume_after_read_barrier:
   add     x0, x0, x1
   .if 1
   ldar    x0, [x0]
   SET_VREG_WIDE x0, w2                // fp[A] <- value
   .elseif 0
   ldar w0, [x0]
   cbnz    wMR, .Lop_sget_wide_mark_after_load
   SET_VREG_OBJECT w0, w2              // fp[A] <- value
   .else
   ldar w0, [x0]
   
   SET_VREG w0, w2                     // fp[A] <- value
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.Lop_sget_wide_slow_path_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_sget_wide_slow_path_resume_after_read_barrier

    NAME_END nterp_op_sget_wide_helper
    NAME_START nterp_op_sput_helper_str00
   mov     x0, xSELF
   ldr     x1, [sp]
   mov     x2, xPC
   .if 0
   mov     x3, x26
   .else
   mov     x3, #0
   .endif
   EXPORT_PC
   bl      nterp_get_static_field
   .if 0
   // Reload the value as it may have moved.
   lsr     w2, wINST, #8               // w2 <- A
   GET_VREG w26, w2                    // w26 <- v[A]
   .endif
   tbz     x0, #0, .Lop_sput_resume
   CLEAR_STATIC_VOLATILE_MARKER x0
   ldr     w1, [x0, #ART_FIELD_OFFSET_OFFSET]
   ldr     w0, [x0, #ART_FIELD_DECLARING_CLASS_OFFSET]
   cbnz    wMR, .Lop_sput_slow_path_read_barrier
.Lop_sput_slow_path_resume_after_read_barrier:
   add     x1, x0, x1
   .if 0
   stlr    x26, [x1]
   .else
   stlr    w26, [x1]
   WRITE_BARRIER_IF_OBJECT 0, w26, w0, .Lop_sput_slow_path_skip_write_barrier
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.Lop_sput_slow_path_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_sput_slow_path_resume_after_read_barrier

    NAME_END nterp_op_sput_helper_str00
    NAME_START nterp_op_sput_helper_str01
   mov     x0, xSELF
   ldr     x1, [sp]
   mov     x2, xPC
   .if 1
   mov     x3, x26
   .else
   mov     x3, #0
   .endif
   EXPORT_PC
   bl      nterp_get_static_field
   .if 1
   // Reload the value as it may have moved.
   lsr     w2, wINST, #8               // w2 <- A
   GET_VREG w26, w2                    // w26 <- v[A]
   .endif
   tbz     x0, #0, .Lop_sput_object_resume
   CLEAR_STATIC_VOLATILE_MARKER x0
   ldr     w1, [x0, #ART_FIELD_OFFSET_OFFSET]
   ldr     w0, [x0, #ART_FIELD_DECLARING_CLASS_OFFSET]
   cbnz    wMR, .Lop_sput_object_slow_path_read_barrier
.Lop_sput_object_slow_path_resume_after_read_barrier:
   add     x1, x0, x1
   .if 0
   stlr    x26, [x1]
   .else
   stlr    w26, [x1]
   WRITE_BARRIER_IF_OBJECT 1, w26, w0, .Lop_sput_object_slow_path_skip_write_barrier
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.Lop_sput_object_slow_path_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_sput_object_slow_path_resume_after_read_barrier

    NAME_END nterp_op_sput_helper_str01
    NAME_START nterp_op_sput_helper_str10
   mov     x0, xSELF
   ldr     x1, [sp]
   mov     x2, xPC
   .if 0
   mov     x3, x26
   .else
   mov     x3, #0
   .endif
   EXPORT_PC
   bl      nterp_get_static_field
   .if 0
   // Reload the value as it may have moved.
   lsr     w2, wINST, #8               // w2 <- A
   GET_VREG w26, w2                    // w26 <- v[A]
   .endif
   tbz     x0, #0, .Lop_sput_wide_resume
   CLEAR_STATIC_VOLATILE_MARKER x0
   ldr     w1, [x0, #ART_FIELD_OFFSET_OFFSET]
   ldr     w0, [x0, #ART_FIELD_DECLARING_CLASS_OFFSET]
   cbnz    wMR, .Lop_sput_wide_slow_path_read_barrier
.Lop_sput_wide_slow_path_resume_after_read_barrier:
   add     x1, x0, x1
   .if 1
   stlr    x26, [x1]
   .else
   stlr    w26, [x1]
   WRITE_BARRIER_IF_OBJECT 0, w26, w0, .Lop_sput_wide_slow_path_skip_write_barrier
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.Lop_sput_wide_slow_path_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_sput_wide_slow_path_resume_after_read_barrier

    NAME_END nterp_op_sput_helper_str10
    NAME_START nterp_op_sput_helper_strb00
   mov     x0, xSELF
   ldr     x1, [sp]
   mov     x2, xPC
   .if 0
   mov     x3, x26
   .else
   mov     x3, #0
   .endif
   EXPORT_PC
   bl      nterp_get_static_field
   .if 0
   // Reload the value as it may have moved.
   lsr     w2, wINST, #8               // w2 <- A
   GET_VREG w26, w2                    // w26 <- v[A]
   .endif
   tbz     x0, #0, .Lop_sput_byte_resume
   CLEAR_STATIC_VOLATILE_MARKER x0
   ldr     w1, [x0, #ART_FIELD_OFFSET_OFFSET]
   ldr     w0, [x0, #ART_FIELD_DECLARING_CLASS_OFFSET]
   cbnz    wMR, .Lop_sput_byte_slow_path_read_barrier
.Lop_sput_byte_slow_path_resume_after_read_barrier:
   add     x1, x0, x1
   .if 0
   stlrb    x26, [x1]
   .else
   stlrb    w26, [x1]
   WRITE_BARRIER_IF_OBJECT 0, w26, w0, .Lop_sput_byte_slow_path_skip_write_barrier
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.Lop_sput_byte_slow_path_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_sput_byte_slow_path_resume_after_read_barrier

    NAME_END nterp_op_sput_helper_strb00
    NAME_START nterp_op_sput_helper_strh00
   mov     x0, xSELF
   ldr     x1, [sp]
   mov     x2, xPC
   .if 0
   mov     x3, x26
   .else
   mov     x3, #0
   .endif
   EXPORT_PC
   bl      nterp_get_static_field
   .if 0
   // Reload the value as it may have moved.
   lsr     w2, wINST, #8               // w2 <- A
   GET_VREG w26, w2                    // w26 <- v[A]
   .endif
   tbz     x0, #0, .Lop_sput_short_resume
   CLEAR_STATIC_VOLATILE_MARKER x0
   ldr     w1, [x0, #ART_FIELD_OFFSET_OFFSET]
   ldr     w0, [x0, #ART_FIELD_DECLARING_CLASS_OFFSET]
   cbnz    wMR, .Lop_sput_short_slow_path_read_barrier
.Lop_sput_short_slow_path_resume_after_read_barrier:
   add     x1, x0, x1
   .if 0
   stlrh    x26, [x1]
   .else
   stlrh    w26, [x1]
   WRITE_BARRIER_IF_OBJECT 0, w26, w0, .Lop_sput_short_slow_path_skip_write_barrier
   .endif
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
.Lop_sput_short_slow_path_read_barrier:
   bl      art_quick_read_barrier_mark_reg00
   b       .Lop_sput_short_slow_path_resume_after_read_barrier

    NAME_END nterp_op_sput_helper_strh00

/*
 * ===========================================================================
 *  Common subroutines and data
 * ===========================================================================
 */

    .text
    .align  2

// Enclose all code below in a symbol (which gets printed in backtraces).
NAME_START nterp_helper

// Note: mterp also uses the common_* names below for helpers, but that's OK
// as the assembler compiled each interpreter separately.
common_errDivideByZero:
    EXPORT_PC
    bl art_quick_throw_div_zero

// Expect index in w1, length in w3.
common_errArrayIndex:
    EXPORT_PC
    mov x0, x1
    mov x1, x3
    bl art_quick_throw_array_bounds

common_errNullObject:
    EXPORT_PC
    bl art_quick_throw_null_pointer_exception

NterpCommonInvokeStatic:
    COMMON_INVOKE_NON_RANGE is_static=1, suffix="invokeStatic"

NterpCommonInvokeStaticRange:
    COMMON_INVOKE_RANGE is_static=1, suffix="invokeStatic"

NterpCommonInvokeInstance:
    COMMON_INVOKE_NON_RANGE suffix="invokeInstance"

NterpCommonInvokeInstanceRange:
    COMMON_INVOKE_RANGE suffix="invokeInstance"

NterpCommonInvokeInterface:
    COMMON_INVOKE_NON_RANGE is_interface=1, suffix="invokeInterface"

NterpCommonInvokeInterfaceRange:
    COMMON_INVOKE_RANGE is_interface=1, suffix="invokeInterface"

NterpCommonInvokePolymorphic:
    COMMON_INVOKE_NON_RANGE is_polymorphic=1, suffix="invokePolymorphic"

NterpCommonInvokePolymorphicRange:
    COMMON_INVOKE_RANGE is_polymorphic=1, suffix="invokePolymorphic"

NterpCommonInvokeCustom:
    COMMON_INVOKE_NON_RANGE is_static=1, is_custom=1, suffix="invokeCustom"

NterpCommonInvokeCustomRange:
    COMMON_INVOKE_RANGE is_static=1, is_custom=1, suffix="invokeCustom"

NterpHandleStringInit:
   COMMON_INVOKE_NON_RANGE is_string_init=1, suffix="stringInit"

NterpHandleStringInitRange:
   COMMON_INVOKE_RANGE is_string_init=1, suffix="stringInit"

NterpNewArray:
   /* new-array vA, vB, class@CCCC */
   EXPORT_PC
   // Fast-path which gets the class from thread-local cache.
   FETCH_FROM_THREAD_CACHE x0, 2f
   cbnz wMR, 3f
1:
   lsr     w1, wINST, #12              // w1<- B
   GET_VREG w1, w1                     // w1<- vB (array length)
   ldr lr, [xSELF, #THREAD_ALLOC_ARRAY_ENTRYPOINT_OFFSET]
   blr lr
   ubfx    w1, wINST, #8, #4           // w1<- A
   SET_VREG_OBJECT w0, w1
   FETCH_ADVANCE_INST 2
   GET_INST_OPCODE ip
   GOTO_OPCODE ip
2:
   mov x0, xSELF
   ldr x1, [sp, 0]
   mov x2, xPC
   bl nterp_get_class_or_allocate_object
   b 1b
3:
   bl art_quick_read_barrier_mark_reg00
   b 1b

NterpHandleHotnessOverflow:
    add x1, xPC, xINST, lsl #1
    mov x2, xFP
    bl nterp_hot_method
    cbnz x0, 1f
    add     xPC, xPC, wINST, sxtw #1    // update xPC
    FETCH wINST, 0                      // load wINST
    GET_INST_OPCODE ip                  // extract opcode from wINST
    GOTO_OPCODE ip                      // jump to next instruction
1:
    // Drop the current frame.
    ldr ip, [xREFS, #-8]
    mov sp, ip
    .cfi_def_cfa sp, CALLEE_SAVES_SIZE

    // The transition frame of type SaveAllCalleeSaves saves x19 and x20,
    // but not managed ABI. So we need to restore callee-saves of the nterp frame,
    // and save managed ABI callee saves, which will be restored by the callee upon
    // return.
    RESTORE_ALL_CALLEE_SAVES
    INCREASE_FRAME ((CALLEE_SAVES_SIZE) - 16)

    // FP callee-saves
    stp d8, d9, [sp, #0]
    stp d10, d11, [sp, #16]
    stp d12, d13, [sp, #32]
    stp d14, d15, [sp, #48]

    // GP callee-saves.
    SAVE_TWO_REGS x21, x22, 64
    SAVE_TWO_REGS x23, x24, 80
    SAVE_TWO_REGS x25, x26, 96
    SAVE_TWO_REGS x27, x28, 112
    SAVE_TWO_REGS x29, lr, 128

    // Setup the new frame
    ldr x1, [x0, #OSR_DATA_FRAME_SIZE]
    // Given stack size contains all callee saved registers, remove them.
    sub x1, x1, #(CALLEE_SAVES_SIZE - 16)

    // We know x1 cannot be 0, as it at least contains the ArtMethod.

    // Remember CFA in a callee-save register.
    mov xINST, sp
    .cfi_def_cfa_register xINST

    sub sp, sp, x1

    add x2, x0, #OSR_DATA_MEMORY
2:
    sub x1, x1, #8
    ldr ip, [x2, x1]
    str ip, [sp, x1]
    cbnz x1, 2b

    // Fetch the native PC to jump to and save it in a callee-save register.
    ldr xFP, [x0, #OSR_DATA_NATIVE_PC]

    // Free the memory holding OSR Data.
    bl free

    // Jump to the compiled code.
    br xFP

// This is the logical end of ExecuteNterpImpl, where the frame info applies.
// EndExecuteNterpImpl includes the methods below as we want the runtime to
// see them as part of the Nterp PCs.
.cfi_endproc

nterp_to_nterp_static_non_range:
    .cfi_startproc
    SETUP_STACK_FOR_INVOKE
    SETUP_NON_RANGE_ARGUMENTS_AND_EXECUTE is_static=1, is_string_init=0
    .cfi_endproc

nterp_to_nterp_string_init_non_range:
    .cfi_startproc
    SETUP_STACK_FOR_INVOKE
    SETUP_NON_RANGE_ARGUMENTS_AND_EXECUTE is_static=0, is_string_init=1
    .cfi_endproc

nterp_to_nterp_instance_non_range:
    .cfi_startproc
    SETUP_STACK_FOR_INVOKE
    SETUP_NON_RANGE_ARGUMENTS_AND_EXECUTE is_static=0, is_string_init=0
    .cfi_endproc

nterp_to_nterp_static_range:
    .cfi_startproc
    SETUP_STACK_FOR_INVOKE
    SETUP_RANGE_ARGUMENTS_AND_EXECUTE is_static=1
    .cfi_endproc

nterp_to_nterp_instance_range:
    .cfi_startproc
    SETUP_STACK_FOR_INVOKE
    SETUP_RANGE_ARGUMENTS_AND_EXECUTE is_static=0
    .cfi_endproc

nterp_to_nterp_string_init_range:
    .cfi_startproc
    SETUP_STACK_FOR_INVOKE
    SETUP_RANGE_ARGUMENTS_AND_EXECUTE is_static=0, is_string_init=1
    .cfi_endproc

NAME_END nterp_helper

// This is the end of PCs contained by the OatQuickMethodHeader created for the interpreter
// entry point.
    .type EndExecuteNterpImpl, #function
    .hidden EndExecuteNterpImpl
    .global EndExecuteNterpImpl
EndExecuteNterpImpl:

// Entrypoints into runtime.
NTERP_TRAMPOLINE nterp_get_static_field, NterpGetStaticField
NTERP_TRAMPOLINE nterp_get_instance_field_offset, NterpGetInstanceFieldOffset
NTERP_TRAMPOLINE nterp_filled_new_array, NterpFilledNewArray
NTERP_TRAMPOLINE nterp_filled_new_array_range, NterpFilledNewArrayRange
NTERP_TRAMPOLINE nterp_get_class_or_allocate_object, NterpGetClassOrAllocateObject
NTERP_TRAMPOLINE nterp_get_method, NterpGetMethod
NTERP_TRAMPOLINE nterp_hot_method, NterpHotMethod
NTERP_TRAMPOLINE nterp_load_object, NterpLoadObject

// gen_mterp.py will inline the following definitions
// within [ExecuteNterpImpl, EndExecuteNterpImpl).
