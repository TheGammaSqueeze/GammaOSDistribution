// RUN: hlo_to_llvm_ir %s | FileCheck %s

// CHECK: define void @fusion(i8* noalias align 16 dereferenceable(8192) %[[VAL_0:.*]], i8* noalias align 64 dereferenceable(256) %[[VAL_1:.*]], i8* noalias align 64 dereferenceable(256) %[[VAL_2:.*]], i8* noalias align 16 dereferenceable(4) %[[VAL_3:.*]], i8* noalias align 16 dereferenceable(4) %[[VAL_4:.*]]) {
// CHECK:       entry:
// CHECK:         %[[VAL_8:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 0
// CHECK:         %[[VAL_9:.*]] = bitcast i8* %[[VAL_8]] to [2 x [32 x [32 x float]]]*
// CHECK:         %[[VAL_10:.*]] = getelementptr inbounds i8, i8* %[[VAL_3]], i64 0
// CHECK:         %[[VAL_11:.*]] = bitcast i8* %[[VAL_10]] to float*
// CHECK:         %[[VAL_12:.*]] = getelementptr inbounds i8, i8* %[[VAL_4]], i64 0
// CHECK:         %[[VAL_13:.*]] = bitcast i8* %[[VAL_12]] to float*
// CHECK:         %[[VAL_14:.*]] = getelementptr inbounds i8, i8* %[[VAL_1]], i64 0
// CHECK:         %[[VAL_15:.*]] = bitcast i8* %[[VAL_14]] to [2 x [32 x float]]*
// CHECK:         %[[VAL_16:.*]] = getelementptr inbounds i8, i8* %[[VAL_2]], i64 0
// CHECK:         %[[VAL_17:.*]] = bitcast i8* %[[VAL_16]] to [2 x [32 x float]]*
// CHECK:         %[[VAL_18:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !6
// CHECK:         %[[VAL_19:.*]] = zext i32 %[[VAL_18]] to i64
// CHECK:         %[[VAL_20:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !7
// CHECK:         %[[VAL_21:.*]] = zext i32 %[[VAL_20]] to i64
// CHECK:         %[[VAL_22:.*]] = mul nuw nsw i64 %[[VAL_19]], 64
// CHECK:         %[[VAL_23:.*]] = add nuw nsw i64 %[[VAL_22]], %[[VAL_21]]
// CHECK:         %[[VAL_24:.*]] = icmp ult i64 %[[VAL_23]], 64
// CHECK:         call void @llvm.assume(i1 %[[VAL_24]])
// CHECK:         %[[VAL_25:.*]] = udiv i64 %[[VAL_23]], 1
// CHECK:         %[[VAL_26:.*]] = urem i64 %[[VAL_25]], 32
// CHECK:         %[[VAL_27:.*]] = udiv i64 %[[VAL_23]], 32
// CHECK:         %[[VAL_28:.*]] = icmp ult i64 %[[VAL_23]], 64
// CHECK:         br i1 %[[VAL_28]], label %[[VAL_29:.*]], label %[[VAL_30:.*]]
// CHECK:       fusion.in_bounds-after:                           ; preds = %[[VAL_29]], %[[VAL_31:.*]]
// CHECK:         ret void
// CHECK:       fusion.in_bounds-true:                            ; preds = %[[VAL_31]]
// CHECK:         %[[VAL_32:.*]] = load float, float* %[[VAL_11]], align 4, !invariant.load !8
// CHECK:         %[[VAL_33:.*]] = bitcast [2 x [32 x float]]* %[[VAL_15]] to float*
// CHECK:         %[[VAL_34:.*]] = getelementptr inbounds float, float* %[[VAL_33]], i64 %[[VAL_23]]
// CHECK:         store float %[[VAL_32]], float* %[[VAL_34]], align 4
// CHECK:         br label %[[VAL_30]]
// CHECK:       }
// CHECK:       ; Function Attrs: nounwind readnone
// CHECK:       declare i32 @llvm.nvvm.read.ptx.sreg.ctaid.x() #0
// CHECK:       ; Function Attrs: nounwind readnone
// CHECK:       declare i32 @llvm.nvvm.read.ptx.sreg.tid.x() #0
// CHECK:       ; Function Attrs: nofree nosync nounwind willreturn
// CHECK:       declare void @llvm.assume(i1 noundef) #1

// CHECK: define void @fusion__1(i8* noalias align 16 dereferenceable(8192) %[[VAL_0:.*]], i8* noalias align 64 dereferenceable(256) %[[VAL_1:.*]], i8* noalias align 64 dereferenceable(256) %[[VAL_2:.*]], i8* noalias align 16 dereferenceable(4) %[[VAL_3:.*]], i8* noalias align 16 dereferenceable(4) %[[VAL_4:.*]]) {
// CHECK:       entry:
// CHECK:         %[[VAL_8:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 0
// CHECK:         %[[VAL_9:.*]] = bitcast i8* %[[VAL_8]] to [2 x [32 x [32 x float]]]*
// CHECK:         %[[VAL_10:.*]] = getelementptr inbounds i8, i8* %[[VAL_3]], i64 0
// CHECK:         %[[VAL_11:.*]] = bitcast i8* %[[VAL_10]] to float*
// CHECK:         %[[VAL_12:.*]] = getelementptr inbounds i8, i8* %[[VAL_4]], i64 0
// CHECK:         %[[VAL_13:.*]] = bitcast i8* %[[VAL_12]] to float*
// CHECK:         %[[VAL_14:.*]] = getelementptr inbounds i8, i8* %[[VAL_1]], i64 0
// CHECK:         %[[VAL_15:.*]] = bitcast i8* %[[VAL_14]] to [2 x [32 x float]]*
// CHECK:         %[[VAL_16:.*]] = getelementptr inbounds i8, i8* %[[VAL_2]], i64 0
// CHECK:         %[[VAL_17:.*]] = bitcast i8* %[[VAL_16]] to [2 x [32 x float]]*
// CHECK:         %[[VAL_18:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !6
// CHECK:         %[[VAL_19:.*]] = zext i32 %[[VAL_18]] to i64
// CHECK:         %[[VAL_20:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !7
// CHECK:         %[[VAL_21:.*]] = zext i32 %[[VAL_20]] to i64
// CHECK:         %[[VAL_22:.*]] = mul nuw nsw i64 %[[VAL_19]], 64
// CHECK:         %[[VAL_23:.*]] = add nuw nsw i64 %[[VAL_22]], %[[VAL_21]]
// CHECK:         %[[VAL_24:.*]] = icmp ult i64 %[[VAL_23]], 64
// CHECK:         call void @llvm.assume(i1 %[[VAL_24]])
// CHECK:         %[[VAL_25:.*]] = udiv i64 %[[VAL_23]], 1
// CHECK:         %[[VAL_26:.*]] = urem i64 %[[VAL_25]], 32
// CHECK:         %[[VAL_27:.*]] = udiv i64 %[[VAL_23]], 32
// CHECK:         %[[VAL_28:.*]] = icmp ult i64 %[[VAL_23]], 64
// CHECK:         br i1 %[[VAL_28]], label %[[VAL_29:.*]], label %[[VAL_30:.*]]
// CHECK:       fusion.in_bounds-after:                           ; preds = %[[VAL_29]], %[[VAL_31:.*]]
// CHECK:         ret void
// CHECK:       fusion.in_bounds-true:                            ; preds = %[[VAL_31]]
// CHECK:         %[[VAL_32:.*]] = load float, float* %[[VAL_13]], align 4, !invariant.load !8
// CHECK:         %[[VAL_33:.*]] = bitcast [2 x [32 x float]]* %[[VAL_17]] to float*
// CHECK:         %[[VAL_34:.*]] = getelementptr inbounds float, float* %[[VAL_33]], i64 %[[VAL_23]]
// CHECK:         store float %[[VAL_32]], float* %[[VAL_34]], align 4
// CHECK:         br label %[[VAL_30]]
// CHECK:       }

// CHECK: define void @fusion__2(i8* noalias align 16 dereferenceable(8192) %[[VAL_0:.*]], i8* noalias align 64 dereferenceable(256) %[[VAL_1:.*]], i8* noalias align 64 dereferenceable(256) %[[VAL_2:.*]], i8* noalias align 16 dereferenceable(4) %[[VAL_3:.*]], i8* noalias align 16 dereferenceable(4) %[[VAL_4:.*]]) {
// CHECK:       entry:
// CHECK:         %[[VAL_8:.*]] = alloca i32, align 4
// CHECK:         %[[VAL_9:.*]] = alloca i32, align 4
// CHECK:         %[[VAL_10:.*]] = alloca float, align 4
// CHECK:         %[[VAL_11:.*]] = alloca float, align 4
// CHECK:         %[[VAL_12:.*]] = alloca float, align 4
// CHECK:         %[[VAL_13:.*]] = alloca float, align 4
// CHECK:         %[[VAL_14:.*]] = alloca float, align 4
// CHECK:         %[[VAL_15:.*]] = alloca float, align 4
// CHECK:         %[[VAL_16:.*]] = alloca float, align 4
// CHECK:         %[[VAL_17:.*]] = alloca float, align 4
// CHECK:         %[[VAL_18:.*]] = alloca float, align 4
// CHECK:         %[[VAL_19:.*]] = alloca float, align 4
// CHECK:         %[[VAL_20:.*]] = alloca float, align 4
// CHECK:         %[[VAL_21:.*]] = alloca float, align 4
// CHECK:         %[[VAL_22:.*]] = alloca float, align 4
// CHECK:         %[[VAL_23:.*]] = alloca float, align 4
// CHECK:         %[[VAL_24:.*]] = alloca float, align 4
// CHECK:         %[[VAL_25:.*]] = alloca float, align 4
// CHECK:         %[[VAL_26:.*]] = alloca float, align 4
// CHECK:         %[[VAL_27:.*]] = alloca float, align 4
// CHECK:         %[[VAL_28:.*]] = alloca float, align 4
// CHECK:         %[[VAL_29:.*]] = alloca float, align 4
// CHECK:         %[[VAL_30:.*]] = alloca float, align 4
// CHECK:         %[[VAL_31:.*]] = alloca float, align 4
// CHECK:         %[[VAL_32:.*]] = alloca i32, align 4
// CHECK:         %[[VAL_33:.*]] = alloca float, align 4
// CHECK:         %[[VAL_34:.*]] = alloca float, align 4
// CHECK:         %[[VAL_35:.*]] = alloca float, align 4
// CHECK:         %[[VAL_36:.*]] = alloca float, align 4
// CHECK:         %[[VAL_37:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 0
// CHECK:         %[[VAL_38:.*]] = bitcast i8* %[[VAL_37]] to [2 x [32 x [32 x float]]]*
// CHECK:         %[[VAL_39:.*]] = getelementptr inbounds i8, i8* %[[VAL_3]], i64 0
// CHECK:         %[[VAL_40:.*]] = bitcast i8* %[[VAL_39]] to float*
// CHECK:         %[[VAL_41:.*]] = getelementptr inbounds i8, i8* %[[VAL_4]], i64 0
// CHECK:         %[[VAL_42:.*]] = bitcast i8* %[[VAL_41]] to float*
// CHECK:         %[[VAL_43:.*]] = getelementptr inbounds i8, i8* %[[VAL_1]], i64 0
// CHECK:         %[[VAL_44:.*]] = bitcast i8* %[[VAL_43]] to [2 x [32 x float]]*
// CHECK:         %[[VAL_45:.*]] = getelementptr inbounds i8, i8* %[[VAL_2]], i64 0
// CHECK:         %[[VAL_46:.*]] = bitcast i8* %[[VAL_45]] to [2 x [32 x float]]*
// CHECK:         %[[VAL_47:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.y(), !range !6
// CHECK:         %[[VAL_48:.*]] = icmp eq i32 %[[VAL_47]], 0
// CHECK:         br i1 %[[VAL_48]], label %[[VAL_49:.*]], label %[[VAL_50:.*]]
// CHECK:       reduce-group-0-after:                             ; preds = %[[VAL_51:.*]], %[[VAL_52:.*]]
// CHECK:         ret void
// CHECK:       reduce-group-0-true:                              ; preds = %[[VAL_52]]
// CHECK:         %[[VAL_53:.*]] = load float, float* %[[VAL_40]], align 4, !invariant.load !8
// CHECK:         %[[VAL_54:.*]] = getelementptr inbounds float, float* %[[VAL_35]], i32 0
// CHECK:         store float %[[VAL_53]], float* %[[VAL_54]], align 4
// CHECK:         %[[VAL_55:.*]] = load float, float* %[[VAL_42]], align 4, !invariant.load !8
// CHECK:         %[[VAL_56:.*]] = getelementptr inbounds float, float* %[[VAL_33]], i32 0
// CHECK:         store float %[[VAL_55]], float* %[[VAL_56]], align 4
// CHECK:         %[[VAL_57:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !9
// CHECK:         %[[VAL_58:.*]] = urem i32 %[[VAL_57]], 32
// CHECK:         %[[VAL_59:.*]] = udiv i32 %[[VAL_57]], 32
// CHECK:         %[[VAL_60:.*]] = urem i32 %[[VAL_57]], 32
// CHECK:         %[[VAL_61:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !7
// CHECK:         %[[VAL_62:.*]] = udiv i32 %[[VAL_61]], 1
// CHECK:         %[[VAL_63:.*]] = urem i32 %[[VAL_62]], 1
// CHECK:         %[[VAL_64:.*]] = udiv i32 %[[VAL_61]], 1
// CHECK:         %[[VAL_65:.*]] = urem i32 %[[VAL_64]], 64
// CHECK:         %[[VAL_66:.*]] = udiv i32 %[[VAL_61]], 64
// CHECK:         %[[VAL_67:.*]] = mul i32 %[[VAL_66]], 1
// CHECK:         %[[VAL_68:.*]] = icmp eq i32 %[[VAL_65]], 63
// CHECK:         %[[VAL_69:.*]] = select i1 %[[VAL_68]], i32 1, i32 1
// CHECK:         %[[VAL_70:.*]] = icmp eq i32 %[[VAL_63]], 0
// CHECK:         %[[VAL_71:.*]] = select i1 %[[VAL_70]], i32 32, i32 256
// CHECK:         %[[VAL_72:.*]] = mul i32 %[[VAL_65]], 1
// CHECK:         %[[VAL_73:.*]] = mul i32 %[[VAL_63]], 256
// CHECK:         %[[VAL_74:.*]] = mul i32 %[[VAL_58]], 2
// CHECK:         %[[VAL_75:.*]] = add i32 %[[VAL_73]], %[[VAL_74]]
// CHECK:         %[[VAL_76:.*]] = sub i32 %[[VAL_69]], %[[VAL_59]]
// CHECK:         %[[VAL_77:.*]] = add i32 %[[VAL_76]], 1
// CHECK:         %[[VAL_78:.*]] = add i32 %[[VAL_77]], -1
// CHECK:         %[[VAL_79:.*]] = udiv i32 %[[VAL_78]], 1
// CHECK:         store i32 0, i32* %[[VAL_32]], align 4
// CHECK:         br label %[[VAL_80:.*]]
// CHECK:       output_y_in_tile.loop_header:                     ; preds = %[[VAL_81:.*]], %[[VAL_49]]
// CHECK:         %[[VAL_82:.*]] = load i32, i32* %[[VAL_32]], align 4
// CHECK:         %[[VAL_83:.*]] = icmp uge i32 %[[VAL_82]], %[[VAL_79]]
// CHECK:         br i1 %[[VAL_83]], label %[[VAL_84:.*]], label %[[VAL_85:.*]]
// CHECK:       output_y_in_tile.loop_body:                       ; preds = %[[VAL_80]]
// CHECK:         %[[VAL_86:.*]] = add nuw nsw i32 %[[VAL_82]], 1
// CHECK:         store i32 %[[VAL_86]], i32* %[[VAL_32]], align 4
// CHECK:         %[[VAL_87:.*]] = icmp eq i32 %[[VAL_82]], 0
// CHECK:         %[[VAL_88:.*]] = mul i32 %[[VAL_82]], 1
// CHECK:         %[[VAL_89:.*]] = add i32 %[[VAL_59]], %[[VAL_88]]
// CHECK:         %[[VAL_90:.*]] = icmp eq i32 256, %[[VAL_71]]
// CHECK:         br i1 %[[VAL_90]], label %[[VAL_91:.*]], label %[[VAL_92:.*]]
// CHECK:       output_is_full_tile-after:                        ; preds = %[[VAL_93:.*]], %[[VAL_91]]
// CHECK:         br label %[[VAL_80]], !llvm.loop !10
// CHECK:       output_y_in_tile.loop_exit:                       ; preds = %[[VAL_80]]
// CHECK:         %[[VAL_94:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !9
// CHECK:         %[[VAL_95:.*]] = urem i32 %[[VAL_94]], 32
// CHECK:         %[[VAL_96:.*]] = udiv i32 %[[VAL_94]], 32
// CHECK:         %[[VAL_97:.*]] = urem i32 %[[VAL_94]], 32
// CHECK:         %[[VAL_98:.*]] = mul i32 %[[VAL_95]], 2
// CHECK:         %[[VAL_99:.*]] = add i32 %[[VAL_72]], %[[VAL_96]]
// CHECK:         %[[VAL_100:.*]] = add i32 %[[VAL_73]], %[[VAL_98]]
// CHECK:         %[[VAL_101:.*]] = add i32 %[[VAL_100]], 0
// CHECK:         %[[VAL_102:.*]] = udiv i32 %[[VAL_99]], 1
// CHECK:         %[[VAL_103:.*]] = urem i32 %[[VAL_102]], 32
// CHECK:         %[[VAL_104:.*]] = udiv i32 %[[VAL_99]], 32
// CHECK:         %[[VAL_105:.*]] = getelementptr inbounds [2 x [32 x float]], [2 x [32 x float]]* %[[VAL_44]], i32 0, i32 %[[VAL_104]], i32 %[[VAL_103]]
// CHECK:         %[[VAL_106:.*]] = getelementptr inbounds float, float* %[[VAL_35]], i32 0
// CHECK:         %[[VAL_107:.*]] = load float, float* %[[VAL_106]], align 4
// CHECK:         %[[VAL_108:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_107]], i32 16, i32 31)
// CHECK:         store float %[[VAL_108]], float* %[[VAL_31]], align 4
// CHECK:         call void @region_1_4(float* %[[VAL_106]], float* %[[VAL_31]], float* %[[VAL_106]])
// CHECK:         %[[VAL_109:.*]] = load float, float* %[[VAL_106]], align 4
// CHECK:         %[[VAL_110:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_109]], i32 8, i32 31)
// CHECK:         store float %[[VAL_110]], float* %[[VAL_30]], align 4
// CHECK:         call void @region_1_4(float* %[[VAL_106]], float* %[[VAL_30]], float* %[[VAL_106]])
// CHECK:         %[[VAL_111:.*]] = load float, float* %[[VAL_106]], align 4
// CHECK:         %[[VAL_112:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_111]], i32 4, i32 31)
// CHECK:         store float %[[VAL_112]], float* %[[VAL_29]], align 4
// CHECK:         call void @region_1_4(float* %[[VAL_106]], float* %[[VAL_29]], float* %[[VAL_106]])
// CHECK:         %[[VAL_113:.*]] = load float, float* %[[VAL_106]], align 4
// CHECK:         %[[VAL_114:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_113]], i32 2, i32 31)
// CHECK:         store float %[[VAL_114]], float* %[[VAL_28]], align 4
// CHECK:         call void @region_1_4(float* %[[VAL_106]], float* %[[VAL_28]], float* %[[VAL_106]])
// CHECK:         %[[VAL_115:.*]] = load float, float* %[[VAL_106]], align 4
// CHECK:         %[[VAL_116:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_115]], i32 1, i32 31)
// CHECK:         store float %[[VAL_116]], float* %[[VAL_27]], align 4
// CHECK:         call void @region_1_4(float* %[[VAL_106]], float* %[[VAL_27]], float* %[[VAL_106]])
// CHECK:         %[[VAL_117:.*]] = udiv i32 %[[VAL_95]], 32
// CHECK:         %[[VAL_118:.*]] = icmp eq i32 %[[VAL_97]], 0
// CHECK:         br i1 %[[VAL_118]], label %[[VAL_119:.*]], label %[[VAL_120:.*]]
// CHECK:       intra_warp_reduce_write-after:                    ; preds = %[[VAL_119]], %[[VAL_84]]
// CHECK:         call void @llvm.nvvm.barrier0()
// CHECK:         %[[VAL_121:.*]] = icmp eq i32 %[[VAL_117]], 0
// CHECK:         br i1 %[[VAL_121]], label %[[VAL_122:.*]], label %[[VAL_123:.*]]
// CHECK:       inter_warp_reduce-after:                          ; preds = %[[VAL_124:.*]], %[[VAL_120]]
// CHECK:         %[[VAL_125:.*]] = add i32 %[[VAL_100]], 0
// CHECK:         %[[VAL_126:.*]] = udiv i32 %[[VAL_99]], 1
// CHECK:         %[[VAL_127:.*]] = urem i32 %[[VAL_126]], 32
// CHECK:         %[[VAL_128:.*]] = udiv i32 %[[VAL_99]], 32
// CHECK:         %[[VAL_129:.*]] = getelementptr inbounds [2 x [32 x float]], [2 x [32 x float]]* %[[VAL_46]], i32 0, i32 %[[VAL_128]], i32 %[[VAL_127]]
// CHECK:         %[[VAL_130:.*]] = getelementptr inbounds float, float* %[[VAL_33]], i32 0
// CHECK:         %[[VAL_131:.*]] = load float, float* %[[VAL_130]], align 4
// CHECK:         %[[VAL_132:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_131]], i32 16, i32 31)
// CHECK:         store float %[[VAL_132]], float* %[[VAL_20]], align 4
// CHECK:         call void @region_2_9(float* %[[VAL_130]], float* %[[VAL_20]], float* %[[VAL_130]])
// CHECK:         %[[VAL_133:.*]] = load float, float* %[[VAL_130]], align 4
// CHECK:         %[[VAL_134:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_133]], i32 8, i32 31)
// CHECK:         store float %[[VAL_134]], float* %[[VAL_19]], align 4
// CHECK:         call void @region_2_9(float* %[[VAL_130]], float* %[[VAL_19]], float* %[[VAL_130]])
// CHECK:         %[[VAL_135:.*]] = load float, float* %[[VAL_130]], align 4
// CHECK:         %[[VAL_136:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_135]], i32 4, i32 31)
// CHECK:         store float %[[VAL_136]], float* %[[VAL_18]], align 4
// CHECK:         call void @region_2_9(float* %[[VAL_130]], float* %[[VAL_18]], float* %[[VAL_130]])
// CHECK:         %[[VAL_137:.*]] = load float, float* %[[VAL_130]], align 4
// CHECK:         %[[VAL_138:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_137]], i32 2, i32 31)
// CHECK:         store float %[[VAL_138]], float* %[[VAL_17]], align 4
// CHECK:         call void @region_2_9(float* %[[VAL_130]], float* %[[VAL_17]], float* %[[VAL_130]])
// CHECK:         %[[VAL_139:.*]] = load float, float* %[[VAL_130]], align 4
// CHECK:         %[[VAL_140:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_139]], i32 1, i32 31)
// CHECK:         store float %[[VAL_140]], float* %[[VAL_16]], align 4
// CHECK:         call void @region_2_9(float* %[[VAL_130]], float* %[[VAL_16]], float* %[[VAL_130]])
// CHECK:         %[[VAL_141:.*]] = udiv i32 %[[VAL_95]], 32
// CHECK:         %[[VAL_142:.*]] = icmp eq i32 %[[VAL_97]], 0
// CHECK:         br i1 %[[VAL_142]], label %[[VAL_143:.*]], label %[[VAL_144:.*]]
// CHECK:       intra_warp_reduce_write-after129:                  ; preds = %[[VAL_143]], %[[VAL_123]]
// CHECK:         call void @llvm.nvvm.barrier0()
// CHECK:         %[[VAL_145:.*]] = icmp eq i32 %[[VAL_141]], 0
// CHECK:         br i1 %[[VAL_145]], label %[[VAL_146:.*]], label %[[VAL_51]]
// CHECK:       inter_warp_reduce-after131:                        ; preds = %[[VAL_147:.*]], %[[VAL_144]]
// CHECK:         br label %[[VAL_50]]
// CHECK:       output_is_full_tile-true:                         ; preds = %[[VAL_85]]
// CHECK:         %[[VAL_148:.*]] = add i32 %[[VAL_72]], %[[VAL_89]]
// CHECK:         %[[VAL_149:.*]] = add i32 0, %[[VAL_74]]
// CHECK:         %[[VAL_150:.*]] = add i32 %[[VAL_75]], 0
// CHECK:         %[[VAL_151:.*]] = mul nuw nsw i32 %[[VAL_150]], 1
// CHECK:         %[[VAL_152:.*]] = add nuw nsw i32 0, %[[VAL_151]]
// CHECK:         %[[VAL_153:.*]] = mul nuw nsw i32 %[[VAL_148]], 32
// CHECK:         %[[VAL_154:.*]] = add nuw nsw i32 %[[VAL_152]], %[[VAL_153]]
// CHECK:         %[[VAL_155:.*]] = mul nuw nsw i32 %[[VAL_67]], 2048
// CHECK:         %[[VAL_156:.*]] = add nuw nsw i32 %[[VAL_154]], %[[VAL_155]]
// CHECK:         %[[VAL_157:.*]] = udiv i32 %[[VAL_156]], 1
// CHECK:         %[[VAL_158:.*]] = urem i32 %[[VAL_157]], 32
// CHECK:         %[[VAL_159:.*]] = udiv i32 %[[VAL_156]], 32
// CHECK:         %[[VAL_160:.*]] = urem i32 %[[VAL_159]], 32
// CHECK:         %[[VAL_161:.*]] = udiv i32 %[[VAL_156]], 1024
// CHECK:         %[[VAL_162:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_38]] to float*
// CHECK:         %[[VAL_163:.*]] = getelementptr inbounds float, float* %[[VAL_162]], i32 %[[VAL_156]]
// CHECK:         %[[VAL_164:.*]] = load float, float* %[[VAL_163]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_164]], float* %[[VAL_36]], align 4
// CHECK:         %[[VAL_165:.*]] = getelementptr inbounds float, float* %[[VAL_35]], i32 0
// CHECK:         call void @region_1_4(float* %[[VAL_165]], float* %[[VAL_36]], float* %[[VAL_165]])
// CHECK:         %[[VAL_166:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_38]] to float*
// CHECK:         %[[VAL_167:.*]] = getelementptr inbounds float, float* %[[VAL_166]], i32 %[[VAL_156]]
// CHECK:         %[[VAL_168:.*]] = load float, float* %[[VAL_167]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_168]], float* %[[VAL_34]], align 4
// CHECK:         %[[VAL_169:.*]] = getelementptr inbounds float, float* %[[VAL_33]], i32 0
// CHECK:         call void @region_2_9(float* %[[VAL_169]], float* %[[VAL_34]], float* %[[VAL_169]])
// CHECK:         %[[VAL_170:.*]] = add i32 1, %[[VAL_74]]
// CHECK:         %[[VAL_171:.*]] = add i32 %[[VAL_75]], 1
// CHECK:         %[[VAL_172:.*]] = mul nuw nsw i32 %[[VAL_171]], 1
// CHECK:         %[[VAL_173:.*]] = add nuw nsw i32 0, %[[VAL_172]]
// CHECK:         %[[VAL_174:.*]] = mul nuw nsw i32 %[[VAL_148]], 32
// CHECK:         %[[VAL_175:.*]] = add nuw nsw i32 %[[VAL_173]], %[[VAL_174]]
// CHECK:         %[[VAL_176:.*]] = mul nuw nsw i32 %[[VAL_67]], 2048
// CHECK:         %[[VAL_177:.*]] = add nuw nsw i32 %[[VAL_175]], %[[VAL_176]]
// CHECK:         %[[VAL_178:.*]] = udiv i32 %[[VAL_177]], 1
// CHECK:         %[[VAL_179:.*]] = urem i32 %[[VAL_178]], 32
// CHECK:         %[[VAL_180:.*]] = udiv i32 %[[VAL_177]], 32
// CHECK:         %[[VAL_181:.*]] = urem i32 %[[VAL_180]], 32
// CHECK:         %[[VAL_182:.*]] = udiv i32 %[[VAL_177]], 1024
// CHECK:         %[[VAL_183:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_38]] to float*
// CHECK:         %[[VAL_184:.*]] = getelementptr inbounds float, float* %[[VAL_183]], i32 %[[VAL_177]]
// CHECK:         %[[VAL_185:.*]] = load float, float* %[[VAL_184]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_185]], float* %[[VAL_36]], align 4
// CHECK:         %[[VAL_186:.*]] = getelementptr inbounds float, float* %[[VAL_35]], i32 0
// CHECK:         call void @region_1_4(float* %[[VAL_186]], float* %[[VAL_36]], float* %[[VAL_186]])
// CHECK:         %[[VAL_187:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_38]] to float*
// CHECK:         %[[VAL_188:.*]] = getelementptr inbounds float, float* %[[VAL_187]], i32 %[[VAL_177]]
// CHECK:         %[[VAL_189:.*]] = load float, float* %[[VAL_188]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_189]], float* %[[VAL_34]], align 4
// CHECK:         %[[VAL_190:.*]] = getelementptr inbounds float, float* %[[VAL_33]], i32 0
// CHECK:         call void @region_2_9(float* %[[VAL_190]], float* %[[VAL_34]], float* %[[VAL_190]])
// CHECK:         %[[VAL_191:.*]] = add i32 64, %[[VAL_74]]
// CHECK:         %[[VAL_192:.*]] = add i32 %[[VAL_75]], 64
// CHECK:         %[[VAL_193:.*]] = mul nuw nsw i32 %[[VAL_192]], 1
// CHECK:         %[[VAL_194:.*]] = add nuw nsw i32 0, %[[VAL_193]]
// CHECK:         %[[VAL_195:.*]] = mul nuw nsw i32 %[[VAL_148]], 32
// CHECK:         %[[VAL_196:.*]] = add nuw nsw i32 %[[VAL_194]], %[[VAL_195]]
// CHECK:         %[[VAL_197:.*]] = mul nuw nsw i32 %[[VAL_67]], 2048
// CHECK:         %[[VAL_198:.*]] = add nuw nsw i32 %[[VAL_196]], %[[VAL_197]]
// CHECK:         %[[VAL_199:.*]] = udiv i32 %[[VAL_198]], 1
// CHECK:         %[[VAL_200:.*]] = urem i32 %[[VAL_199]], 32
// CHECK:         %[[VAL_201:.*]] = udiv i32 %[[VAL_198]], 32
// CHECK:         %[[VAL_202:.*]] = urem i32 %[[VAL_201]], 32
// CHECK:         %[[VAL_203:.*]] = udiv i32 %[[VAL_198]], 1024
// CHECK:         %[[VAL_204:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_38]] to float*
// CHECK:         %[[VAL_205:.*]] = getelementptr inbounds float, float* %[[VAL_204]], i32 %[[VAL_198]]
// CHECK:         %[[VAL_206:.*]] = load float, float* %[[VAL_205]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_206]], float* %[[VAL_36]], align 4
// CHECK:         %[[VAL_207:.*]] = getelementptr inbounds float, float* %[[VAL_35]], i32 0
// CHECK:         call void @region_1_4(float* %[[VAL_207]], float* %[[VAL_36]], float* %[[VAL_207]])
// CHECK:         %[[VAL_208:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_38]] to float*
// CHECK:         %[[VAL_209:.*]] = getelementptr inbounds float, float* %[[VAL_208]], i32 %[[VAL_198]]
// CHECK:         %[[VAL_210:.*]] = load float, float* %[[VAL_209]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_210]], float* %[[VAL_34]], align 4
// CHECK:         %[[VAL_211:.*]] = getelementptr inbounds float, float* %[[VAL_33]], i32 0
// CHECK:         call void @region_2_9(float* %[[VAL_211]], float* %[[VAL_34]], float* %[[VAL_211]])
// CHECK:         %[[VAL_212:.*]] = add i32 65, %[[VAL_74]]
// CHECK:         %[[VAL_213:.*]] = add i32 %[[VAL_75]], 65
// CHECK:         %[[VAL_214:.*]] = mul nuw nsw i32 %[[VAL_213]], 1
// CHECK:         %[[VAL_215:.*]] = add nuw nsw i32 0, %[[VAL_214]]
// CHECK:         %[[VAL_216:.*]] = mul nuw nsw i32 %[[VAL_148]], 32
// CHECK:         %[[VAL_217:.*]] = add nuw nsw i32 %[[VAL_215]], %[[VAL_216]]
// CHECK:         %[[VAL_218:.*]] = mul nuw nsw i32 %[[VAL_67]], 2048
// CHECK:         %[[VAL_219:.*]] = add nuw nsw i32 %[[VAL_217]], %[[VAL_218]]
// CHECK:         %[[VAL_220:.*]] = udiv i32 %[[VAL_219]], 1
// CHECK:         %[[VAL_221:.*]] = urem i32 %[[VAL_220]], 32
// CHECK:         %[[VAL_222:.*]] = udiv i32 %[[VAL_219]], 32
// CHECK:         %[[VAL_223:.*]] = urem i32 %[[VAL_222]], 32
// CHECK:         %[[VAL_224:.*]] = udiv i32 %[[VAL_219]], 1024
// CHECK:         %[[VAL_225:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_38]] to float*
// CHECK:         %[[VAL_226:.*]] = getelementptr inbounds float, float* %[[VAL_225]], i32 %[[VAL_219]]
// CHECK:         %[[VAL_227:.*]] = load float, float* %[[VAL_226]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_227]], float* %[[VAL_36]], align 4
// CHECK:         %[[VAL_228:.*]] = getelementptr inbounds float, float* %[[VAL_35]], i32 0
// CHECK:         call void @region_1_4(float* %[[VAL_228]], float* %[[VAL_36]], float* %[[VAL_228]])
// CHECK:         %[[VAL_229:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_38]] to float*
// CHECK:         %[[VAL_230:.*]] = getelementptr inbounds float, float* %[[VAL_229]], i32 %[[VAL_219]]
// CHECK:         %[[VAL_231:.*]] = load float, float* %[[VAL_230]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_231]], float* %[[VAL_34]], align 4
// CHECK:         %[[VAL_232:.*]] = getelementptr inbounds float, float* %[[VAL_33]], i32 0
// CHECK:         call void @region_2_9(float* %[[VAL_232]], float* %[[VAL_34]], float* %[[VAL_232]])
// CHECK:         %[[VAL_233:.*]] = add i32 128, %[[VAL_74]]
// CHECK:         %[[VAL_234:.*]] = add i32 %[[VAL_75]], 128
// CHECK:         %[[VAL_235:.*]] = mul nuw nsw i32 %[[VAL_234]], 1
// CHECK:         %[[VAL_236:.*]] = add nuw nsw i32 0, %[[VAL_235]]
// CHECK:         %[[VAL_237:.*]] = mul nuw nsw i32 %[[VAL_148]], 32
// CHECK:         %[[VAL_238:.*]] = add nuw nsw i32 %[[VAL_236]], %[[VAL_237]]
// CHECK:         %[[VAL_239:.*]] = mul nuw nsw i32 %[[VAL_67]], 2048
// CHECK:         %[[VAL_240:.*]] = add nuw nsw i32 %[[VAL_238]], %[[VAL_239]]
// CHECK:         %[[VAL_241:.*]] = udiv i32 %[[VAL_240]], 1
// CHECK:         %[[VAL_242:.*]] = urem i32 %[[VAL_241]], 32
// CHECK:         %[[VAL_243:.*]] = udiv i32 %[[VAL_240]], 32
// CHECK:         %[[VAL_244:.*]] = urem i32 %[[VAL_243]], 32
// CHECK:         %[[VAL_245:.*]] = udiv i32 %[[VAL_240]], 1024
// CHECK:         %[[VAL_246:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_38]] to float*
// CHECK:         %[[VAL_247:.*]] = getelementptr inbounds float, float* %[[VAL_246]], i32 %[[VAL_240]]
// CHECK:         %[[VAL_248:.*]] = load float, float* %[[VAL_247]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_248]], float* %[[VAL_36]], align 4
// CHECK:         %[[VAL_249:.*]] = getelementptr inbounds float, float* %[[VAL_35]], i32 0
// CHECK:         call void @region_1_4(float* %[[VAL_249]], float* %[[VAL_36]], float* %[[VAL_249]])
// CHECK:         %[[VAL_250:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_38]] to float*
// CHECK:         %[[VAL_251:.*]] = getelementptr inbounds float, float* %[[VAL_250]], i32 %[[VAL_240]]
// CHECK:         %[[VAL_252:.*]] = load float, float* %[[VAL_251]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_252]], float* %[[VAL_34]], align 4
// CHECK:         %[[VAL_253:.*]] = getelementptr inbounds float, float* %[[VAL_33]], i32 0
// CHECK:         call void @region_2_9(float* %[[VAL_253]], float* %[[VAL_34]], float* %[[VAL_253]])
// CHECK:         %[[VAL_254:.*]] = add i32 129, %[[VAL_74]]
// CHECK:         %[[VAL_255:.*]] = add i32 %[[VAL_75]], 129
// CHECK:         %[[VAL_256:.*]] = mul nuw nsw i32 %[[VAL_255]], 1
// CHECK:         %[[VAL_257:.*]] = add nuw nsw i32 0, %[[VAL_256]]
// CHECK:         %[[VAL_258:.*]] = mul nuw nsw i32 %[[VAL_148]], 32
// CHECK:         %[[VAL_259:.*]] = add nuw nsw i32 %[[VAL_257]], %[[VAL_258]]
// CHECK:         %[[VAL_260:.*]] = mul nuw nsw i32 %[[VAL_67]], 2048
// CHECK:         %[[VAL_261:.*]] = add nuw nsw i32 %[[VAL_259]], %[[VAL_260]]
// CHECK:         %[[VAL_262:.*]] = udiv i32 %[[VAL_261]], 1
// CHECK:         %[[VAL_263:.*]] = urem i32 %[[VAL_262]], 32
// CHECK:         %[[VAL_264:.*]] = udiv i32 %[[VAL_261]], 32
// CHECK:         %[[VAL_265:.*]] = urem i32 %[[VAL_264]], 32
// CHECK:         %[[VAL_266:.*]] = udiv i32 %[[VAL_261]], 1024
// CHECK:         %[[VAL_267:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_38]] to float*
// CHECK:         %[[VAL_268:.*]] = getelementptr inbounds float, float* %[[VAL_267]], i32 %[[VAL_261]]
// CHECK:         %[[VAL_269:.*]] = load float, float* %[[VAL_268]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_269]], float* %[[VAL_36]], align 4
// CHECK:         %[[VAL_270:.*]] = getelementptr inbounds float, float* %[[VAL_35]], i32 0
// CHECK:         call void @region_1_4(float* %[[VAL_270]], float* %[[VAL_36]], float* %[[VAL_270]])
// CHECK:         %[[VAL_271:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_38]] to float*
// CHECK:         %[[VAL_272:.*]] = getelementptr inbounds float, float* %[[VAL_271]], i32 %[[VAL_261]]
// CHECK:         %[[VAL_273:.*]] = load float, float* %[[VAL_272]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_273]], float* %[[VAL_34]], align 4
// CHECK:         %[[VAL_274:.*]] = getelementptr inbounds float, float* %[[VAL_33]], i32 0
// CHECK:         call void @region_2_9(float* %[[VAL_274]], float* %[[VAL_34]], float* %[[VAL_274]])
// CHECK:         %[[VAL_275:.*]] = add i32 192, %[[VAL_74]]
// CHECK:         %[[VAL_276:.*]] = add i32 %[[VAL_75]], 192
// CHECK:         %[[VAL_277:.*]] = mul nuw nsw i32 %[[VAL_276]], 1
// CHECK:         %[[VAL_278:.*]] = add nuw nsw i32 0, %[[VAL_277]]
// CHECK:         %[[VAL_279:.*]] = mul nuw nsw i32 %[[VAL_148]], 32
// CHECK:         %[[VAL_280:.*]] = add nuw nsw i32 %[[VAL_278]], %[[VAL_279]]
// CHECK:         %[[VAL_281:.*]] = mul nuw nsw i32 %[[VAL_67]], 2048
// CHECK:         %[[VAL_282:.*]] = add nuw nsw i32 %[[VAL_280]], %[[VAL_281]]
// CHECK:         %[[VAL_283:.*]] = udiv i32 %[[VAL_282]], 1
// CHECK:         %[[VAL_284:.*]] = urem i32 %[[VAL_283]], 32
// CHECK:         %[[VAL_285:.*]] = udiv i32 %[[VAL_282]], 32
// CHECK:         %[[VAL_286:.*]] = urem i32 %[[VAL_285]], 32
// CHECK:         %[[VAL_287:.*]] = udiv i32 %[[VAL_282]], 1024
// CHECK:         %[[VAL_288:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_38]] to float*
// CHECK:         %[[VAL_289:.*]] = getelementptr inbounds float, float* %[[VAL_288]], i32 %[[VAL_282]]
// CHECK:         %[[VAL_290:.*]] = load float, float* %[[VAL_289]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_290]], float* %[[VAL_36]], align 4
// CHECK:         %[[VAL_291:.*]] = getelementptr inbounds float, float* %[[VAL_35]], i32 0
// CHECK:         call void @region_1_4(float* %[[VAL_291]], float* %[[VAL_36]], float* %[[VAL_291]])
// CHECK:         %[[VAL_292:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_38]] to float*
// CHECK:         %[[VAL_293:.*]] = getelementptr inbounds float, float* %[[VAL_292]], i32 %[[VAL_282]]
// CHECK:         %[[VAL_294:.*]] = load float, float* %[[VAL_293]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_294]], float* %[[VAL_34]], align 4
// CHECK:         %[[VAL_295:.*]] = getelementptr inbounds float, float* %[[VAL_33]], i32 0
// CHECK:         call void @region_2_9(float* %[[VAL_295]], float* %[[VAL_34]], float* %[[VAL_295]])
// CHECK:         %[[VAL_296:.*]] = add i32 193, %[[VAL_74]]
// CHECK:         %[[VAL_297:.*]] = add i32 %[[VAL_75]], 193
// CHECK:         %[[VAL_298:.*]] = mul nuw nsw i32 %[[VAL_297]], 1
// CHECK:         %[[VAL_299:.*]] = add nuw nsw i32 0, %[[VAL_298]]
// CHECK:         %[[VAL_300:.*]] = mul nuw nsw i32 %[[VAL_148]], 32
// CHECK:         %[[VAL_301:.*]] = add nuw nsw i32 %[[VAL_299]], %[[VAL_300]]
// CHECK:         %[[VAL_302:.*]] = mul nuw nsw i32 %[[VAL_67]], 2048
// CHECK:         %[[VAL_303:.*]] = add nuw nsw i32 %[[VAL_301]], %[[VAL_302]]
// CHECK:         %[[VAL_304:.*]] = udiv i32 %[[VAL_303]], 1
// CHECK:         %[[VAL_305:.*]] = urem i32 %[[VAL_304]], 32
// CHECK:         %[[VAL_306:.*]] = udiv i32 %[[VAL_303]], 32
// CHECK:         %[[VAL_307:.*]] = urem i32 %[[VAL_306]], 32
// CHECK:         %[[VAL_308:.*]] = udiv i32 %[[VAL_303]], 1024
// CHECK:         %[[VAL_309:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_38]] to float*
// CHECK:         %[[VAL_310:.*]] = getelementptr inbounds float, float* %[[VAL_309]], i32 %[[VAL_303]]
// CHECK:         %[[VAL_311:.*]] = load float, float* %[[VAL_310]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_311]], float* %[[VAL_36]], align 4
// CHECK:         %[[VAL_312:.*]] = getelementptr inbounds float, float* %[[VAL_35]], i32 0
// CHECK:         call void @region_1_4(float* %[[VAL_312]], float* %[[VAL_36]], float* %[[VAL_312]])
// CHECK:         %[[VAL_313:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_38]] to float*
// CHECK:         %[[VAL_314:.*]] = getelementptr inbounds float, float* %[[VAL_313]], i32 %[[VAL_303]]
// CHECK:         %[[VAL_315:.*]] = load float, float* %[[VAL_314]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_315]], float* %[[VAL_34]], align 4
// CHECK:         %[[VAL_316:.*]] = getelementptr inbounds float, float* %[[VAL_33]], i32 0
// CHECK:         call void @region_2_9(float* %[[VAL_316]], float* %[[VAL_34]], float* %[[VAL_316]])
// CHECK:         br label %[[VAL_81]]
// CHECK:       output_is_full_tile-false:                        ; preds = %[[VAL_85]]
// CHECK:         %[[VAL_317:.*]] = add i32 %[[VAL_72]], %[[VAL_89]]
// CHECK:         %[[VAL_318:.*]] = add i32 0, %[[VAL_74]]
// CHECK:         %[[VAL_319:.*]] = add i32 %[[VAL_75]], 0
// CHECK:         %[[VAL_320:.*]] = icmp ult i32 %[[VAL_318]], %[[VAL_71]]
// CHECK:         br i1 %[[VAL_320]], label %[[VAL_321:.*]], label %[[VAL_322:.*]]
// CHECK:       output_x_in_tile-after:                           ; preds = %[[VAL_321]], %[[VAL_92]]
// CHECK:         %[[VAL_323:.*]] = add i32 1, %[[VAL_74]]
// CHECK:         %[[VAL_324:.*]] = add i32 %[[VAL_75]], 1
// CHECK:         %[[VAL_325:.*]] = icmp ult i32 %[[VAL_323]], %[[VAL_71]]
// CHECK:         br i1 %[[VAL_325]], label %[[VAL_326:.*]], label %[[VAL_327:.*]]
// CHECK:       output_x_in_tile-after48:                         ; preds = %[[VAL_326]], %[[VAL_322]]
// CHECK:         %[[VAL_328:.*]] = add i32 64, %[[VAL_74]]
// CHECK:         %[[VAL_329:.*]] = add i32 %[[VAL_75]], 64
// CHECK:         %[[VAL_330:.*]] = icmp ult i32 %[[VAL_328]], %[[VAL_71]]
// CHECK:         br i1 %[[VAL_330]], label %[[VAL_331:.*]], label %[[VAL_332:.*]]
// CHECK:       output_x_in_tile-after55:                         ; preds = %[[VAL_331]], %[[VAL_327]]
// CHECK:         %[[VAL_333:.*]] = add i32 65, %[[VAL_74]]
// CHECK:         %[[VAL_334:.*]] = add i32 %[[VAL_75]], 65
// CHECK:         %[[VAL_335:.*]] = icmp ult i32 %[[VAL_333]], %[[VAL_71]]
// CHECK:         br i1 %[[VAL_335]], label %[[VAL_336:.*]], label %[[VAL_337:.*]]
// CHECK:       output_x_in_tile-after62:                         ; preds = %[[VAL_336]], %[[VAL_332]]
// CHECK:         %[[VAL_338:.*]] = add i32 128, %[[VAL_74]]
// CHECK:         %[[VAL_339:.*]] = add i32 %[[VAL_75]], 128
// CHECK:         %[[VAL_340:.*]] = icmp ult i32 %[[VAL_338]], %[[VAL_71]]
// CHECK:         br i1 %[[VAL_340]], label %[[VAL_341:.*]], label %[[VAL_342:.*]]
// CHECK:       output_x_in_tile-after69:                         ; preds = %[[VAL_341]], %[[VAL_337]]
// CHECK:         %[[VAL_343:.*]] = add i32 129, %[[VAL_74]]
// CHECK:         %[[VAL_344:.*]] = add i32 %[[VAL_75]], 129
// CHECK:         %[[VAL_345:.*]] = icmp ult i32 %[[VAL_343]], %[[VAL_71]]
// CHECK:         br i1 %[[VAL_345]], label %[[VAL_346:.*]], label %[[VAL_347:.*]]
// CHECK:       output_x_in_tile-after76:                         ; preds = %[[VAL_346]], %[[VAL_342]]
// CHECK:         %[[VAL_348:.*]] = add i32 192, %[[VAL_74]]
// CHECK:         %[[VAL_349:.*]] = add i32 %[[VAL_75]], 192
// CHECK:         %[[VAL_350:.*]] = icmp ult i32 %[[VAL_348]], %[[VAL_71]]
// CHECK:         br i1 %[[VAL_350]], label %[[VAL_351:.*]], label %[[VAL_352:.*]]
// CHECK:       output_x_in_tile-after83:                         ; preds = %[[VAL_351]], %[[VAL_347]]
// CHECK:         %[[VAL_353:.*]] = add i32 193, %[[VAL_74]]
// CHECK:         %[[VAL_354:.*]] = add i32 %[[VAL_75]], 193
// CHECK:         %[[VAL_355:.*]] = icmp ult i32 %[[VAL_353]], %[[VAL_71]]
// CHECK:         br i1 %[[VAL_355]], label %[[VAL_356:.*]], label %[[VAL_93]]
// CHECK:       output_x_in_tile-after90:                         ; preds = %[[VAL_356]], %[[VAL_352]]
// CHECK:         br label %[[VAL_81]]
// CHECK:       output_x_in_tile-true:                            ; preds = %[[VAL_92]]
// CHECK:         %[[VAL_357:.*]] = mul nuw nsw i32 %[[VAL_319]], 1
// CHECK:         %[[VAL_358:.*]] = add nuw nsw i32 0, %[[VAL_357]]
// CHECK:         %[[VAL_359:.*]] = mul nuw nsw i32 %[[VAL_317]], 32
// CHECK:         %[[VAL_360:.*]] = add nuw nsw i32 %[[VAL_358]], %[[VAL_359]]
// CHECK:         %[[VAL_361:.*]] = mul nuw nsw i32 %[[VAL_67]], 2048
// CHECK:         %[[VAL_362:.*]] = add nuw nsw i32 %[[VAL_360]], %[[VAL_361]]
// CHECK:         %[[VAL_363:.*]] = udiv i32 %[[VAL_362]], 1
// CHECK:         %[[VAL_364:.*]] = urem i32 %[[VAL_363]], 32
// CHECK:         %[[VAL_365:.*]] = udiv i32 %[[VAL_362]], 32
// CHECK:         %[[VAL_366:.*]] = urem i32 %[[VAL_365]], 32
// CHECK:         %[[VAL_367:.*]] = udiv i32 %[[VAL_362]], 1024
// CHECK:         %[[VAL_368:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_38]] to float*
// CHECK:         %[[VAL_369:.*]] = getelementptr inbounds float, float* %[[VAL_368]], i32 %[[VAL_362]]
// CHECK:         %[[VAL_370:.*]] = load float, float* %[[VAL_369]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_370]], float* %[[VAL_36]], align 4
// CHECK:         %[[VAL_371:.*]] = getelementptr inbounds float, float* %[[VAL_35]], i32 0
// CHECK:         call void @region_1_4(float* %[[VAL_371]], float* %[[VAL_36]], float* %[[VAL_371]])
// CHECK:         %[[VAL_372:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_38]] to float*
// CHECK:         %[[VAL_373:.*]] = getelementptr inbounds float, float* %[[VAL_372]], i32 %[[VAL_362]]
// CHECK:         %[[VAL_374:.*]] = load float, float* %[[VAL_373]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_374]], float* %[[VAL_34]], align 4
// CHECK:         %[[VAL_375:.*]] = getelementptr inbounds float, float* %[[VAL_33]], i32 0
// CHECK:         call void @region_2_9(float* %[[VAL_375]], float* %[[VAL_34]], float* %[[VAL_375]])
// CHECK:         br label %[[VAL_322]]
// CHECK:       output_x_in_tile-true47:                          ; preds = %[[VAL_322]]
// CHECK:         %[[VAL_376:.*]] = mul nuw nsw i32 %[[VAL_324]], 1
// CHECK:         %[[VAL_377:.*]] = add nuw nsw i32 0, %[[VAL_376]]
// CHECK:         %[[VAL_378:.*]] = mul nuw nsw i32 %[[VAL_317]], 32
// CHECK:         %[[VAL_379:.*]] = add nuw nsw i32 %[[VAL_377]], %[[VAL_378]]
// CHECK:         %[[VAL_380:.*]] = mul nuw nsw i32 %[[VAL_67]], 2048
// CHECK:         %[[VAL_381:.*]] = add nuw nsw i32 %[[VAL_379]], %[[VAL_380]]
// CHECK:         %[[VAL_382:.*]] = udiv i32 %[[VAL_381]], 1
// CHECK:         %[[VAL_383:.*]] = urem i32 %[[VAL_382]], 32
// CHECK:         %[[VAL_384:.*]] = udiv i32 %[[VAL_381]], 32
// CHECK:         %[[VAL_385:.*]] = urem i32 %[[VAL_384]], 32
// CHECK:         %[[VAL_386:.*]] = udiv i32 %[[VAL_381]], 1024
// CHECK:         %[[VAL_387:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_38]] to float*
// CHECK:         %[[VAL_388:.*]] = getelementptr inbounds float, float* %[[VAL_387]], i32 %[[VAL_381]]
// CHECK:         %[[VAL_389:.*]] = load float, float* %[[VAL_388]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_389]], float* %[[VAL_36]], align 4
// CHECK:         %[[VAL_390:.*]] = getelementptr inbounds float, float* %[[VAL_35]], i32 0
// CHECK:         call void @region_1_4(float* %[[VAL_390]], float* %[[VAL_36]], float* %[[VAL_390]])
// CHECK:         %[[VAL_391:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_38]] to float*
// CHECK:         %[[VAL_392:.*]] = getelementptr inbounds float, float* %[[VAL_391]], i32 %[[VAL_381]]
// CHECK:         %[[VAL_393:.*]] = load float, float* %[[VAL_392]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_393]], float* %[[VAL_34]], align 4
// CHECK:         %[[VAL_394:.*]] = getelementptr inbounds float, float* %[[VAL_33]], i32 0
// CHECK:         call void @region_2_9(float* %[[VAL_394]], float* %[[VAL_34]], float* %[[VAL_394]])
// CHECK:         br label %[[VAL_327]]
// CHECK:       output_x_in_tile-true54:                          ; preds = %[[VAL_327]]
// CHECK:         %[[VAL_395:.*]] = mul nuw nsw i32 %[[VAL_329]], 1
// CHECK:         %[[VAL_396:.*]] = add nuw nsw i32 0, %[[VAL_395]]
// CHECK:         %[[VAL_397:.*]] = mul nuw nsw i32 %[[VAL_317]], 32
// CHECK:         %[[VAL_398:.*]] = add nuw nsw i32 %[[VAL_396]], %[[VAL_397]]
// CHECK:         %[[VAL_399:.*]] = mul nuw nsw i32 %[[VAL_67]], 2048
// CHECK:         %[[VAL_400:.*]] = add nuw nsw i32 %[[VAL_398]], %[[VAL_399]]
// CHECK:         %[[VAL_401:.*]] = udiv i32 %[[VAL_400]], 1
// CHECK:         %[[VAL_402:.*]] = urem i32 %[[VAL_401]], 32
// CHECK:         %[[VAL_403:.*]] = udiv i32 %[[VAL_400]], 32
// CHECK:         %[[VAL_404:.*]] = urem i32 %[[VAL_403]], 32
// CHECK:         %[[VAL_405:.*]] = udiv i32 %[[VAL_400]], 1024
// CHECK:         %[[VAL_406:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_38]] to float*
// CHECK:         %[[VAL_407:.*]] = getelementptr inbounds float, float* %[[VAL_406]], i32 %[[VAL_400]]
// CHECK:         %[[VAL_408:.*]] = load float, float* %[[VAL_407]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_408]], float* %[[VAL_36]], align 4
// CHECK:         %[[VAL_409:.*]] = getelementptr inbounds float, float* %[[VAL_35]], i32 0
// CHECK:         call void @region_1_4(float* %[[VAL_409]], float* %[[VAL_36]], float* %[[VAL_409]])
// CHECK:         %[[VAL_410:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_38]] to float*
// CHECK:         %[[VAL_411:.*]] = getelementptr inbounds float, float* %[[VAL_410]], i32 %[[VAL_400]]
// CHECK:         %[[VAL_412:.*]] = load float, float* %[[VAL_411]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_412]], float* %[[VAL_34]], align 4
// CHECK:         %[[VAL_413:.*]] = getelementptr inbounds float, float* %[[VAL_33]], i32 0
// CHECK:         call void @region_2_9(float* %[[VAL_413]], float* %[[VAL_34]], float* %[[VAL_413]])
// CHECK:         br label %[[VAL_332]]
// CHECK:       output_x_in_tile-true61:                          ; preds = %[[VAL_332]]
// CHECK:         %[[VAL_414:.*]] = mul nuw nsw i32 %[[VAL_334]], 1
// CHECK:         %[[VAL_415:.*]] = add nuw nsw i32 0, %[[VAL_414]]
// CHECK:         %[[VAL_416:.*]] = mul nuw nsw i32 %[[VAL_317]], 32
// CHECK:         %[[VAL_417:.*]] = add nuw nsw i32 %[[VAL_415]], %[[VAL_416]]
// CHECK:         %[[VAL_418:.*]] = mul nuw nsw i32 %[[VAL_67]], 2048
// CHECK:         %[[VAL_419:.*]] = add nuw nsw i32 %[[VAL_417]], %[[VAL_418]]
// CHECK:         %[[VAL_420:.*]] = udiv i32 %[[VAL_419]], 1
// CHECK:         %[[VAL_421:.*]] = urem i32 %[[VAL_420]], 32
// CHECK:         %[[VAL_422:.*]] = udiv i32 %[[VAL_419]], 32
// CHECK:         %[[VAL_423:.*]] = urem i32 %[[VAL_422]], 32
// CHECK:         %[[VAL_424:.*]] = udiv i32 %[[VAL_419]], 1024
// CHECK:         %[[VAL_425:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_38]] to float*
// CHECK:         %[[VAL_426:.*]] = getelementptr inbounds float, float* %[[VAL_425]], i32 %[[VAL_419]]
// CHECK:         %[[VAL_427:.*]] = load float, float* %[[VAL_426]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_427]], float* %[[VAL_36]], align 4
// CHECK:         %[[VAL_428:.*]] = getelementptr inbounds float, float* %[[VAL_35]], i32 0
// CHECK:         call void @region_1_4(float* %[[VAL_428]], float* %[[VAL_36]], float* %[[VAL_428]])
// CHECK:         %[[VAL_429:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_38]] to float*
// CHECK:         %[[VAL_430:.*]] = getelementptr inbounds float, float* %[[VAL_429]], i32 %[[VAL_419]]
// CHECK:         %[[VAL_431:.*]] = load float, float* %[[VAL_430]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_431]], float* %[[VAL_34]], align 4
// CHECK:         %[[VAL_432:.*]] = getelementptr inbounds float, float* %[[VAL_33]], i32 0
// CHECK:         call void @region_2_9(float* %[[VAL_432]], float* %[[VAL_34]], float* %[[VAL_432]])
// CHECK:         br label %[[VAL_337]]
// CHECK:       output_x_in_tile-true68:                          ; preds = %[[VAL_337]]
// CHECK:         %[[VAL_433:.*]] = mul nuw nsw i32 %[[VAL_339]], 1
// CHECK:         %[[VAL_434:.*]] = add nuw nsw i32 0, %[[VAL_433]]
// CHECK:         %[[VAL_435:.*]] = mul nuw nsw i32 %[[VAL_317]], 32
// CHECK:         %[[VAL_436:.*]] = add nuw nsw i32 %[[VAL_434]], %[[VAL_435]]
// CHECK:         %[[VAL_437:.*]] = mul nuw nsw i32 %[[VAL_67]], 2048
// CHECK:         %[[VAL_438:.*]] = add nuw nsw i32 %[[VAL_436]], %[[VAL_437]]
// CHECK:         %[[VAL_439:.*]] = udiv i32 %[[VAL_438]], 1
// CHECK:         %[[VAL_440:.*]] = urem i32 %[[VAL_439]], 32
// CHECK:         %[[VAL_441:.*]] = udiv i32 %[[VAL_438]], 32
// CHECK:         %[[VAL_442:.*]] = urem i32 %[[VAL_441]], 32
// CHECK:         %[[VAL_443:.*]] = udiv i32 %[[VAL_438]], 1024
// CHECK:         %[[VAL_444:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_38]] to float*
// CHECK:         %[[VAL_445:.*]] = getelementptr inbounds float, float* %[[VAL_444]], i32 %[[VAL_438]]
// CHECK:         %[[VAL_446:.*]] = load float, float* %[[VAL_445]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_446]], float* %[[VAL_36]], align 4
// CHECK:         %[[VAL_447:.*]] = getelementptr inbounds float, float* %[[VAL_35]], i32 0
// CHECK:         call void @region_1_4(float* %[[VAL_447]], float* %[[VAL_36]], float* %[[VAL_447]])
// CHECK:         %[[VAL_448:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_38]] to float*
// CHECK:         %[[VAL_449:.*]] = getelementptr inbounds float, float* %[[VAL_448]], i32 %[[VAL_438]]
// CHECK:         %[[VAL_450:.*]] = load float, float* %[[VAL_449]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_450]], float* %[[VAL_34]], align 4
// CHECK:         %[[VAL_451:.*]] = getelementptr inbounds float, float* %[[VAL_33]], i32 0
// CHECK:         call void @region_2_9(float* %[[VAL_451]], float* %[[VAL_34]], float* %[[VAL_451]])
// CHECK:         br label %[[VAL_342]]
// CHECK:       output_x_in_tile-true75:                          ; preds = %[[VAL_342]]
// CHECK:         %[[VAL_452:.*]] = mul nuw nsw i32 %[[VAL_344]], 1
// CHECK:         %[[VAL_453:.*]] = add nuw nsw i32 0, %[[VAL_452]]
// CHECK:         %[[VAL_454:.*]] = mul nuw nsw i32 %[[VAL_317]], 32
// CHECK:         %[[VAL_455:.*]] = add nuw nsw i32 %[[VAL_453]], %[[VAL_454]]
// CHECK:         %[[VAL_456:.*]] = mul nuw nsw i32 %[[VAL_67]], 2048
// CHECK:         %[[VAL_457:.*]] = add nuw nsw i32 %[[VAL_455]], %[[VAL_456]]
// CHECK:         %[[VAL_458:.*]] = udiv i32 %[[VAL_457]], 1
// CHECK:         %[[VAL_459:.*]] = urem i32 %[[VAL_458]], 32
// CHECK:         %[[VAL_460:.*]] = udiv i32 %[[VAL_457]], 32
// CHECK:         %[[VAL_461:.*]] = urem i32 %[[VAL_460]], 32
// CHECK:         %[[VAL_462:.*]] = udiv i32 %[[VAL_457]], 1024
// CHECK:         %[[VAL_463:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_38]] to float*
// CHECK:         %[[VAL_464:.*]] = getelementptr inbounds float, float* %[[VAL_463]], i32 %[[VAL_457]]
// CHECK:         %[[VAL_465:.*]] = load float, float* %[[VAL_464]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_465]], float* %[[VAL_36]], align 4
// CHECK:         %[[VAL_466:.*]] = getelementptr inbounds float, float* %[[VAL_35]], i32 0
// CHECK:         call void @region_1_4(float* %[[VAL_466]], float* %[[VAL_36]], float* %[[VAL_466]])
// CHECK:         %[[VAL_467:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_38]] to float*
// CHECK:         %[[VAL_468:.*]] = getelementptr inbounds float, float* %[[VAL_467]], i32 %[[VAL_457]]
// CHECK:         %[[VAL_469:.*]] = load float, float* %[[VAL_468]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_469]], float* %[[VAL_34]], align 4
// CHECK:         %[[VAL_470:.*]] = getelementptr inbounds float, float* %[[VAL_33]], i32 0
// CHECK:         call void @region_2_9(float* %[[VAL_470]], float* %[[VAL_34]], float* %[[VAL_470]])
// CHECK:         br label %[[VAL_347]]
// CHECK:       output_x_in_tile-true82:                          ; preds = %[[VAL_347]]
// CHECK:         %[[VAL_471:.*]] = mul nuw nsw i32 %[[VAL_349]], 1
// CHECK:         %[[VAL_472:.*]] = add nuw nsw i32 0, %[[VAL_471]]
// CHECK:         %[[VAL_473:.*]] = mul nuw nsw i32 %[[VAL_317]], 32
// CHECK:         %[[VAL_474:.*]] = add nuw nsw i32 %[[VAL_472]], %[[VAL_473]]
// CHECK:         %[[VAL_475:.*]] = mul nuw nsw i32 %[[VAL_67]], 2048
// CHECK:         %[[VAL_476:.*]] = add nuw nsw i32 %[[VAL_474]], %[[VAL_475]]
// CHECK:         %[[VAL_477:.*]] = udiv i32 %[[VAL_476]], 1
// CHECK:         %[[VAL_478:.*]] = urem i32 %[[VAL_477]], 32
// CHECK:         %[[VAL_479:.*]] = udiv i32 %[[VAL_476]], 32
// CHECK:         %[[VAL_480:.*]] = urem i32 %[[VAL_479]], 32
// CHECK:         %[[VAL_481:.*]] = udiv i32 %[[VAL_476]], 1024
// CHECK:         %[[VAL_482:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_38]] to float*
// CHECK:         %[[VAL_483:.*]] = getelementptr inbounds float, float* %[[VAL_482]], i32 %[[VAL_476]]
// CHECK:         %[[VAL_484:.*]] = load float, float* %[[VAL_483]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_484]], float* %[[VAL_36]], align 4
// CHECK:         %[[VAL_485:.*]] = getelementptr inbounds float, float* %[[VAL_35]], i32 0
// CHECK:         call void @region_1_4(float* %[[VAL_485]], float* %[[VAL_36]], float* %[[VAL_485]])
// CHECK:         %[[VAL_486:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_38]] to float*
// CHECK:         %[[VAL_487:.*]] = getelementptr inbounds float, float* %[[VAL_486]], i32 %[[VAL_476]]
// CHECK:         %[[VAL_488:.*]] = load float, float* %[[VAL_487]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_488]], float* %[[VAL_34]], align 4
// CHECK:         %[[VAL_489:.*]] = getelementptr inbounds float, float* %[[VAL_33]], i32 0
// CHECK:         call void @region_2_9(float* %[[VAL_489]], float* %[[VAL_34]], float* %[[VAL_489]])
// CHECK:         br label %[[VAL_352]]
// CHECK:       output_x_in_tile-true89:                          ; preds = %[[VAL_352]]
// CHECK:         %[[VAL_490:.*]] = mul nuw nsw i32 %[[VAL_354]], 1
// CHECK:         %[[VAL_491:.*]] = add nuw nsw i32 0, %[[VAL_490]]
// CHECK:         %[[VAL_492:.*]] = mul nuw nsw i32 %[[VAL_317]], 32
// CHECK:         %[[VAL_493:.*]] = add nuw nsw i32 %[[VAL_491]], %[[VAL_492]]
// CHECK:         %[[VAL_494:.*]] = mul nuw nsw i32 %[[VAL_67]], 2048
// CHECK:         %[[VAL_495:.*]] = add nuw nsw i32 %[[VAL_493]], %[[VAL_494]]
// CHECK:         %[[VAL_496:.*]] = udiv i32 %[[VAL_495]], 1
// CHECK:         %[[VAL_497:.*]] = urem i32 %[[VAL_496]], 32
// CHECK:         %[[VAL_498:.*]] = udiv i32 %[[VAL_495]], 32
// CHECK:         %[[VAL_499:.*]] = urem i32 %[[VAL_498]], 32
// CHECK:         %[[VAL_500:.*]] = udiv i32 %[[VAL_495]], 1024
// CHECK:         %[[VAL_501:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_38]] to float*
// CHECK:         %[[VAL_502:.*]] = getelementptr inbounds float, float* %[[VAL_501]], i32 %[[VAL_495]]
// CHECK:         %[[VAL_503:.*]] = load float, float* %[[VAL_502]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_503]], float* %[[VAL_36]], align 4
// CHECK:         %[[VAL_504:.*]] = getelementptr inbounds float, float* %[[VAL_35]], i32 0
// CHECK:         call void @region_1_4(float* %[[VAL_504]], float* %[[VAL_36]], float* %[[VAL_504]])
// CHECK:         %[[VAL_505:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_38]] to float*
// CHECK:         %[[VAL_506:.*]] = getelementptr inbounds float, float* %[[VAL_505]], i32 %[[VAL_495]]
// CHECK:         %[[VAL_507:.*]] = load float, float* %[[VAL_506]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_507]], float* %[[VAL_34]], align 4
// CHECK:         %[[VAL_508:.*]] = getelementptr inbounds float, float* %[[VAL_33]], i32 0
// CHECK:         call void @region_2_9(float* %[[VAL_508]], float* %[[VAL_34]], float* %[[VAL_508]])
// CHECK:         br label %[[VAL_93]]
// CHECK:       intra_warp_reduce_write-true:                     ; preds = %[[VAL_84]]
// CHECK:         %[[VAL_509:.*]] = getelementptr inbounds [1 x [32 x float]], [1 x [32 x float]] addrspace(3)* @shared_cache_0, i32 0, i32 0, i32 %[[VAL_117]]
// CHECK:         %[[VAL_510:.*]] = addrspacecast float addrspace(3)* %[[VAL_509]] to float*
// CHECK:         %[[VAL_511:.*]] = load float, float* %[[VAL_106]], align 4
// CHECK:         store float %[[VAL_511]], float* %[[VAL_510]], align 4
// CHECK:         br label %[[VAL_120]]
// CHECK:       inter_warp_reduce-true:                           ; preds = %[[VAL_120]]
// CHECK:         %[[VAL_512:.*]] = getelementptr inbounds [1 x [32 x float]], [1 x [32 x float]] addrspace(3)* @shared_cache_0, i32 0, i32 0, i32 %[[VAL_97]]
// CHECK:         %[[VAL_513:.*]] = addrspacecast float addrspace(3)* %[[VAL_512]] to float*
// CHECK:         store float %[[VAL_53]], float* %[[VAL_26]], align 4
// CHECK:         %[[VAL_514:.*]] = icmp ult i32 %[[VAL_95]], 1
// CHECK:         %[[VAL_515:.*]] = select i1 %[[VAL_514]], float* %[[VAL_513]], float* %[[VAL_26]]
// CHECK:         %[[VAL_516:.*]] = load float, float* %[[VAL_515]], align 4
// CHECK:         %[[VAL_517:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_516]], i32 16, i32 31)
// CHECK:         store float %[[VAL_517]], float* %[[VAL_25]], align 4
// CHECK:         call void @region_1_4(float* %[[VAL_515]], float* %[[VAL_25]], float* %[[VAL_515]])
// CHECK:         %[[VAL_518:.*]] = load float, float* %[[VAL_515]], align 4
// CHECK:         %[[VAL_519:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_518]], i32 8, i32 31)
// CHECK:         store float %[[VAL_519]], float* %[[VAL_24]], align 4
// CHECK:         call void @region_1_4(float* %[[VAL_515]], float* %[[VAL_24]], float* %[[VAL_515]])
// CHECK:         %[[VAL_520:.*]] = load float, float* %[[VAL_515]], align 4
// CHECK:         %[[VAL_521:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_520]], i32 4, i32 31)
// CHECK:         store float %[[VAL_521]], float* %[[VAL_23]], align 4
// CHECK:         call void @region_1_4(float* %[[VAL_515]], float* %[[VAL_23]], float* %[[VAL_515]])
// CHECK:         %[[VAL_522:.*]] = load float, float* %[[VAL_515]], align 4
// CHECK:         %[[VAL_523:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_522]], i32 2, i32 31)
// CHECK:         store float %[[VAL_523]], float* %[[VAL_22]], align 4
// CHECK:         call void @region_1_4(float* %[[VAL_515]], float* %[[VAL_22]], float* %[[VAL_515]])
// CHECK:         %[[VAL_524:.*]] = load float, float* %[[VAL_515]], align 4
// CHECK:         %[[VAL_525:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_524]], i32 1, i32 31)
// CHECK:         store float %[[VAL_525]], float* %[[VAL_21]], align 4
// CHECK:         call void @region_1_4(float* %[[VAL_515]], float* %[[VAL_21]], float* %[[VAL_515]])
// CHECK:         %[[VAL_526:.*]] = icmp eq i32 %[[VAL_95]], 0
// CHECK:         br i1 %[[VAL_526]], label %[[VAL_527:.*]], label %[[VAL_124]]
// CHECK:       reduction_atomic_update-after:                    ; preds = %[[VAL_527]], %[[VAL_122]]
// CHECK:         br label %[[VAL_123]]
// CHECK:       reduction_atomic_update-true:                     ; preds = %[[VAL_122]]
// CHECK:         %[[VAL_528:.*]] = load float, float* %[[VAL_513]], align 4
// CHECK:         %[[VAL_529:.*]] = atomicrmw fadd float* %[[VAL_105]], float %[[VAL_528]] seq_cst
// CHECK:         br label %[[VAL_124]]
// CHECK:       intra_warp_reduce_write-true128:                   ; preds = %[[VAL_123]]
// CHECK:         %[[VAL_530:.*]] = getelementptr inbounds [1 x [32 x float]], [1 x [32 x float]] addrspace(3)* @shared_cache_1, i32 0, i32 0, i32 %[[VAL_141]]
// CHECK:         %[[VAL_531:.*]] = addrspacecast float addrspace(3)* %[[VAL_530]] to float*
// CHECK:         %[[VAL_532:.*]] = load float, float* %[[VAL_130]], align 4
// CHECK:         store float %[[VAL_532]], float* %[[VAL_531]], align 4
// CHECK:         br label %[[VAL_144]]
// CHECK:       inter_warp_reduce-true130:                         ; preds = %[[VAL_144]]
// CHECK:         %[[VAL_533:.*]] = getelementptr inbounds [1 x [32 x float]], [1 x [32 x float]] addrspace(3)* @shared_cache_1, i32 0, i32 0, i32 %[[VAL_97]]
// CHECK:         %[[VAL_534:.*]] = addrspacecast float addrspace(3)* %[[VAL_533]] to float*
// CHECK:         store float %[[VAL_55]], float* %[[VAL_15]], align 4
// CHECK:         %[[VAL_535:.*]] = icmp ult i32 %[[VAL_95]], 1
// CHECK:         %[[VAL_536:.*]] = select i1 %[[VAL_535]], float* %[[VAL_534]], float* %[[VAL_15]]
// CHECK:         %[[VAL_537:.*]] = load float, float* %[[VAL_536]], align 4
// CHECK:         %[[VAL_538:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_537]], i32 16, i32 31)
// CHECK:         store float %[[VAL_538]], float* %[[VAL_14]], align 4
// CHECK:         call void @region_2_9(float* %[[VAL_536]], float* %[[VAL_14]], float* %[[VAL_536]])
// CHECK:         %[[VAL_539:.*]] = load float, float* %[[VAL_536]], align 4
// CHECK:         %[[VAL_540:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_539]], i32 8, i32 31)
// CHECK:         store float %[[VAL_540]], float* %[[VAL_13]], align 4
// CHECK:         call void @region_2_9(float* %[[VAL_536]], float* %[[VAL_13]], float* %[[VAL_536]])
// CHECK:         %[[VAL_541:.*]] = load float, float* %[[VAL_536]], align 4
// CHECK:         %[[VAL_542:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_541]], i32 4, i32 31)
// CHECK:         store float %[[VAL_542]], float* %[[VAL_12]], align 4
// CHECK:         call void @region_2_9(float* %[[VAL_536]], float* %[[VAL_12]], float* %[[VAL_536]])
// CHECK:         %[[VAL_543:.*]] = load float, float* %[[VAL_536]], align 4
// CHECK:         %[[VAL_544:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_543]], i32 2, i32 31)
// CHECK:         store float %[[VAL_544]], float* %[[VAL_11]], align 4
// CHECK:         call void @region_2_9(float* %[[VAL_536]], float* %[[VAL_11]], float* %[[VAL_536]])
// CHECK:         %[[VAL_545:.*]] = load float, float* %[[VAL_536]], align 4
// CHECK:         %[[VAL_546:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_545]], i32 1, i32 31)
// CHECK:         store float %[[VAL_546]], float* %[[VAL_10]], align 4
// CHECK:         call void @region_2_9(float* %[[VAL_536]], float* %[[VAL_10]], float* %[[VAL_536]])
// CHECK:         %[[VAL_547:.*]] = icmp eq i32 %[[VAL_95]], 0
// CHECK:         br i1 %[[VAL_547]], label %[[VAL_548:.*]], label %[[VAL_147]]
// CHECK:       reduction_atomic_update-after144:                  ; preds = %[[VAL_549:.*]], %[[VAL_146]]
// CHECK:         br label %[[VAL_51]]
// CHECK:       reduction_atomic_update-true143:                   ; preds = %[[VAL_146]]
// CHECK:         %[[VAL_550:.*]] = load float, float* %[[VAL_534]], align 4
// CHECK:         %[[VAL_551:.*]] = bitcast float* %[[VAL_129]] to i32*
// CHECK:         %[[VAL_552:.*]] = bitcast i32* %[[VAL_8]] to float*
// CHECK:         %[[VAL_553:.*]] = load i32, i32* %[[VAL_551]], align 4
// CHECK:         store i32 %[[VAL_553]], i32* %[[VAL_9]], align 4
// CHECK:         br label %[[VAL_554:.*]]
// CHECK:       atomic_op_loop_exit:                              ; preds = %[[VAL_554]]
// CHECK:         br label %[[VAL_147]]
// CHECK:       atomic_op_loop_body:                              ; preds = %[[VAL_554]], %[[VAL_548]]
// CHECK:         %[[VAL_555:.*]] = load i32, i32* %[[VAL_9]], align 4
// CHECK:         store i32 %[[VAL_555]], i32* %[[VAL_8]], align 4
// CHECK:         call void @region_2_9(float* %[[VAL_552]], float* %[[VAL_534]], float* %[[VAL_552]])
// CHECK:         %[[VAL_556:.*]] = load i32, i32* %[[VAL_8]], align 4
// CHECK:         %[[VAL_557:.*]] = cmpxchg i32* %[[VAL_551]], i32 %[[VAL_555]], i32 %[[VAL_556]] seq_cst seq_cst
// CHECK:         %[[VAL_558:.*]] = extractvalue { i32, i1 } %[[VAL_557]], 0
// CHECK:         store i32 %[[VAL_558]], i32* %[[VAL_9]], align 4
// CHECK:         %[[VAL_559:.*]] = extractvalue { i32, i1 } %[[VAL_557]], 1
// CHECK:         br i1 %[[VAL_559]], label %[[VAL_549]], label %[[VAL_554]]
// CHECK:       }
// CHECK:       ; Function Attrs: nounwind readnone
// CHECK:       declare i32 @llvm.nvvm.read.ptx.sreg.ctaid.y() #0

// CHECK: define internal void @region_1_4(float* dereferenceable(4) %[[VAL_0:.*]], float* dereferenceable(4) %[[VAL_1:.*]], float* dereferenceable(4) %[[VAL_2:.*]]) {
// CHECK:       entry:
// CHECK:         %[[VAL_3:.*]] = alloca float, align 4
// CHECK:         %[[VAL_4:.*]] = load float, float* %[[VAL_0]], align 4
// CHECK:         %[[VAL_5:.*]] = load float, float* %[[VAL_1]], align 4
// CHECK:         %[[VAL_6:.*]] = fadd float %[[VAL_4]], %[[VAL_5]]
// CHECK:         store float %[[VAL_6]], float* %[[VAL_3]], align 4
// CHECK:         %[[VAL_7:.*]] = load float, float* %[[VAL_3]], align 4
// CHECK:         store float %[[VAL_7]], float* %[[VAL_2]], align 4
// CHECK:         ret void
// CHECK:       }

// CHECK: define internal void @region_2_9(float* dereferenceable(4) %[[VAL_0:.*]], float* dereferenceable(4) %[[VAL_1:.*]], float* dereferenceable(4) %[[VAL_2:.*]]) {
// CHECK:       entry:
// CHECK:         %[[VAL_3:.*]] = alloca float, align 4
// CHECK:         %[[VAL_4:.*]] = load float, float* %[[VAL_0]], align 4
// CHECK:         %[[VAL_5:.*]] = load float, float* %[[VAL_1]], align 4
// CHECK:         %[[VAL_6:.*]] = call float @llvm.maxnum.f32(float %[[VAL_4]], float %[[VAL_5]])
// CHECK:         store float %[[VAL_6]], float* %[[VAL_3]], align 4
// CHECK:         %[[VAL_7:.*]] = load float, float* %[[VAL_3]], align 4
// CHECK:         store float %[[VAL_7]], float* %[[VAL_2]], align 4
// CHECK:         ret void
// CHECK:       }

HloModule Test

Add {
  lhsadd = f32[] parameter(0)
  rhsadd = f32[] parameter(1)
  ROOT add = f32[] add(lhsadd, rhsadd)
}

Max {
  lhsmax = f32[] parameter(0)
  rhsmax = f32[] parameter(1)
  ROOT max = f32[] maximum(lhsmax, rhsmax)
}


fused_reduce {
  p0 = f32[2,32,32]{2,1,0} parameter(0)
  init1 = f32[] parameter(1)
  init2 = f32[] parameter(2)
  r1 = f32[2,32]{1,0} reduce(p0, init1), dimensions={2}, to_apply=Add
  r2 = f32[2,32]{1,0} reduce(p0, init2), dimensions={2}, to_apply=Max
  ROOT tuple = (f32[2,32]{1,0}, f32[2,32]{1,0}) tuple(r1, r2)
}

ENTRY reduce {
  p = f32[2,32,32]{2,1,0} parameter(0)
  i = f32[] parameter(1)
  j = f32[] parameter(2)
  ROOT fusion = (f32[2,32]{1,0}, f32[2,32]{1,0}) fusion(p, i, j),
   kind=kInput, calls=fused_reduce
}
